unit DirectX.Math;
//-------------------------------------------------------------------------------------
// DirectXMath.h -- SIMD C++ Math library

// THIS CODE AND INFORMATION IS PROVIDED "AS IS" WITHOUT WARRANTY OF
// ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO
// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND/OR FITNESS FOR A
// PARTICULAR PURPOSE.

// Copyright (c) Microsoft Corporation. All rights reserved.

// http://go.microsoft.com/fwlink/?LinkID=615560
//-------------------------------------------------------------------------------------
// Copyright (c) Pascal Translation
// Norbert Sonnleitner
//-------------------------------------------------------------------------------------


(* DirectXMath Library Compiler Directives
   see: https://msdn.microsoft.com/en-us/library/windows/desktop/ee415579(v=vs.85).aspx

   Compiler directives tune the functionality that the DirectXMath library uses.

   _XM_NO_INTRINSICS_
   When _XM_NO_INTRINSICS_ is defined, DirectXMath operations are implemented without using any platform-specific intrinsics.
   Instead, DirectXMath uses only standard floating point operations. By default, _XM_NO_INTRINSICS_ is not defined.
   This directive overrides all other directives. Therefore, we recommend that you check for _XM_NO_INTRINSICS_ before you
   determine the status of _XM_ARM_NEON_INTRINSICS_ or _XM_SSE_INTRINSICS_.

   _XM_SSE_INTRINSICS_
   When _XM_SSE_INTRINSICS_ is defined, code is compiled to use supporting SSE and SSE2 on platforms that support these instruction sets.
   The Windows versions providing SSE intrinsics support both SSE and SSE2.
   _XM_SSE_INTRINSICS_ has no effect on systems that do not support SSE and SSE2.
   By default, _XM_SSE_INTRINSICS_ is defined when users compile for a Windows platform.
   DirectXMath uses the standard compiler defines (_M_IX86 / _M_AMD64) to determine Windows x86 or Windows x64 targets.

   _XM_ARM_NEON_INTRINSICS_
   This indicates the DirectXMath implementation uses of the ARM-NEON intrinsics. By defualt, _XM_ARM_NEON_INTRINSICS_ is defined
   when users compile for a Windows RT platform. DirectXMath uses the standard compiler define (_M_ARM, _M_ARM64) to determine this binary target.
   ARM-NEON intrinsics support is new to DirectXMath and is not supported by XNAMath 2.x.

   _XM_SSE4_INTRINSICS_
   New for Windows 10 Anniversary SDK.
   When you specify this compiler directive, DirectXMath functions are implemented to make use of SSE3 and SSE4.1 instriniscs where applicable;
   otherwise it uses to SSE/SSE2.

   _XM_AVX_INTRINSICS_
   New for Windows 10 Anniversary SDK. Use of /arch:AVX will enable this directive.
   When you specify this compiler directive, DirectXMath functions are implemented to make use of SSE3, SSE4.1, and AVX 128-bit intrinsics where applicable; otherwise DirectXMath uses SSE/SSE2. When you specify this compiler directive, it also implies _XM_SSE4_INTRINSICS_ and _XM_SSE_INTRINSICS_, and includes a small subset of SSE3 instructions.

   _XM_F16C_INTRINSICS_
   New for Windows 10 Anniversary SDK.
   Use of /arch:AVX2 will enable this directive.
   When you specify this compiler directive, DirectXMath functions are implemented to make use of SSE3, SSE4.1, AVX 128-bit, and F16C/CVT16 intrinsics where applicable; otherwise DirectXMath uses SSE/SSE2. When you use _XM_F16C_INTRINSICS_, it implies _XM_AVX_INTRINSICS_, _XM_SSE4_INTRINSICS_, and _XM_SSE_INTRINSICS_.

   _XM_VECTORCALL_
   This enables the use of the new __vectorcall calling convention for Windows x86 or Windows x64 targets. It defaults to _XM_VECTORCALL_=1
   when the compiler supports this feature. You can manually set it to _XM_VECTORCALL_=0 to always disable it.
   __vectorcall is not supported for the Windows RT platform or Managed C++.

   For FPC -Sv is the option, but also select a suitable FPU. (SSE(X), AVX, AVX2)
*)
//  fpc -al -Sd -O4 -Sv -Cfavx2 -Cpcoreavx2 -Opcoreavx2

{$mode delphiunicode}{$H+}
{$ASMMODE intel}

//{$DEFINE __AVX2__}
//{$DEFINE __AVX__}

{$IF  not defined(_XM_AVX2_INTRINSICS_) AND defined(__AVX2__) AND not defined(_XM_NO_INTRINSICS_)}
    {$define _XM_AVX2_INTRINSICS_}
{$ENDIF}

{$IF  NOT defined(_XM_FMA3_INTRINSICS_) AND defined(_XM_AVX2_INTRINSICS_) AND NOT defined(_XM_NO_INTRINSICS_)}
    {$define _XM_FMA3_INTRINSICS_}
{$ENDIF}

{$IF NOT defined(_XM_F16C_INTRINSICS_) AND defined(_XM_AVX2_INTRINSICS_)  AND NOT defined(_XM_NO_INTRINSICS_)}
    {$define _XM_F16C_INTRINSICS_}
{$ENDIF}

{$IF defined(_XM_FMA3_INTRINSICS_) AND NOT defined(_XM_AVX_INTRINSICS_)}
    {$define _XM_AVX_INTRINSICS_}
{$ENDIF}

{$IF defined(_XM_F16C_INTRINSICS_) AND NOT defined(_XM_AVX_INTRINSICS_)}
   {$define _XM_AVX_INTRINSICS_}
{$ENDIF}

{$IF NOT defined(_XM_AVX_INTRINSICS_) AND defined(__AVX__) AND NOT defined(_XM_NO_INTRINSICS_)}
   {$define _XM_AVX_INTRINSICS_}
{$ENDIF}

{$IF defined(_XM_AVX_INTRINSICS_) AND NOT defined(_XM_SSE4_INTRINSICS_)}
    {$define _XM_SSE4_INTRINSICS_}
{$ENDIF}

{$IF defined(_XM_SSE4_INTRINSICS_) AND  NOT defined(_XM_SSE3_INTRINSICS_)}
    {$define _XM_SSE3_INTRINSICS_}
{$ENDIF}

{$IF defined(_XM_SSE3_INTRINSICS_) AND NOT defined(_XM_SSE_INTRINSICS_)}
    {$define _XM_SSE_INTRINSICS_}
{$ENDIF}

{$IF NOT defined(_XM_ARM_NEON_INTRINSICS_) AND NOT defined(_XM_SSE_INTRINSICS_) AND NOT defined(_XM_NO_INTRINSICS_)}
{$IF (defined(CPU386) OR defined(CPUX86_64)) AND NOT defined(_M_HYBRID_X86_ARM64)}
{$define _XM_SSE_INTRINSICS_}
{$ELSEIF defined(CPUARM ) OR  defined(_M_ARM64) OR defined(_M_HYBRID_X86_ARM64)}
{$define _XM_ARM_NEON_INTRINSICS_}
{$ELSEIF NOT defined(_XM_NO_INTRINSICS_)}
{$ERROR DirectX Math does not support this target}
{$ENDIF}
{$ENDIF}//  NOT _XM_ARM_NEON_INTRINSICS_  AND   NOT _XM_SSE_INTRINSICS_  AND   NOT _XM_NO_INTRINSICS_

{$IF NOT defined(_XM_NO_XMVECTOR_OVERLOADS_)  AND  defined(__clang__)}
{$define _XM_NO_XMVECTOR_OVERLOADS_}
{$ENDIF}

//{$define _XM_NO_INTRINSICS_}
{$define _XM_SSE_INTRINSICS_}
// {$define _XM_SSE4_INTRINSICS_}
// {$define _XM_SSE3_INTRINSICS_}
{$define _XM_AVX_INTRINSICS_}




interface

uses
    Windows, Classes, SysUtils;

const
    DIRECTX_MATH_VERSION = 311;

const
    _MM_FROUND_TO_NEAREST_INT = $0; // Round to nearest (even).
    _MM_FROUND_TO_NEG_INF = $1; // Round down (toward -∞).
    _MM_FROUND_TO_POS_INF = $2; // Round up (toward +∞).
    _MM_FROUND_TO_ZERO = $3; // Round toward zero (truncate).
    _MM_FROUND_CUR_DIRECTION = $4; // Use current MXCSR setting.
    _MM_FROUND_RAISE_EXC = $0; // Signal precision exception on SNaN.
    _MM_FROUND_NO_EXC = $8; // Do not signal precision exception on SNaN.

const
    {***************************************************************************
     *
     * Constant definitions
     *
     ***************************************************************************}
    XM_PI: single = 3.141592654;
    XM_2PI: single = 6.283185307;
    XM_1DIVPI: single = 0.318309886;
    XM_1DIV2PI: single = 0.159154943;
    XM_PIDIV2: single = 1.570796327;
    XM_PIDIV4: single = 0.785398163;

    XM_SELECT_0: UINT32 = $00000000;
    XM_SELECT_1: UINT32 = $FFFFFFFF;

    XM_PERMUTE_0X: UINT32 = 0;
    XM_PERMUTE_0Y: UINT32 = 1;
    XM_PERMUTE_0Z: UINT32 = 2;
    XM_PERMUTE_0W: UINT32 = 3;
    XM_PERMUTE_1X: UINT32 = 4;
    XM_PERMUTE_1Y: UINT32 = 5;
    XM_PERMUTE_1Z: UINT32 = 6;
    XM_PERMUTE_1W: UINT32 = 7;

    XM_SWIZZLE_X: UINT32 = 0;
    XM_SWIZZLE_Y: UINT32 = 1;
    XM_SWIZZLE_Z: UINT32 = 2;
    XM_SWIZZLE_W: UINT32 = 3;

    XM_CRMASK_CR6: UINT32 = $000000F0;
    XM_CRMASK_CR6TRUE: UINT32 = $00000080;
    XM_CRMASK_CR6FALSE: UINT32 = $00000020;
    XM_CRMASK_CR6BOUNDS: UINT32 = $00000020; // XM_CRMASK_CR6FALSE

    XM_CACHE_LINE_SIZE: Size_T = 64;
    XM3_DECOMP_EPSILON: single = 0.0001;

{***************************************************************************
 *
 * Macros
 *
 ***************************************************************************}
// Unit conversion
function XMConvertToRadians(fDegrees: single): single; inline;
function XMConvertToDegrees(fRadians: single): single; inline;
// Condition register evaluation proceeding a recording (R) comparison
function XMComparisonAllTrue(CR: uint32): boolean; inline;
function XMComparisonAnyTrue(CR: uint32): boolean; inline;
function XMComparisonAllFalse(CR: uint32): boolean; inline;
function XMComparisonAnyFalse(CR: uint32): boolean; inline;
function XMComparisonMixed(CR: uint32): boolean; inline;
function XMComparisonAllInBounds(CR: uint32): boolean; inline;
function XMComparisonAnyOutOfBounds(CR: uint32): boolean; inline;

type

    TCPUType = (CPU_XM_SSE3, CPU_XM_SSE4, CPU_AVX, CPU_AVX2);
    TCPUInfo = array [0..3] of cardinal;

    {$PACKRECORDS 4}

    { TXMVECTOR }
    //------------------------------------------------------------------------------
    // Vector intrinsic: Four 32 bit floating point components aligned on a 16 byte
    // boundary and mapped to hardware vector registers
    TXMVECTOR = record
        constructor Create(x, y, z, w: single);
    //------------------------------------------------------------------------------
    // Vector operators
        class operator Positive(a: TXMVECTOR): TXMVECTOR; inline;
        class operator Negative(a: TXMVECTOR): TXMVECTOR; inline;
        class operator Add(a, b: TXMVECTOR): TXMVECTOR; inline;
        class operator Subtract(a, b: TXMVECTOR): TXMVECTOR; inline;
        class operator Multiply(a, b: TXMVECTOR): TXMVECTOR; inline;
        class operator Divide(a, b: TXMVECTOR): TXMVECTOR; inline;
        class operator Multiply(v: TXMVECTOR; s: single): TXMVECTOR; inline;
        class operator Divide(v: TXMVECTOR; s: single): TXMVECTOR; inline;
        case integer of
            0: (f32: array [0..3] of single);
            1: (u32: array [0..3] of uint32);
            2: (i32: array [0..3] of int32);
    end;

    PXMVECTOR = ^TXMVECTOR;
    TXMVECTORArray = array of TXMVECTOR;

    { TXMVECTORF32 }
    //------------------------------------------------------------------------------
    // Conversion types for constants
    TXMVECTORF32 = record
        class operator Implicit(a: TXMVECTORF32): PSingle;
        class operator Implicit(a: TXMVECTORF32): TXMVECTOR;
        case integer of
            0: (f: array [0..3] of single);
            1: (v: TXMVECTOR);
    end;

    { TXMVECTORU32 }

    TXMVECTORU32 = record
        class operator Implicit(a: TXMVECTORU32): Puint32;
        class operator Implicit(a: TXMVECTORU32): TXMVECTOR;
        case integer of
            0: (u: array [0..3] of uint32);
            1: (v: TXMVECTOR);
    end;


    { TXMVECTORI32 }

    TXMVECTORI32 = record
        class operator Implicit(a: TXMVECTORI32): Pint32;
        class operator Implicit(a: TXMVECTORI32): TXMVECTOR;
        case integer of
            0: (i: array [0..3] of int32);
            1: (v: TXMVECTOR);
    end;

    { TXMVECTORU8 }

    TXMVECTORU8 = record
        class operator Implicit(a: TXMVECTORU8): PByte;
        class operator Implicit(a: TXMVECTORU8): TXMVECTOR;
        case integer of
            0: (u: array [0..15] of byte);
            1: (v: TXMVECTOR);
    end;


    //------------------------------------------------------------------------------
    // Matrix type: Sixteen 32 bit floating point components aligned on a
    // 16 byte boundary and mapped to four hardware vector registers

    { TXMMATRIX }

    TXMMATRIX = record
        constructor Create(R0, R1, R2, R3: TXMVECTOR); overload;
        constructor Create(m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23, m30, m31, m32, m33: single);
                overload;
        constructor Create(pArray: PSingle); overload;
        class operator Positive(a: TXMMATRIX): TXMMATRIX; inline;
        class operator Negative(a: TXMMATRIX): TXMMATRIX; inline;
        class operator Add(a, b: TXMMATRIX): TXMMATRIX; inline;
        class operator Subtract(a, b: TXMMATRIX): TXMMATRIX; inline;
        class operator Multiply(a, b: TXMMATRIX): TXMMATRIX; inline;
        class operator Multiply(M: TXMMATRIX; s: single): TXMMATRIX; inline;
        class operator Divide(M: TXMMATRIX; s: single): TXMMATRIX; inline;
        function Get(Row, Column: size_t): single;

        case integer of
            0: (r: array [0..3] of TXMVECTOR);
            1: (_11, _12, _13, _14: single;
                _21, _22, _23, _24: single;
                _31, _32, _33, _34: single;
                _41, _42, _43, _44: single;
            );
            2: (m: array[0..3, 0..3] of single);
            3: (n: array [0..15] of single);
            4: (u: array [0..15] of UINT32);
            5: (r0, r1, r2, r3: TXMVECTOR);
    end;



    //------------------------------------------------------------------------------
    // 2D Vector; 32 bit floating point components
    {$PACKRECORDS 4}
    { TXMFLOAT2 }

    TXMFLOAT2 = record
        x: single;
        y: single;
        constructor Create(_x, _y: single); overload;
        constructor Create(pArray: PSingle); overload;
    end;

    PXMFLOAT2 = ^TXMFLOAT2;

    // 2D Vector; 32 bit floating point components aligned on a 16 byte boundary
     {$PACKRECORDS 16}
    { TXMFLOAT2A }
    TXMFLOAT2A = record
        x: single;
        y: single;
        constructor Create(_x, _y: single); overload;
        constructor Create(pArray: PSingle); overload;
        class operator Implicit(Float2: TXMFLOAT2): TXMFLOAT2A;
    end;



    { TXMINT2 - 2D Vector; 32 bit signed integer components }
    {$PACKRECORDS 4}
    TXMINT2 = record
        x: int32;
        y: int32;
        constructor Create(_x, _y: int32); overload;
        constructor Create(pArray: PINT32); overload;
    end;

    PXMINT2 = ^TXMINT2;

    { TXMUINT2 - 2D Vector; 32 bit unsigned integer components  }

    TXMUINT2 = record
        x: uint32;
        y: uint32;
        constructor Create(_x, _y: uint32); overload;
        constructor Create(pArray: PUINT32); overload;
    end;

    PXMUINT2 = ^TXMUINT2;

    //------------------------------------------------------------------------------

    { TXMFLOAT3 - 3D Vector; 32 bit floating point components }

    TXMFLOAT3 = record
        x: single;
        y: single;
        z: single;
        constructor Create(_x, _y, _z: single); overload;
        constructor Create(pArray: Psingle); overload;
    end;
    PXMFLOAT3 = ^TXMFLOAT3;

    // 3D Vector; 32 bit floating point components aligned on a 16 byte boundary
    {$PACKRECORDS 16}

    { TXMFLOAT3A }

    TXMFLOAT3A = record
        x: single;
        y: single;
        z: single;
        constructor Create(_x, _y, _z: single); overload;
        constructor Create(pArray: Psingle); overload;
        class operator Implicit(Float3: TXMFLOAT3): TXMFLOAT3A;
    end;

    //------------------------------------------------------------------------------

    { TXMINT3 - 3D Vector; 32 bit signed integer components }
    {$PACKRECORDS 4}
    TXMINT3 = record
        x: int32;
        y: int32;
        z: int32;
        constructor Create(_x, _y, _z: int32); overload;
        constructor Create(pArray: Pint32); overload;
    end;

    PXMINT3 = ^TXMINT3;

    { TXMUINT3 - 3D Vector; 32 bit unsigned integer components }

    TXMUINT3 = record
        x: uint32;
        y: uint32;
        z: uint32;
        constructor Create(_x, _y, _z: Uint32); overload;
        constructor Create(pArray: PUint32); overload;

    end;

    PXMUINT3 = ^TXMUINT3;

    //------------------------------------------------------------------------------

    { TXMFLOAT4 - 4D Vector; 32 bit floating point components }
    {$PACKRECORDS 4}
    TXMFLOAT4 = record
        x: single;
        y: single;
        z: single;
        w: single;
        constructor Create(pArray: PSingle); overload;
        constructor Create(_X, _Y, _Z, _W: single); overload;
    end;

    PXMFLOAT4 = ^TXMFLOAT4;


    { TXMFLOAT4A - 4D Vector; 32 bit floating point components aligned on a 16 byte boundary }
   {$PACKRECORDS 16}
    TXMFLOAT4A = record
        x: single;
        y: single;
        z: single;
        w: single;
        constructor Create(pArray: PSingle); overload;
        constructor Create(_X, _Y, _Z, _W: single); overload;
        class operator Implicit(Float4: TXMFLOAT4): TXMFLOAT4A;
    end;

    PXMFLOAT4A = ^TXMFLOAT4A;


    //------------------------------------------------------------------------------
    { TXMINT4 - 4D Vector; 32 bit signed integer components }
    {$PACKRECORDS 4}
    TXMINT4 = record
        x: int32;
        y: int32;
        z: int32;
        w: int32;
        constructor Create(_X, _Y, _Z, _W: int32); overload;
        constructor Create(pArray: Pint32); overload;
    end;

    PXMINT4 = ^TXMINT4;

    // 4D Vector; 32 bit unsigned integer components

    { TXMUINT4 - 4D Vector; 32 bit unsigned integer components }
    {$PACKRECORDS 4}
    TXMUINT4 = record
        x: uint32;
        y: uint32;
        z: uint32;
        w: uint32;
        constructor Create(_X, _Y, _Z, _W: uint32); overload;
        constructor Create(pArray: Puint32); overload;
    end;

    PXMUINT4 = ^TXMUINT4;

    //------------------------------------------------------------------------------
    { TXMFLOAT3X3 - 3x3 Matrix: 32 bit floating point components }

    TXMFLOAT3X3 = record
        constructor Create(constref pArray: PSingle); overload;
        constructor Create(m00, m01, m02, m10, m11, m12, m20, m21, m22: single); overload;
        function Get(Row, Column: size_t): single;
        case integer of
            0: (_11, _12, _13: single;
                _21, _22, _23: single;
                _31, _32, _33: single;);
            1: (m: array [0..2, 0..2] of single);
    end;

    PXMFLOAT3X3 = ^TXMFLOAT3X3;

    //------------------------------------------------------------------------------
    { TXMFLOAT4X3 - 4x3 Matrix: 32 bit floating point components }

    TXMFLOAT4X3 = record
        constructor Create(m00, m01, m02, m10, m11, m12, m20, m21, m22, m30, m31, m32: single); overload;
        constructor Create(constref pArray: PSingle);
        function Get(Row, Column: size_t): single;
        case integer of
            0: (_11, _12, _13: single;
                _21, _22, _23: single;
                _31, _32, _33: single;
                _41, _42, _43: single;);
            1: (m: array [0..3, 0..2] of single);

    end;

    PXMFLOAT4X3 = ^TXMFLOAT4X3;

    // 4x3 Matrix: 32 bit floating point components aligned on a 16 byte boundary
{$PACKRECORDS 16}

    { TXMFLOAT4X3A }

    TXMFLOAT4X3A = record
        constructor Create(m00, m01, m02, m10, m11, m12, m20, m21, m22, m30, m31, m32: single); overload;
        constructor Create(constref pArray: PSingle);
        function Get(Row, Column: size_t): single;
        case integer of
            0: (_11, _12, _13: single;
                _21, _22, _23: single;
                _31, _32, _33: single;
                _41, _42, _43: single;);
            1: (m: array [0..3, 0..2] of single);
    end;
    PXMFLOAT4X3A = ^TXMFLOAT4X3A;



    //------------------------------------------------------------------------------
    { TXMFLOAT4X4 - 4x4 Matrix: 32 bit floating point components }
    {$PACKRECORDS 4}
    TXMFLOAT4X4 = record
        constructor Create(m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23, m30, m31, m32, m33: single); overload;
        constructor Create(constref pArray: PSingle); overload;
        function Get(Row, Column: size_t): single;
        case integer of
            0: (
                _11, _12, _13, _14: single;
                _21, _22, _23, _24: single;
                _31, _32, _33, _34: single;
                _41, _42, _43, _44: single;
            );
            1: (m: array [0..3, 0..3] of single);
    end;

    PXMFLOAT4X4 = ^TXMFLOAT4X4;

    { TXMFLOAT4X4A - 4x4 Matrix: 32 bit floating point components aligned on a 16 byte boundary }
    {$PACKRECORDS 16}
    TXMFLOAT4X4A = record
        constructor Create(m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23, m30, m31, m32, m33: single); overload;
        constructor Create(constref pArray: PSingle); overload;
        function Get(Row, Column: size_t): single;
        case integer of
            0: (
                _11, _12, _13, _14: single;
                _21, _22, _23, _24: single;
                _31, _32, _33, _34: single;
                _41, _42, _43, _44: single;
            );
            1: (m: array [0..3, 0..3] of single);
    end;
    PXMFLOAT4X4A = ^TXMFLOAT4X4A;

  {$PACKRECORDS 4}


{****************************************************************************
 *
 * Globals
 *
 ****************************************************************************}

// The purpose of the following global constants is to prevent redundant
// reloading of the constants when they are referenced by more than one
// separate inline math routine called within the same function.  Declaring
// a constant locally within a routine is sufficient to prevent redundant
// reloads of that constant when that single routine is called multiple
// times in a function, but if the constant is used (and declared) in a
// separate math routine it would be reloaded.

const
    g_XMSinCoefficients0: TXMVECTORF32 = (f: (-0.16666667, +0.0083333310, -0.00019840874, +2.7525562e-06));
    g_XMSinCoefficients1: TXMVECTORF32 = (f: (-2.3889859e-08, -0.16665852 {Est1}, +0.0083139502 {Est2}, -0.00018524670 {Est3}));
    g_XMCosCoefficients0: TXMVECTORF32 = (f: (-0.5, +0.041666638, -0.0013888378, +2.4760495e-05));
    g_XMCosCoefficients1: TXMVECTORF32 = (f: (-2.6051615e-07, -0.49992746 {Est1}, +0.041493919 {Est2}, -0.0012712436 {Est3}));
    g_XMTanCoefficients0: TXMVECTORF32 = (f: (1.0, 0.333333333, 0.133333333, 5.396825397e-2));
    g_XMTanCoefficients1: TXMVECTORF32 = (f: (2.186948854e-2, 8.863235530e-3, 3.592128167e-3, 1.455834485e-3));
    g_XMTanCoefficients2: TXMVECTORF32 = (f: (5.900274264e-4, 2.391290764e-4, 9.691537707e-5, 3.927832950e-5));
    g_XMArcCoefficients0: TXMVECTORF32 = (f: (+1.5707963050, -0.2145988016, +0.0889789874, -0.0501743046));
    g_XMArcCoefficients1: TXMVECTORF32 = (f: (+0.0308918810, -0.0170881256, +0.0066700901, -0.0012624911));
    g_XMATanCoefficients0: TXMVECTORF32 = (f: (-0.3333314528, +0.1999355085, -0.1420889944, +0.1065626393));
    g_XMATanCoefficients1: TXMVECTORF32 = (f: (-0.0752896400, +0.0429096138, -0.0161657367, +0.0028662257));
    g_XMATanEstCoefficients0: TXMVECTORF32 = (f: (+0.999866, +0.999866, +0.999866, +0.999866));
    g_XMATanEstCoefficients1: TXMVECTORF32 = (f: (-0.3302995, +0.180141, -0.085133, +0.0208351));
    g_XMTanEstCoefficients: TXMVECTORF32 = (f: (2.484, -1.954923183e-1, 2.467401101, {XM_1DIVPI}0.318309886));
    g_XMArcEstCoefficients: TXMVECTORF32 = (f: (+1.5707288, -0.2121144, +0.0742610, -0.0187293));
    g_XMPiConstants0: TXMVECTORF32 = (f: ( {XM_PI}3.141592654, {XM_2PI}6.283185307, {XM_1DIVPI}0.318309886,        {XM_1DIV2PI}0.159154943));
    g_XMIdentityR0: TXMVECTORF32 = (f: (1.0, 0.0, 0.0, 0.0));
    g_XMIdentityR1: TXMVECTORF32 = (f: (0.0, 1.0, 0.0, 0.0));
    g_XMIdentityR2: TXMVECTORF32 = (f: (0.0, 0.0, 1.0, 0.0));
    g_XMIdentityR3: TXMVECTORF32 = (f: (0.0, 0.0, 0.0, 1.0));
    g_XMNegIdentityR0: TXMVECTORF32 = (f: (-1.0, 0.0, 0.0, 0.0));
    g_XMNegIdentityR1: TXMVECTORF32 = (f: (0.0, -1.0, 0.0, 0.0));
    g_XMNegIdentityR2: TXMVECTORF32 = (f: (0.0, 0.0, -1.0, 0.0));
    g_XMNegIdentityR3: TXMVECTORF32 = (f: (0.0, 0.0, 0.0, -1.0));
    g_XMNegativeZero: TXMVECTORU32 = (u: ($80000000, $80000000, $80000000, $80000000));
    g_XMNegate3: TXMVECTORU32 = (u: ($80000000, $80000000, $80000000, $00000000));
    g_XMMaskXY: TXMVECTORU32 = (u: ($FFFFFFFF, $FFFFFFFF, $00000000, $00000000));
    g_XMMask3: TXMVECTORU32 = (u: ($FFFFFFFF, $FFFFFFFF, $FFFFFFFF, $00000000));
    g_XMMaskX: TXMVECTORU32 = (u: ($FFFFFFFF, $00000000, $00000000, $00000000));
    g_XMMaskY: TXMVECTORU32 = (u: ($00000000, $FFFFFFFF, $00000000, $00000000));
    g_XMMaskZ: TXMVECTORU32 = (u: ($00000000, $00000000, $FFFFFFFF, $00000000));
    g_XMMaskW: TXMVECTORU32 = (u: ($00000000, $00000000, $00000000, $FFFFFFFF));
    g_XMOne: TXMVECTORF32 = (f: (1.0, 1.0, 1.0, 1.0));
    g_XMOne3: TXMVECTORF32 = (f: (1.0, 1.0, 1.0, 0.0));
    g_XMZero: TXMVECTORF32 = (f: (0.0, 0.0, 0.0, 0.0));
    g_XMTwo: TXMVECTORF32 = (f: (2.0, 2.0, 2.0, 2.0));
    g_XMFour: TXMVECTORF32 = (f: (4.0, 4.0, 4.0, 4.0));
    g_XMSix: TXMVECTORF32 = (f: (6.0, 6.0, 6.0, 6.0));
    g_XMNegativeOne: TXMVECTORF32 = (f: (-1.0, -1.0, -1.0, -1.0));
    g_XMOneHalf: TXMVECTORF32 = (f: (0.5, 0.5, 0.5, 0.5));
    g_XMNegativeOneHalf: TXMVECTORF32 = (f: (-0.5, -0.5, -0.5, -0.5));
    g_XMNegativeTwoPi: TXMVECTORF32 = (f: (-{XM_2PI}6.283185307, -{XM_2PI}6.283185307, -{XM_2PI}6.283185307, -{XM_2PI}6.283185307));
    g_XMNegativePi: TXMVECTORF32 = (f: (-{XM_PI}3.141592654, -{XM_PI}3.141592654, -{XM_PI}3.141592654, -{XM_PI}3.141592654));
    g_XMHalfPi: TXMVECTORF32 = (f: ( {XM_PIDIV2}1.570796327, {XM_PIDIV2}1.570796327, {XM_PIDIV2}1.570796327,          {XM_PIDIV2}1.570796327));
    g_XMPi: TXMVECTORF32 = (f: ( {XM_PI}3.141592654, {XM_PI}3.141592654,           {XM_PI}3.141592654, {XM_PI}3.141592654));
    g_XMReciprocalPi: TXMVECTORF32 = (f: ( {XM_1DIVPI}0.318309886, {XM_1DIVPI}0.318309886, {XM_1DIVPI}0.318309886,         {XM_1DIVPI}0.318309886));
    g_XMTwoPi: TXMVECTORF32 = (f: ( {XM_2PI}6.283185307, {XM_2PI}6.283185307, {XM_2PI}6.283185307,           {XM_2PI}6.283185307));
    g_XMReciprocalTwoPi: TXMVECTORF32 = (f: ( {XM_1DIV2PI}0.159154943, {XM_1DIV2PI}0.159154943, {XM_1DIV2PI}0.159154943,        {XM_1DIV2PI}0.159154943));
    g_XMEpsilon: TXMVECTORF32 = (f: (1.192092896e-7, 1.192092896e-7, 1.192092896e-7, 1.192092896e-7));
    g_XMInfinity: TXMVECTORI32 = (i: ($7F800000, $7F800000, $7F800000, $7F800000));
    g_XMQNaN: TXMVECTORI32 = (i: ($7FC00000, $7FC00000, $7FC00000, $7FC00000));
    g_XMQNaNTest: TXMVECTORI32 = (i: ($007FFFFF, $007FFFFF, $007FFFFF, $007FFFFF));
    g_XMAbsMask: TXMVECTORI32 = (i: ($7FFFFFFF, $7FFFFFFF, $7FFFFFFF, $7FFFFFFF));
    g_XMFltMin: TXMVECTORI32 = (i: ($00800000, $00800000, $00800000, $00800000));
    g_XMFltMax: TXMVECTORI32 = (i: ($7F7FFFFF, $7F7FFFFF, $7F7FFFFF, $7F7FFFFF));
    g_XMNegOneMask: TXMVECTORU32 = (u: ($FFFFFFFF, $FFFFFFFF, $FFFFFFFF, $FFFFFFFF));
    g_XMMaskA8R8G8B8: TXMVECTORU32 = (u: ($00FF0000, $0000FF00, $000000F, $FF000000));
    g_XMFlipA8R8G8B8: TXMVECTORU32 = (u: ($00000000, $00000000, $00000000, $80000000));
    g_XMFixAA8R8G8B8: TXMVECTORF32 = (f: (0.0, 0.0, 0.0, ($80000000)));
    g_XMNormalizeA8R8G8B8: TXMVECTORF32 = (f: (1.0 / (255.0 * ($10000)), 1.0 / (255.0 * ($100)), 1.0 / 255.0, 1.0 / (255.0 * ($1000000))));
    g_XMMaskA2B10G10R10: TXMVECTORU32 = (u: ($000003F, $000FFC00, $3FF00000, $C0000000));
    g_XMFlipA2B10G10R10: TXMVECTORU32 = (u: ($00000200, $00080000, $20000000, $80000000));
    g_XMFixAA2B10G10R10: TXMVECTORF32 = (f: (-512.0, -512.0 * ($400), -512.0 * ($100000), $80000000));
    g_XMNormalizeA2B10G10R10: TXMVECTORF32 = (f: (1.0 / 511.0, 1.0 / (511.0 * $400), 1.0 / (511.0 * $100000), 1.0 / (3.0 * $40000000)));
    g_XMMaskX16Y16: TXMVECTORU32 = (u: ($0000FFFF, $FFFF0000, $00000000, $00000000));
    g_XMFlipX16Y16: TXMVECTORI32 = (i: ($00008000, $00000000, $00000000, $00000000));
    g_XMFixX16Y16: TXMVECTORF32 = (f: (-32768.0, 0.0, 0.0, 0.0));
    g_XMNormalizeX16Y16: TXMVECTORF32 = (f: (1.0 / 32767.0, 1.0 / (32767.0 * 65536.0), 0.0, 0.0));
    g_XMMaskX16Y16Z16W16: TXMVECTORU32 = (u: ($0000FFFF, $0000FFFF, $FFFF0000, $FFFF0000));
    g_XMFlipX16Y16Z16W16: TXMVECTORI32 = (i: ($00008000, $00008000, $00000000, $00000000));
    g_XMFixX16Y16Z16W16: TXMVECTORF32 = (f: (-32768.0, -32768.0, 0.0, 0.0));
    g_XMNormalizeX16Y16Z16W16: TXMVECTORF32 = (f: (1.0 / 32767.0, 1.0 / 32767.0, 1.0 / (32767.0 * 65536.0), 1.0 / (32767.0 * 65536.0)));
    g_XMNoFraction: TXMVECTORF32 = (f: (8388608.0, 8388608.0, 8388608.0, 8388608.0));
    g_XMMaskByte: TXMVECTORI32 = (i: ($000000FF, $000000FF, $000000FF, $000000FF));
    g_XMNegateX: TXMVECTORF32 = (f: (-1.0, 1.0, 1.0, 1.0));
    g_XMNegateY: TXMVECTORF32 = (f: (1.0, -1.0, 1.0, 1.0));
    g_XMNegateZ: TXMVECTORF32 = (f: (1.0, 1.0, -1.0, 1.0));
    g_XMNegateW: TXMVECTORF32 = (f: (1.0, 1.0, 1.0, -1.0));
    g_XMSelect0101: TXMVECTORU32 = (u: ($00000000, $FFFFFFFF, $00000000, $FFFFFFFF));
    g_XMSelect1010: TXMVECTORU32 = (u: ($FFFFFFFF, $00000000, $FFFFFFFF, $00000000));
    g_XMOneHalfMinusEpsilon: TXMVECTORI32 = (i: ($3EFFFFFD, $3EFFFFFD, $3EFFFFFD, $3EFFFFFD));
    g_XMSelect1000: TXMVECTORU32 = (u: ($FFFFFFFF, $00000000, $00000000, $00000000));
    g_XMSelect1100: TXMVECTORU32 = (u: ($FFFFFFFF, $FFFFFFFF, $00000000, $00000000));
    g_XMSelect1110: TXMVECTORU32 = (u: ($FFFFFFFF, $FFFFFFFF, $FFFFFFFF, $00000000));
    g_XMSelect1011: TXMVECTORU32 = (u: ($FFFFFFFF, $00000000, $FFFFFFFF, $FFFFFFFF));
    g_XMFixupY16: TXMVECTORF32 = (f: (1.0, 1.0 / 65536.0, 0.0, 0.0));
    g_XMFixupY16W16: TXMVECTORF32 = (f: (1.0, 1.0, 1.0 / 65536.0, 1.0 / 65536.0));
    g_XMFlipY: TXMVECTORU32 = (u: (0, $80000000, 0, 0));
    g_XMFlipZ: TXMVECTORU32 = (u: (0, 0, $80000000, 0));
    g_XMFlipW: TXMVECTORU32 = (u: (0, 0, 0, $80000000));
    g_XMFlipYZ: TXMVECTORU32 = (u: (0, $80000000, $80000000, 0));
    g_XMFlipZW: TXMVECTORU32 = (u: (0, 0, $80000000, $80000000));
    g_XMFlipYW: TXMVECTORU32 = (u: (0, $80000000, 0, $80000000));
    g_XMMaskDec4: TXMVECTORI32 = (i: ($3FF, $3FF shl 10, $3FF shl 20, $3 shl 30));
    g_XMXorDec4: TXMVECTORI32 = (i: ($200, $200 shl 10, $200 shl 20, 0));
    g_XMAddUDec4: TXMVECTORF32 = (f: (0, 0, 0, 32768.0 * 65536.0));
    g_XMAddDec4: TXMVECTORF32 = (f: (-512.0, -512.0 * 1024.0, -512.0 * 1024.0 * 1024.0, 0));
    g_XMMulDec4: TXMVECTORF32 = (f: (1.0, 1.0 / 1024.0, 1.0 / (1024.0 * 1024.0), 1.0 / (1024.0 * 1024.0 * 1024.0)));
    g_XMMaskByte4: TXMVECTORU32 = (u: ($FF, $FF00, $FF0000, $FF000000));
    g_XMXorByte4: TXMVECTORI32 = (i: ($80, $8000, $800000, $00000000));
    g_XMAddByte4: TXMVECTORF32 = (f: (-128.0, -128.0 * 256.0, -128.0 * 65536.0, 0));
    g_XMFixUnsigned: TXMVECTORF32 = (f: (32768.0 * 65536.0, 32768.0 * 65536.0, 32768.0 * 65536.0, 32768.0 * 65536.0));
    g_XMMaxInt: TXMVECTORF32 = (f: (65536.0 * 32768.0 - 128.0, 65536.0 * 32768.0 - 128.0, 65536.0 * 32768.0 - 128.0, 65536.0 * 32768.0 - 128.0));
    g_XMMaxUInt: TXMVECTORF32 = (f: (65536.0 * 65536.0 - 256.0, 65536.0 * 65536.0 - 256.0, 65536.0 * 65536.0 - 256.0, 65536.0 * 65536.0 - 256.0));
    g_XMUnsignedFix: TXMVECTORF32 = (f: (32768.0 * 65536.0, 32768.0 * 65536.0, 32768.0 * 65536.0, 32768.0 * 65536.0));
    g_XMsrgbScale: TXMVECTORF32 = (f: (12.92, 12.92, 12.92, 1.0));
    g_XMsrgbA: TXMVECTORF32 = (f: (0.055, 0.055, 0.055, 0.0));
    g_XMsrgbA1: TXMVECTORF32 = (f: (1.055, 1.055, 1.055, 1.0));
    g_XMExponentBias: TXMVECTORI32 = (i: (127, 127, 127, 127));
    g_XMSubnormalExponent: TXMVECTORI32 = (i: (-126, -126, -126, -126));
    g_XMNumTrailing: TXMVECTORI32 = (i: (23, 23, 23, 23));
    g_XMMinNormal: TXMVECTORI32 = (i: ($00800000, $00800000, $00800000, $00800000));
    g_XMNegInfinity: TXMVECTORU32 = (u: ($FF800000, $FF800000, $FF800000, $FF800000));
    g_XMNegQNaN: TXMVECTORU32 = (u: ($FFC00000, $FFC00000, $FFC00000, $FFC00000));
    g_XMBin128: TXMVECTORI32 = (i: ($43000000, $43000000, $43000000, $43000000));
    g_XMBinNeg150: TXMVECTORU32 = (u: ($C3160000, $C3160000, $C3160000, $C3160000));
    g_XM253: TXMVECTORI32 = (i: (253, 253, 253, 253));
    g_XMExpEst1: TXMVECTORF32 = (f: (-6.93147182e-1, -6.93147182e-1, -6.93147182e-1, -6.93147182e-1));
    g_XMExpEst2: TXMVECTORF32 = (f: (+2.40226462e-1, +2.40226462e-1, +2.40226462e-1, +2.40226462e-1));
    g_XMExpEst3: TXMVECTORF32 = (f: (-5.55036440e-2, -5.55036440e-2, -5.55036440e-2, -5.55036440e-2));
    g_XMExpEst4: TXMVECTORF32 = (f: (+9.61597636e-3, +9.61597636e-3, +9.61597636e-3, +9.61597636e-3));
    g_XMExpEst5: TXMVECTORF32 = (f: (-1.32823968e-3, -1.32823968e-3, -1.32823968e-3, -1.32823968e-3));
    g_XMExpEst6: TXMVECTORF32 = (f: (+1.47491097e-4, +1.47491097e-4, +1.47491097e-4, +1.47491097e-4));
    g_XMExpEst7: TXMVECTORF32 = (f: (-1.08635004e-5, -1.08635004e-5, -1.08635004e-5, -1.08635004e-5));
    g_XMLogEst0: TXMVECTORF32 = (f: (+1.442693, +1.442693, +1.442693, +1.442693));
    g_XMLogEst1: TXMVECTORF32 = (f: (-0.721242, -0.721242, -0.721242, -0.721242));
    g_XMLogEst2: TXMVECTORF32 = (f: (+0.479384, +0.479384, +0.479384, +0.479384));
    g_XMLogEst3: TXMVECTORF32 = (f: (-0.350295, -0.350295, -0.350295, -0.350295));
    g_XMLogEst4: TXMVECTORF32 = (f: (+0.248590, +0.248590, +0.248590, +0.248590));
    g_XMLogEst5: TXMVECTORF32 = (f: (-0.145700, -0.145700, -0.145700, -0.145700));
    g_XMLogEst6: TXMVECTORF32 = (f: (+0.057148, +0.057148, +0.057148, +0.057148));
    g_XMLogEst7: TXMVECTORF32 = (f: (-0.010578, -0.010578, -0.010578, -0.010578));
    g_XMLgE: TXMVECTORF32 = (f: (+1.442695, +1.442695, +1.442695, +1.442695));
    g_XMInvLgE: TXMVECTORF32 = (f: (+6.93147182e-1, +6.93147182e-1, +6.93147182e-1, +6.93147182e-1));
    g_UByteMax: TXMVECTORF32 = (f: (255.0, 255.0, 255.0, 255.0));
    g_ByteMin: TXMVECTORF32 = (f: (-127.0, -127.0, -127.0, -127.0));
    g_ByteMax: TXMVECTORF32 = (f: (127.0, 127.0, 127.0, 127.0));
    g_ShortMin: TXMVECTORF32 = (f: (-32767.0, -32767.0, -32767.0, -32767.0));
    g_ShortMax: TXMVECTORF32 = (f: (32767.0, 32767.0, 32767.0, 32767.0));
    g_UShortMax: TXMVECTORF32 = (f: (65535.0, 65535.0, 65535.0, 65535.0));




{***************************************************************************
 *
 * Data conversion operations
 *
 ***************************************************************************}

function XMConvertVectorIntToFloat(constref VInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR;
function XMConvertVectorFloatToInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR;
function XMConvertVectorUIntToFloat(constref VUInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR;
function XMConvertVectorFloatToUInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR;


function XMVectorSetBinaryConstant(constref C0: UINT32; constref C1: UINT32; constref C2: UINT32; constref C3: UINT32): TXMVECTOR;
function XMVectorSplatConstant(IntConstant: INT32; DivExponent: UINT32): TXMVECTOR;
function XMVectorSplatConstantInt(IntConstant: INT32): TXMVECTOR; inline;


{***************************************************************************
 *
 * Load operations
 *
 ***************************************************************************}

function XMLoadInt(constref pSource: PUINT32): TXMVECTOR; inline;
function XMLoadFloat(constref pSource: PSingle): TXMVECTOR; inline;

function XMLoadInt2(const pSource: PUINT32): TXMVECTOR; inline;  // _In_reads_(2)
function XMLoadInt2A(const PSource: PUINT32): TXMVECTOR; inline; // _In_reads_(2)
function XMLoadFloat2(const pSource: TXMFLOAT2): TXMVECTOR; inline;
function XMLoadFloat2A(const pSource: TXMFLOAT2A): TXMVECTOR; inline;
function XMLoadSInt2(const pSource: TXMINT2): TXMVECTOR; inline;
function XMLoadUInt2(const pSource: TXMUINT2): TXMVECTOR; inline;

function XMLoadInt3(const pSource: PUINT32): TXMVECTOR; inline; // _In_reads_(3)
function XMLoadInt3A(const pSource: PUINT32): TXMVECTOR; inline; // _In_reads_(3)

function XMLoadFloat3(constref pSource: TXMFLOAT3): TXMVECTOR; inline; overload;
function XMLoadFloat3(constref pSource: pSingle): TXMVECTOR; inline; overload; // array [0..2] of single;

function XMLoadFloat3A(const pSource: TXMFLOAT3A): TXMVECTOR; inline;
function XMLoadSInt3(const pSource: TXMINT3): TXMVECTOR; inline;
function XMLoadUInt3(const pSource: TXMUINT3): TXMVECTOR; inline;

function XMLoadInt4(const pSource: PUINT32): TXMVECTOR; inline; // _In_reads_(4)
function XMLoadInt4A(const pSource: PUINT32): TXMVECTOR; inline; // _In_reads_(4)

function XMLoadFloat4(constref pSource: TXMFLOAT4): TXMVECTOR; overload; inline;
function XMLoadFloat4(constref pSource: PSingle): TXMVECTOR; overload inline;

function XMLoadFloat4A(const pSource: TXMFLOAT4A): TXMVECTOR; inline;
function XMLoadSInt4(const pSource: TXMINT4): TXMVECTOR; inline;
function XMLoadUInt4(const pSource: TXMUINT4): TXMVECTOR; inline;

function XMLoadFloat3x3(const pSource: TXMFLOAT3X3): TXMMATRIX; inline;
function XMLoadFloat4x3(const pSource: TXMFLOAT4X3): TXMMATRIX; inline;
function XMLoadFloat4x3A(const pSource: TXMFLOAT4X3A): TXMMATRIX; inline;
function XMLoadFloat4x4(const pSource: TXMFLOAT4X4): TXMMATRIX; inline;
function XMLoadFloat4x4A(const pSource: TXMFLOAT4X4A): TXMMATRIX; inline;

{***************************************************************************
 *
 * Store operations
 *
 ***************************************************************************}

procedure XMStoreInt(out pDestination: UINT32; constref V: TXMVECTOR);

procedure XMStoreFloat(out pDestination: single; V: TXMVECTOR);

procedure XMStoreInt2(out pDestination: PUINT32; constref V: TXMVECTOR); // _Out_writes_(2)
procedure XMStoreInt2A(out pDestination: PUINT32; constref V: TXMVECTOR); // _Out_writes_(2)
procedure XMStoreFloat2(out pDestination: TXMFLOAT2; constref V: TXMVECTOR);
procedure XMStoreFloat2A(out pDestination: TXMFLOAT2A; constref V: TXMVECTOR);
procedure XMStoreSInt2(out pDestination: TXMINT2; constref V: TXMVECTOR);
procedure XMStoreUInt2(out pDestination: TXMUINT2; constref V: TXMVECTOR);

procedure XMStoreInt3(out pDestination: PUINT32; constref V: TXMVECTOR); // _Out_writes_(3)
procedure XMStoreInt3A(out pDestination: PUINT32; constref V: TXMVECTOR); // _Out_writes_(3)
procedure XMStoreFloat3(out pDestination: TXMFLOAT3; constref V: TXMVECTOR);
procedure XMStoreFloat3A(out pDestination: TXMFLOAT3A; constref V: TXMVECTOR);
procedure XMStoreSInt3(out pDestination: TXMINT3; constref V: TXMVECTOR);
procedure XMStoreUInt3(out pDestination: TXMUINT3; constref V: TXMVECTOR);

procedure XMStoreInt4(out pDestination: PUINT32; constref V: TXMVECTOR);  // _Out_writes_(4)
procedure XMStoreInt4A(out pDestination: PUINT32; constref V: TXMVECTOR); // _Out_writes_(4)
procedure XMStoreFloat4(out pDestination: TXMFLOAT4; constref V: TXMVECTOR);
procedure XMStoreFloat4A(out pDestination: TXMFLOAT4A; constref V: TXMVECTOR);
procedure XMStoreSInt4(out pDestination: TXMINT4; constref V: TXMVECTOR);
procedure XMStoreUInt4(out pDestination: TXMUINT4; constref V: TXMVECTOR);

procedure XMStoreFloat3x3(out pDestination: TXMFLOAT3X3; constref M: TXMMATRIX);
procedure XMStoreFloat4x3(out pDestination: TXMFLOAT4X3; constref M: TXMMATRIX);
procedure XMStoreFloat4x3A(out pDestination: TXMFLOAT4X3A; constref M: TXMMATRIX);
procedure XMStoreFloat4x4(out pDestination: TXMFLOAT4X4; constref M: TXMMATRIX);
procedure XMStoreFloat4x4A(out pDestination: TXMFLOAT4X4A; constref M: TXMMATRIX);




{***************************************************************************
 *
 * General vector operations
 *
 ***************************************************************************}
function XMVectorZero(): TXMVECTOR;
function XMVectorSet(constref x, y, z, w: single): TXMVECTOR;
function XMVectorSetInt(x, y, z, w: UINT32): TXMVECTOR;
function XMVectorReplicate(constref Value: single): TXMVECTOR;
function XMVectorReplicatePtr(pValue: PSingle): TXMVECTOR;
function XMVectorReplicateInt(constref Value: UINT32): TXMVECTOR;
function XMVectorReplicateIntPtr(pValue: PUINT32): TXMVECTOR;
function XMVectorTrueInt(): TXMVECTOR;
function XMVectorFalseInt(): TXMVECTOR;
function XMVectorSplatX(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSplatY(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSplatZ(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSplatW(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSplatOne(): TXMVECTOR;
function XMVectorSplatInfinity(): TXMVECTOR;
function XMVectorSplatQNaN(): TXMVECTOR;
function XMVectorSplatEpsilon(): TXMVECTOR;
function XMVectorSplatSignMask(): TXMVECTOR;

function XMVectorGetByIndex(V: TXMVECTOR; i: size_t): single;
function XMVectorGetX(constref V: TXMVECTOR): single;
function XMVectorGetY(constref V: TXMVECTOR): single;
function XMVectorGetZ(constref V: TXMVECTOR): single;
function XMVectorGetW(constref V: TXMVECTOR): single;

procedure XMVectorGetByIndexPtr(out f: single; V: TXMVECTOR; i: size_t);
procedure XMVectorGetXPtr(out x: single; constref V: TXMVECTOR);
procedure XMVectorGetYPtr(out y: single; constref V: TXMVECTOR);
procedure XMVectorGetZPtr(out z: single; constref V: TXMVECTOR);
procedure XMVectorGetWPtr(out w: single; constref V: TXMVECTOR);

function XMVectorGetIntByIndex(constref V: TXMVECTOR; constref i: size_t): UINT32;
function XMVectorGetIntX(constref V: TXMVECTOR): UINT32;
function XMVectorGetIntY(constref V: TXMVECTOR): UINT32;
function XMVectorGetIntZ(constref V: TXMVECTOR): UINT32;
function XMVectorGetIntW(constref V: TXMVECTOR): UINT32;

procedure XMVectorGetIntByIndexPtr(out x: UINT32; constref V: TXMVECTOR; constref i: size_t);
procedure XMVectorGetIntXPtr(out x: UINT32; constref V: TXMVECTOR);
procedure XMVectorGetIntYPtr(out y: UINT32; constref V: TXMVECTOR);
procedure XMVectorGetIntZPtr(out z: UINT32; constref V: TXMVECTOR);
procedure XMVectorGetIntWPtr(out w: UINT32; constref V: TXMVECTOR);

function XMVectorSetByIndex(constref V: TXMVECTOR; constref f: single; constref i: size_t): TXMVECTOR;
function XMVectorSetX(constref V: TXMVECTOR; constref x: single): TXMVECTOR;
function XMVectorSetY(constref V: TXMVECTOR; constref y: single): TXMVECTOR;
function XMVectorSetZ(constref V: TXMVECTOR; constref z: single): TXMVECTOR;
function XMVectorSetW(constref V: TXMVECTOR; constref w: single): TXMVECTOR;

function XMVectorSetByIndexPtr(constref V: TXMVECTOR; constref f: Psingle; constref i: size_t): TXMVECTOR;
function XMVectorSetXPtr(constref V: TXMVECTOR; constref x: Psingle): TXMVECTOR;
function XMVectorSetYPtr(constref V: TXMVECTOR; constref y: Psingle): TXMVECTOR;
function XMVectorSetZPtr(constref V: TXMVECTOR; constref z: Psingle): TXMVECTOR;
function XMVectorSetWPtr(constref V: TXMVECTOR; constref w: Psingle): TXMVECTOR;

function XMVectorSetIntByIndex(constref V: TXMVECTOR; constref x: UINT32; constref i: size_t): TXMVECTOR;
function XMVectorSetIntX(constref V: TXMVECTOR; constref x: UINT32): TXMVECTOR;
function XMVectorSetIntY(constref V: TXMVECTOR; constref y: UINT32): TXMVECTOR;
function XMVectorSetIntZ(constref V: TXMVECTOR; constref z: UINT32): TXMVECTOR;
function XMVectorSetIntW(constref V: TXMVECTOR; constref w: UINT32): TXMVECTOR;

function XMVectorSetIntByIndexPtr(constref V: TXMVECTOR; constref x: PUINT32; constref i: size_t): TXMVECTOR;
function XMVectorSetIntXPtr(constref V: TXMVECTOR; constref x: PUINT32): TXMVECTOR;
function XMVectorSetIntYPtr(constref V: TXMVECTOR; constref y: PUINT32): TXMVECTOR;
function XMVectorSetIntZPtr(constref V: TXMVECTOR; constref z: PUINT32): TXMVECTOR;
function XMVectorSetIntWPtr(constref V: TXMVECTOR; constref w: PUINT32): TXMVECTOR;

function XMVectorSwizzle(constref V: TXMVECTOR; constref SwizzleX, SwizzleY, SwizzleZ, SwizzleW: UINT32): TXMVECTOR;
function XMVectorPermute(V1: TXMVECTOR; V2: TXMVECTOR; PermuteX: UINT32; PermuteY: UINT32; PermuteZ: UINT32; PermuteW: UINT32): TXMVECTOR;

function XMVectorSelectControl(constref VectorIndex0: UINT32; constref VectorIndex1: UINT32; constref VectorIndex2: UINT32; constref VectorIndex3: UINT32): TXMVECTOR;
function XMVectorSelect(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Control: TXMVECTOR): TXMVECTOR;
function XMVectorMergeXY(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorMergeZW(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;


function XMVectorShiftLeft(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Elements: UINT32): TXMVECTOR;
function XMVectorRotateLeft(constref V: TXMVECTOR; constref Elements: UINT32): TXMVECTOR;
function XMVectorRotateRight(constref V: TXMVECTOR; constref Elements: UINT32): TXMVECTOR;
function XMVectorInsert(constref VD: TXMVECTOR; constref VS: TXMVECTOR; constref VSLeftRotateElements: UINT32; constref Select0: UINT32; constref Select1: UINT32; constref Select2: UINT32; constref Select3: UINT32): TXMVECTOR;

function XMVectorEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorEqualIntR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorNearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): TXMVECTOR; inline;
function XMVectorNotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorNotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorGreater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorGreaterR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorGreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorGreaterOrEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorLess(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorLessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorInBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR; inline;
function XMVectorInBoundsR(out pCR: UINT32; constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR; inline;

function XMVectorIsNaN(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorIsInfinite(constref V: TXMVECTOR): TXMVECTOR;

function XMVectorMin(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorMax(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorRound(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorTruncate(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorFloor(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorCeiling(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorClamp(constref V: TXMVECTOR; constref Min: TXMVECTOR; constref Max: TXMVECTOR): TXMVECTOR;
function XMVectorSaturate(constref V: TXMVECTOR): TXMVECTOR;

function XMVectorAndInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorAndCInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorOrInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorNorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorXorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;

function XMVectorNegate(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorSum(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorAddAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorSubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorSubtractAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorMultiply(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorMultiplyAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
function XMVectorDivide(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorNegativeMultiplySubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
function XMVectorScale(constref V: TXMVECTOR; constref ScaleFactor: single): TXMVECTOR;
function XMVectorReciprocalEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorReciprocal(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSqrtEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSqrt(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorReciprocalSqrtEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorReciprocalSqrt(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorExp2(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorExpE(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorExp(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorLog2(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorLogE(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorLog(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorPow(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorAbs(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorMod(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVectorModAngles(constref Angles: TXMVECTOR): TXMVECTOR;
function XMVectorSin(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSinEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorCos(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorCosEst(constref V: TXMVECTOR): TXMVECTOR;
procedure XMVectorSinCos(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR);
procedure XMVectorSinCosEst(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR);
function XMVectorTan(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorTanEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorSinH(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorCosH(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorTanH(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorASin(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorASinEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorACos(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorACosEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorATan(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorATanEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVectorATan2(constref Y: TXMVECTOR; constref X: TXMVECTOR): TXMVECTOR;
function XMVectorATan2Est(constref Y: TXMVECTOR; constref X: TXMVECTOR): TXMVECTOR;
function XMVectorLerp(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref t: single): TXMVECTOR;
function XMVectorLerpV(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR;
function XMVectorHermite(constref Position0: TXMVECTOR; constref Tangent0: TXMVECTOR; constref Position1: TXMVECTOR; constref Tangent1: TXMVECTOR; constref t: single): TXMVECTOR;
function XMVectorHermiteV(constref Position0: TXMVECTOR; constref Tangent0: TXMVECTOR; constref Position1: TXMVECTOR; constref Tangent1: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR;
function XMVectorCatmullRom(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref Position3: TXMVECTOR; constref t: single): TXMVECTOR;
function XMVectorCatmullRomV(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref Position3: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR;
function XMVectorBaryCentric(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref f: single; constref g: single): TXMVECTOR;
function XMVectorBaryCentricV(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref F: TXMVECTOR; constref G: TXMVECTOR): TXMVECTOR;



{***************************************************************************
 *
 * 2D vector operations
 *
 ***************************************************************************}

function XMVector2Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector2EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector2NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
function XMVector2NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector2GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector2Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector2InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;

function XMVector2IsNaN(constref V: TXMVECTOR): boolean;
function XMVector2IsInfinite(constref V: TXMVECTOR): boolean;

function XMVector2Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVector2Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVector2LengthSq(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2LengthEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2Length(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2Normalize(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2ClampLength(constref V: TXMVECTOR; constref LengthMin: single; constref LengthMax: single): TXMVECTOR;
function XMVector2ClampLengthV(constref V: TXMVECTOR; constref LengthMin: TXMVECTOR; constref LengthMax: TXMVECTOR): TXMVECTOR;
function XMVector2Reflect(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR): TXMVECTOR;
function XMVector2Refract(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: single): TXMVECTOR;
function XMVector2RefractV(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: TXMVECTOR): TXMVECTOR;
function XMVector2Orthogonal(constref V: TXMVECTOR): TXMVECTOR;
function XMVector2AngleBetweenNormalsEst(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
function XMVector2AngleBetweenNormals(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
function XMVector2AngleBetweenVectors(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVector2LinePointDistance(constref LinePoint1: TXMVECTOR; constref LinePoint2: TXMVECTOR; constref Point: TXMVECTOR): TXMVECTOR;
function XMVector2IntersectLine(constref Line1Point1: TXMVECTOR; constref Line1Point2: TXMVECTOR; constref Line2Point1: TXMVECTOR; constref Line2Point2: TXMVECTOR): TXMVECTOR;
function XMVector2Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
function XMVector2TransformStream(out pOutputStream: PXMFLOAT4; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT4;
function XMVector2TransformCoord(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
function XMVector2TransformCoordStream(out pOutputStream: PXMFLOAT2; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT2;
function XMVector2TransformNormal(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
function XMVector2TransformNormalStream(out pOutputStream: PXMFLOAT2; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT2;


{***************************************************************************
 *
 * 3D vector operations
 *
 ***************************************************************************}

function XMVector3Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector3EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector3NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
function XMVector3NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector3GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector3Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector3InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;

function XMVector3IsNaN(constref V: TXMVECTOR): boolean;
function XMVector3IsInfinite(constref V: TXMVECTOR): boolean;

function XMVector3Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVector3Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVector3LengthSq(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3LengthEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3Length(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3Normalize(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3ClampLength(constref V: TXMVECTOR; constref LengthMin: single; constref LengthMax: single): TXMVECTOR;
function XMVector3ClampLengthV(constref V: TXMVECTOR; constref LengthMin: TXMVECTOR; constref LengthMax: TXMVECTOR): TXMVECTOR;
function XMVector3Reflect(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR): TXMVECTOR;
function XMVector3Refract(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: single): TXMVECTOR;
function XMVector3RefractV(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: TXMVECTOR): TXMVECTOR;
function XMVector3Orthogonal(constref V: TXMVECTOR): TXMVECTOR;
function XMVector3AngleBetweenNormalsEst(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
function XMVector3AngleBetweenNormals(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
function XMVector3AngleBetweenVectors(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVector3LinePointDistance(constref LinePoint1: TXMVECTOR; constref LinePoint2: TXMVECTOR; constref Point: TXMVECTOR): TXMVECTOR;
procedure XMVector3ComponentsFromNormal(out pParallel: TXMVECTOR; out pPerpendicular: TXMVECTOR; constref V: TXMVECTOR; constref Normal: TXMVECTOR);
function XMVector3Rotate(constref V: TXMVECTOR; constref RotationQuaternion: TXMVECTOR): TXMVECTOR;
function XMVector3InverseRotate(constref V: TXMVECTOR; constref RotationQuaternion: TXMVECTOR): TXMVECTOR;
function XMVector3Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
function XMVector3TransformStream(out pOutputStream: PXMFLOAT4; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT4;
function XMVector3TransformCoord(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
function XMVector3TransformCoordStream(out pOutputStream: PXMFLOAT3; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT3;
function XMVector3TransformNormal(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
function XMVector3TransformNormalStream(out pOutputStream: PXMFLOAT3; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT3;
function XMVector3Project(V: TXMVECTOR; ViewportX: single; ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX;
    View: TXMMATRIX; World: TXMMATRIX): TXMVECTOR;
function XMVector3ProjectStream(out pOutputStream: PXMFLOAT3; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t;
    constref ViewportX: single; constref ViewportY: single; constref ViewportWidth: single; constref ViewportHeight: single; constref ViewportMinZ: single; constref ViewportMaxZ: single;
    constref Projection: TXMMATRIX; constref View: TXMMATRIX; constref World: TXMMATRIX): PXMFLOAT3; inline;

function XMVector3Unproject(V: TXMVECTOR; ViewportX: single; ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX;
    View: TXMMATRIX; World: TXMMATRIX): TXMVECTOR;


function XMVector3UnprojectStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; constref pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; ViewportX: single;
    ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX; View: TXMMATRIX; World: TXMMATRIX): PXMFLOAT3;

{***************************************************************************
 *
 * 4D vector operations
 *
 ***************************************************************************}

function XMVector4Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector4EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector4NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
function XMVector4NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector4GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
function XMVector4Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
function XMVector4InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;

function XMVector4IsNaN(constref V: TXMVECTOR): boolean;
function XMVector4IsInfinite(constref V: TXMVECTOR): boolean;

function XMVector4Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
function XMVector4Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
function XMVector4LengthSq(constref V: TXMVECTOR): TXMVECTOR;
function XMVector4ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector4ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
function XMVector4LengthEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector4Length(constref V: TXMVECTOR): TXMVECTOR;
function XMVector4NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
function XMVector4Normalize(constref V: TXMVECTOR): TXMVECTOR;
function XMVector4ClampLength(V: TXMVECTOR; LengthMin: single; LengthMax: single): TXMVECTOR;
function XMVector4ClampLengthV(V: TXMVECTOR; LengthMin: TXMVECTOR; LengthMax: TXMVECTOR): TXMVECTOR;
function XMVector4Reflect(Incident: TXMVECTOR; Normal: TXMVECTOR): TXMVECTOR;
function XMVector4Refract(Incident: TXMVECTOR; Normal: TXMVECTOR; RefractionIndex: single): TXMVECTOR;
function XMVector4RefractV(Incident: TXMVECTOR; Normal: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR;
function XMVector4Orthogonal(constref V: TXMVECTOR): TXMVECTOR; inline;
function XMVector4AngleBetweenNormalsEst(N1: TXMVECTOR; N2: TXMVECTOR): TXMVECTOR; inline;
function XMVector4AngleBetweenNormals(N1: TXMVECTOR; N2: TXMVECTOR): TXMVECTOR; inline;
function XMVector4AngleBetweenVectors(V1: TXMVECTOR; V2: TXMVECTOR): TXMVECTOR; inline;
function XMVector4Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
function XMVector4TransformStream(out pOutputStream: PXMFLOAT4; constref OutputStride: size_t; constref pInputStream: PXMFLOAT4; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT4;

{***************************************************************************
 *
 * Matrix operations
 *
 ***************************************************************************}

function XMMatrixIsNaN(constref M: TXMMATRIX): boolean;
function XMMatrixIsInfinite(M: TXMMATRIX): boolean;
function XMMatrixIsIdentity(M: TXMMATRIX): boolean;

function XMMatrixMultiply(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX;
function XMMatrixMultiplyTranspose(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX;
function XMMatrixTranspose(M: TXMMATRIX): TXMMATRIX;
function XMMatrixInverse(out pDeterminant: TXMVECTOR; M: TXMMATRIX): TXMMATRIX;
function XMMatrixDeterminant(M: TXMMATRIX): TXMVECTOR;
function XMMatrixDecompose(out outScale: TXMVECTOR; out outRotQuat: TXMVECTOR; out outTrans: TXMVECTOR; M: TXMMATRIX): boolean;

function XMMatrixIdentity(): TXMMATRIX;
function XMMatrixSet(m00: single; m01: single; m02: single; m03: single; m10: single; m11: single; m12: single; m13: single; m20: single; m21: single; m22: single; m23: single;
    m30: single; m31: single; m32: single; m33: single): TXMMATRIX;
function XMMatrixTranslation(OffsetX: single; OffsetY: single; OffsetZ: single): TXMMATRIX;
function XMMatrixTranslationFromVector(Offset: TXMVECTOR): TXMMATRIX;
function XMMatrixScaling(ScaleX: single; ScaleY: single; ScaleZ: single): TXMMATRIX;
function XMMatrixScalingFromVector(constref Scale: TXMVECTOR): TXMMATRIX;
function XMMatrixRotationX(Angle: single): TXMMATRIX;
function XMMatrixRotationY(Angle: single): TXMMATRIX;
function XMMatrixRotationZ(Angle: single): TXMMATRIX;
function XMMatrixRotationRollPitchYaw(Pitch: single; Yaw: single; Roll: single): TXMMATRIX;
function XMMatrixRotationRollPitchYawFromVector(Angles: TXMVECTOR): TXMMATRIX;
function XMMatrixRotationNormal(constref NormalAxis: TXMVECTOR; constref Angle: single): TXMMATRIX;
function XMMatrixRotationAxis(Axis: TXMVECTOR; Angle: single): TXMMATRIX;
function XMMatrixRotationQuaternion(Quaternion: TXMVECTOR): TXMMATRIX;
function XMMatrixTransformation2D(ScalingOrigin: TXMVECTOR; ScalingOrientation: single; Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; Rotation: single; Translation: TXMVECTOR): TXMMATRIX;
function XMMatrixTransformation(ScalingOrigin: TXMVECTOR; ScalingOrientationQuaternion: TXMVECTOR; Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; RotationQuaternion: TXMVECTOR; Translation: TXMVECTOR): TXMMATRIX;
function XMMatrixAffineTransformation2D(Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; Rotation: single; Translation: TXMVECTOR): TXMMATRIX;
function XMMatrixAffineTransformation(Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; RotationQuaternion: TXMVECTOR; Translation: TXMVECTOR): TXMMATRIX;
function XMMatrixReflect(ReflectionPlane: TXMVECTOR): TXMMATRIX;
function XMMatrixShadow(ShadowPlane: TXMVECTOR; LightPosition: TXMVECTOR): TXMMATRIX;

function XMMatrixLookAtLH(EyePosition: TXMVECTOR; FocusPosition: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
function XMMatrixLookAtRH(EyePosition: TXMVECTOR; FocusPosition: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
function XMMatrixLookToLH(EyePosition: TXMVECTOR; EyeDirection: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
function XMMatrixLookToRH(EyePosition: TXMVECTOR; EyeDirection: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
function XMMatrixPerspectiveLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixPerspectiveRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixPerspectiveFovLH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixPerspectiveFovRH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixPerspectiveOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixPerspectiveOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixOrthographicLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixOrthographicRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixOrthographicOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
function XMMatrixOrthographicOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;


{***************************************************************************
 *
 * Quaternion operations
 *
 ***************************************************************************}

function XMQuaternionEqual(Q1: TXMVECTOR; Q2: TXMVECTOR): boolean;
function XMQuaternionNotEqual(Q1: TXMVECTOR; Q2: TXMVECTOR): boolean;

function XMQuaternionIsNaN(Q: TXMVECTOR): boolean;
function XMQuaternionIsInfinite(Q: TXMVECTOR): boolean;
function XMQuaternionIsIdentity(Q: TXMVECTOR): boolean;

function XMQuaternionDot(Q1: TXMVECTOR; Q2: TXMVECTOR): TXMVECTOR;
function XMQuaternionMultiply(Q1: TXMVECTOR; Q2: TXMVECTOR): TXMVECTOR;
function XMQuaternionLengthSq(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionReciprocalLength(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionLength(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionNormalizeEst(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionNormalize(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionConjugate(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionInverse(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionLn(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionExp(Q: TXMVECTOR): TXMVECTOR;
function XMQuaternionSlerp(Q0: TXMVECTOR; Q1: TXMVECTOR; t: single): TXMVECTOR;
function XMQuaternionSlerpV(Q0: TXMVECTOR; Q1: TXMVECTOR; T: TXMVECTOR): TXMVECTOR;
function XMQuaternionSquad(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; Q3: TXMVECTOR; t: single): TXMVECTOR;
function XMQuaternionSquadV(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; Q3: TXMVECTOR; T: TXMVECTOR): TXMVECTOR;
procedure XMQuaternionSquadSetup(out pA: TXMVECTOR; out pB: TXMVECTOR; out pC: TXMVECTOR; Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; Q3: TXMVECTOR);
function XMQuaternionBaryCentric(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; f: single; g: single): TXMVECTOR;
function XMQuaternionBaryCentricV(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; F: TXMVECTOR; G: TXMVECTOR): TXMVECTOR;

function XMQuaternionIdentity(): TXMVECTOR;
function XMQuaternionRotationRollPitchYaw(Pitch: single; Yaw: single; Roll: single): TXMVECTOR;
function XMQuaternionRotationRollPitchYawFromVector(Angles: TXMVECTOR): TXMVECTOR;
function XMQuaternionRotationNormal(NormalAxis: TXMVECTOR; Angle: single): TXMVECTOR;
function XMQuaternionRotationAxis(Axis: TXMVECTOR; Angle: single): TXMVECTOR;
function XMQuaternionRotationMatrix(M: TXMMATRIX): TXMVECTOR;

procedure XMQuaternionToAxisAngle(out pAxis: TXMVECTOR; out pAngle: single; Q: TXMVECTOR);

{***************************************************************************
 *
 * Plane operations
 *
 ***************************************************************************}

function XMPlaneEqual(P1: TXMVECTOR; P2: TXMVECTOR): boolean;
function XMPlaneNearEqual(P1: TXMVECTOR; P2: TXMVECTOR; Epsilon: TXMVECTOR): boolean;
function XMPlaneNotEqual(P1: TXMVECTOR; P2: TXMVECTOR): boolean;

function XMPlaneIsNaN(P: TXMVECTOR): boolean;
function XMPlaneIsInfinite(P: TXMVECTOR): boolean;

function XMPlaneDot(P: TXMVECTOR; V: TXMVECTOR): TXMVECTOR;
function XMPlaneDotCoord(P: TXMVECTOR; V: TXMVECTOR): TXMVECTOR;
function XMPlaneDotNormal(P: TXMVECTOR; V: TXMVECTOR): TXMVECTOR;
function XMPlaneNormalizeEst(P: TXMVECTOR): TXMVECTOR;
function XMPlaneNormalize(P: TXMVECTOR): TXMVECTOR;
function XMPlaneIntersectLine(P: TXMVECTOR; LinePoint1: TXMVECTOR; LinePoint2: TXMVECTOR): TXMVECTOR;
procedure XMPlaneIntersectPlane(out pLinePoint1: TXMVECTOR; out pLinePoint2: TXMVECTOR; P1: TXMVECTOR; P2: TXMVECTOR);
function XMPlaneTransform(P: TXMVECTOR; M: TXMMATRIX): TXMVECTOR;
function XMPlaneTransformStream(out pOutputStream: PXMFLOAT4; OutputStride: size_t; constref pInputStream: PXMFLOAT4; InputStride: size_t; PlaneCount: size_t; M: TXMMATRIX): PXMFLOAT4;

function XMPlaneFromPointNormal(Point: TXMVECTOR; Normal: TXMVECTOR): TXMVECTOR;
function XMPlaneFromPoints(Point1: TXMVECTOR; Point2: TXMVECTOR; Point3: TXMVECTOR): TXMVECTOR;

{***************************************************************************
 *
 * Color operations
 *
 ***************************************************************************}

function XMColorEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean;
function XMColorNotEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean;
function XMColorGreater(C1: TXMVECTOR; C2: TXMVECTOR): boolean;
function XMColorGreaterOrEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean;
function XMColorLess(C1: TXMVECTOR; C2: TXMVECTOR): boolean;
function XMColorLessOrEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean;

function XMColorIsNaN(C: TXMVECTOR): boolean;
function XMColorIsInfinite(C: TXMVECTOR): boolean;

function XMColorNegative(constref vColor: TXMVECTOR): TXMVECTOR;
function XMColorModulate(C1: TXMVECTOR; C2: TXMVECTOR): TXMVECTOR;
function XMColorAdjustSaturation(constref vColor: TXMVECTOR; Saturation: single): TXMVECTOR;
function XMColorAdjustContrast(constref vColor: TXMVECTOR; constref Contrast: single): TXMVECTOR;

function XMColorRGBToHSL(rgb: TXMVECTOR): TXMVECTOR;
function XMColorHSLToRGB(hsl: TXMVECTOR): TXMVECTOR;

function XMColorRGBToHSV(rgb: TXMVECTOR): TXMVECTOR;
function XMColorHSVToRGB(hsv: TXMVECTOR): TXMVECTOR;

function XMColorRGBToYUV(rgb: TXMVECTOR): TXMVECTOR;
function XMColorYUVToRGB(yuv: TXMVECTOR): TXMVECTOR;

function XMColorRGBToYUV_HD(rgb: TXMVECTOR): TXMVECTOR;
function XMColorYUVToRGB_HD(yuv: TXMVECTOR): TXMVECTOR;

function XMColorRGBToXYZ(rgb: TXMVECTOR): TXMVECTOR;
function XMColorXYZToRGB(xyz: TXMVECTOR): TXMVECTOR;

function XMColorXYZToSRGB(xyz: TXMVECTOR): TXMVECTOR;
function XMColorSRGBToXYZ(srgb: TXMVECTOR): TXMVECTOR;

function XMColorRGBToSRGB(rgb: TXMVECTOR): TXMVECTOR;
function XMColorSRGBToRGB(srgb: TXMVECTOR): TXMVECTOR;


{***************************************************************************
 *
 * Miscellaneous operations
 *
 ***************************************************************************}

function XMVerifyCPUSupport(CPURequired: TCPUType): boolean;

function XMFresnelTerm(CosIncidentAngle: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR;

function XMScalarNearEqual(S1: single; S2: single; Epsilon: single): boolean;
function XMScalarModAngle(Angle: single): single;

function XMScalarSin(Value: single): single;
function XMScalarSinEst(Value: single): single;

function XMScalarCos(Value: single): single;
function XMScalarCosEst(Value: single): single;

procedure XMScalarSinCos(out pSin: single; out pCos: single; Value: single);
procedure XMScalarSinCosEst(out pSin: single; out pCos: single; Value: single);

function XMScalarASin(Value: single): single;
function XMScalarASinEst(Value: single): single;

function XMScalarACos(Value: single): single;
function XMScalarACosEst(Value: single): single;

// template<class T> inline T XMMin(T a, T b) { return (a < b) ? a : b; }
// template<class T> inline T XMMax(T a, T b) { return (a > b) ? a : b; }
function XMMin(a, b: single): single;
function XMMax(a, b: single): single;


{****************************************************************************
 *
 * Templates
 *
 ****************************************************************************}

// Special-case permute templates
function XMVectorPermute_0X_0Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_1Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_0Y_1X_1Y(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1Z_1W_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_1X_0Y_1Y(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0Z_1Z_0W_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0Z_0W_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_0Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_1Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_1Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_0Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_0Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_1Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_1Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_0Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_0Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_1Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_1Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_0Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_1X_0Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;
function XMVectorPermute_0X_1Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; inline;


// Specialized swizzles
function XMVectorSwizzle_X_Y_Z_W(V: TXMVECTOR): TXMVECTOR; inline;
function XMVectorSwizzle_X_Y_X_Y(V: TXMVECTOR): TXMVECTOR; inline;
function XMVectorSwizzle_Z_W_Z_W(V: TXMVECTOR): TXMVECTOR; inline;
function XMVectorSwizzle_X_X_Y_Y(V: TXMVECTOR): TXMVECTOR; inline;
function XMVectorSwizzle_Z_Z_W_W(V: TXMVECTOR): TXMVECTOR; inline;
function XMVectorSwizzle_X_X_Z_Z(V: TXMVECTOR): TXMVECTOR; inline;
function XMVectorSwizzle_Y_Y_W_W(V: TXMVECTOR): TXMVECTOR; inline;
function XMVectorSwizzle_X_X_X_X(V: TXMVECTOR): TXMVECTOR; inline;

function _MM_SHUFFLE(z, y, x, w: byte): byte; inline;

{****************************************************************************
 *
 * Conditional intrinsics
 *
****************************************************************************}
// this function is normally done by ASM directly, FPC don't have inefficiency intrinsic >:)
// but for some functions we need this anyway ...
function XM_PERMUTE_PS(v: TXMVECTOR; constref c: byte): TXMVECTOR;

implementation

uses
    Math;

const
    { Define all possible Shuffle operations }
    // _MM_SHUFFLE_z_y_x_w
    // 0,0
    _MM_SHUFFLE_0_0_0_0 = (0 shl 6) or (0 shl 4) or (0 shl 2) or 0; // $00
    _MM_SHUFFLE_0_0_0_1 = (0 shl 6) or (0 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_0_0_0_2 = (0 shl 6) or (0 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_0_0_0_3 = (0 shl 6) or (0 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_0_0_1_0 = (0 shl 6) or (0 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_0_0_1_1 = (0 shl 6) or (0 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_0_0_1_2 = (0 shl 6) or (0 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_0_0_1_3 = (0 shl 6) or (0 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_0_0_2_0 = (0 shl 6) or (0 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_0_0_2_1 = (0 shl 6) or (0 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_0_0_2_2 = (0 shl 6) or (0 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_0_0_2_3 = (0 shl 6) or (0 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_0_0_3_0 = (0 shl 6) or (0 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_0_0_3_1 = (0 shl 6) or (0 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_0_0_3_2 = (0 shl 6) or (0 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_0_0_3_3 = (0 shl 6) or (0 shl 4) or (3 shl 2) or 3; // $

    // 0,1
    _MM_SHUFFLE_0_1_0_0 = (0 shl 6) or (1 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_0_1_0_1 = (0 shl 6) or (1 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_0_1_0_2 = (0 shl 6) or (1 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_0_1_0_3 = (0 shl 6) or (1 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_0_1_1_0 = (0 shl 6) or (1 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_0_1_1_1 = (0 shl 6) or (1 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_0_1_1_2 = (0 shl 6) or (1 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_0_1_1_3 = (0 shl 6) or (1 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_0_1_2_0 = (0 shl 6) or (1 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_0_1_2_1 = (0 shl 6) or (1 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_0_1_2_2 = (0 shl 6) or (1 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_0_1_2_3 = (0 shl 6) or (1 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_0_1_3_0 = (0 shl 6) or (1 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_0_1_3_1 = (0 shl 6) or (1 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_0_1_3_2 = (0 shl 6) or (1 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_0_1_3_3 = (0 shl 6) or (1 shl 4) or (3 shl 2) or 3; // $

    // 0,2
    _MM_SHUFFLE_0_2_0_0 = (0 shl 6) or (2 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_0_2_0_1 = (0 shl 6) or (2 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_0_2_0_2 = (0 shl 6) or (2 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_0_2_0_3 = (0 shl 6) or (2 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_0_2_1_0 = (0 shl 6) or (2 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_0_2_1_1 = (0 shl 6) or (2 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_0_2_1_2 = (0 shl 6) or (2 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_0_2_1_3 = (0 shl 6) or (2 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_0_2_2_0 = (0 shl 6) or (2 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_0_2_2_1 = (0 shl 6) or (2 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_0_2_2_2 = (0 shl 6) or (2 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_0_2_2_3 = (0 shl 6) or (2 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_0_2_3_0 = (0 shl 6) or (2 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_0_2_3_1 = (0 shl 6) or (2 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_0_2_3_2 = (0 shl 6) or (2 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_0_2_3_3 = (0 shl 6) or (2 shl 4) or (3 shl 2) or 3; // $

    // 0,3
    _MM_SHUFFLE_0_3_0_0 = (0 shl 6) or (3 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_0_3_0_1 = (0 shl 6) or (3 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_0_3_0_2 = (0 shl 6) or (3 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_0_3_0_3 = (0 shl 6) or (3 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_0_3_1_0 = (0 shl 6) or (3 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_0_3_1_1 = (0 shl 6) or (3 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_0_3_1_2 = (0 shl 6) or (3 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_0_3_1_3 = (0 shl 6) or (3 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_0_3_2_0 = (0 shl 6) or (3 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_0_3_2_1 = (0 shl 6) or (3 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_0_3_2_2 = (0 shl 6) or (3 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_0_3_2_3 = (0 shl 6) or (3 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_0_3_3_0 = (0 shl 6) or (3 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_0_3_3_1 = (0 shl 6) or (3 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_0_3_3_2 = (0 shl 6) or (3 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_0_3_3_3 = (0 shl 6) or (3 shl 4) or (3 shl 2) or 3; // $

    // 1,0
    _MM_SHUFFLE_1_0_0_0 = (1 shl 6) or (0 shl 4) or (0 shl 2) or 0; // $00
    _MM_SHUFFLE_1_0_0_1 = (1 shl 6) or (0 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_1_0_0_2 = (1 shl 6) or (0 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_1_0_0_3 = (1 shl 6) or (0 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_1_0_1_0 = (1 shl 6) or (0 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_1_0_1_1 = (1 shl 6) or (0 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_1_0_1_2 = (1 shl 6) or (0 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_1_0_1_3 = (1 shl 6) or (0 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_1_0_2_0 = (1 shl 6) or (0 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_1_0_2_1 = (1 shl 6) or (0 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_1_0_2_2 = (1 shl 6) or (0 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_1_0_2_3 = (1 shl 6) or (0 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_1_0_3_0 = (1 shl 6) or (0 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_1_0_3_1 = (1 shl 6) or (0 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_1_0_3_2 = (1 shl 6) or (0 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_1_0_3_3 = (1 shl 6) or (0 shl 4) or (3 shl 2) or 3; // $

    // 1,1
    _MM_SHUFFLE_1_1_0_0 = (1 shl 6) or (1 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_1_1_0_1 = (1 shl 6) or (1 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_1_1_0_2 = (1 shl 6) or (1 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_1_1_0_3 = (1 shl 6) or (1 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_1_1_1_0 = (1 shl 6) or (1 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_1_1_1_1 = (1 shl 6) or (1 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_1_1_1_2 = (1 shl 6) or (1 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_1_1_1_3 = (1 shl 6) or (1 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_1_1_2_0 = (1 shl 6) or (1 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_1_1_2_1 = (1 shl 6) or (1 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_1_1_2_2 = (1 shl 6) or (1 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_1_1_2_3 = (1 shl 6) or (1 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_1_1_3_0 = (1 shl 6) or (1 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_1_1_3_1 = (1 shl 6) or (1 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_1_1_3_2 = (1 shl 6) or (1 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_1_1_3_3 = (1 shl 6) or (1 shl 4) or (3 shl 2) or 3; // $

    // 1,2
    _MM_SHUFFLE_1_2_0_0 = (1 shl 6) or (2 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_1_2_0_1 = (1 shl 6) or (2 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_1_2_0_2 = (1 shl 6) or (2 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_1_2_0_3 = (1 shl 6) or (2 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_1_2_1_0 = (1 shl 6) or (2 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_1_2_1_1 = (1 shl 6) or (2 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_1_2_1_2 = (1 shl 6) or (2 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_1_2_1_3 = (1 shl 6) or (2 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_1_2_2_0 = (1 shl 6) or (2 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_1_2_2_1 = (1 shl 6) or (2 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_1_2_2_2 = (1 shl 6) or (2 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_1_2_2_3 = (1 shl 6) or (2 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_1_2_3_0 = (1 shl 6) or (2 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_1_2_3_1 = (1 shl 6) or (2 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_1_2_3_2 = (1 shl 6) or (2 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_1_2_3_3 = (1 shl 6) or (2 shl 4) or (3 shl 2) or 3; // $

    // 1,3
    _MM_SHUFFLE_1_3_0_0 = (1 shl 6) or (3 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_1_3_0_1 = (1 shl 6) or (3 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_1_3_0_2 = (1 shl 6) or (3 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_1_3_0_3 = (1 shl 6) or (3 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_1_3_1_0 = (1 shl 6) or (3 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_1_3_1_1 = (1 shl 6) or (3 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_1_3_1_2 = (1 shl 6) or (3 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_1_3_1_3 = (1 shl 6) or (3 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_1_3_2_0 = (1 shl 6) or (3 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_1_3_2_1 = (1 shl 6) or (3 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_1_3_2_2 = (1 shl 6) or (3 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_1_3_2_3 = (1 shl 6) or (3 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_1_3_3_0 = (1 shl 6) or (3 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_1_3_3_1 = (1 shl 6) or (3 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_1_3_3_2 = (1 shl 6) or (3 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_1_3_3_3 = (1 shl 6) or (3 shl 4) or (3 shl 2) or 3; // $


    // 2,0
    _MM_SHUFFLE_2_0_0_0 = (2 shl 6) or (0 shl 4) or (0 shl 2) or 0; // $00
    _MM_SHUFFLE_2_0_0_1 = (2 shl 6) or (0 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_2_0_0_2 = (2 shl 6) or (0 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_2_0_0_3 = (2 shl 6) or (0 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_2_0_1_0 = (2 shl 6) or (0 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_2_0_1_1 = (2 shl 6) or (0 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_2_0_1_2 = (2 shl 6) or (0 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_2_0_1_3 = (2 shl 6) or (0 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_2_0_2_0 = (2 shl 6) or (0 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_2_0_2_1 = (2 shl 6) or (0 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_2_0_2_2 = (2 shl 6) or (0 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_2_0_2_3 = (2 shl 6) or (0 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_2_0_3_0 = (2 shl 6) or (0 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_2_0_3_1 = (2 shl 6) or (0 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_2_0_3_2 = (2 shl 6) or (0 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_2_0_3_3 = (2 shl 6) or (0 shl 4) or (3 shl 2) or 3; // $

    // 2,1
    _MM_SHUFFLE_2_1_0_0 = (2 shl 6) or (1 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_2_1_0_1 = (2 shl 6) or (1 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_2_1_0_2 = (2 shl 6) or (1 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_2_1_0_3 = (2 shl 6) or (1 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_2_1_1_0 = (2 shl 6) or (1 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_2_1_1_1 = (2 shl 6) or (1 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_2_1_1_2 = (2 shl 6) or (1 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_2_1_1_3 = (2 shl 6) or (1 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_2_1_2_0 = (2 shl 6) or (1 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_2_1_2_1 = (2 shl 6) or (1 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_2_1_2_2 = (2 shl 6) or (1 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_2_1_2_3 = (2 shl 6) or (1 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_2_1_3_0 = (2 shl 6) or (1 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_2_1_3_1 = (2 shl 6) or (1 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_2_1_3_2 = (2 shl 6) or (1 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_2_1_3_3 = (2 shl 6) or (1 shl 4) or (3 shl 2) or 3; // $

    // 2,2
    _MM_SHUFFLE_2_2_0_0 = (2 shl 6) or (2 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_2_2_0_1 = (2 shl 6) or (2 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_2_2_0_2 = (2 shl 6) or (2 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_2_2_0_3 = (2 shl 6) or (2 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_2_2_1_0 = (2 shl 6) or (2 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_2_2_1_1 = (2 shl 6) or (2 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_2_2_1_2 = (2 shl 6) or (2 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_2_2_1_3 = (2 shl 6) or (2 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_2_2_2_0 = (2 shl 6) or (2 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_2_2_2_1 = (2 shl 6) or (2 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_2_2_2_2 = (2 shl 6) or (2 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_2_2_2_3 = (2 shl 6) or (2 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_2_2_3_0 = (2 shl 6) or (2 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_2_2_3_1 = (2 shl 6) or (2 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_2_2_3_2 = (2 shl 6) or (2 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_2_2_3_3 = (2 shl 6) or (2 shl 4) or (3 shl 2) or 3; // $

    // 2,3
    _MM_SHUFFLE_2_3_0_0 = (2 shl 6) or (3 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_2_3_0_1 = (2 shl 6) or (3 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_2_3_0_2 = (2 shl 6) or (3 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_2_3_0_3 = (2 shl 6) or (3 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_2_3_1_0 = (2 shl 6) or (3 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_2_3_1_1 = (2 shl 6) or (3 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_2_3_1_2 = (2 shl 6) or (3 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_2_3_1_3 = (2 shl 6) or (3 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_2_3_2_0 = (2 shl 6) or (3 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_2_3_2_1 = (2 shl 6) or (3 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_2_3_2_2 = (2 shl 6) or (3 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_2_3_2_3 = (2 shl 6) or (3 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_2_3_3_0 = (2 shl 6) or (3 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_2_3_3_1 = (2 shl 6) or (3 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_2_3_3_2 = (2 shl 6) or (3 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_2_3_3_3 = (2 shl 6) or (3 shl 4) or (3 shl 2) or 3; // $


    // 3,0
    _MM_SHUFFLE_3_0_0_0 = (3 shl 6) or (0 shl 4) or (0 shl 2) or 0; // $00
    _MM_SHUFFLE_3_0_0_1 = (3 shl 6) or (0 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_3_0_0_2 = (3 shl 6) or (0 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_3_0_0_3 = (3 shl 6) or (0 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_3_0_1_0 = (3 shl 6) or (0 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_3_0_1_1 = (3 shl 6) or (0 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_3_0_1_2 = (3 shl 6) or (0 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_3_0_1_3 = (3 shl 6) or (0 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_3_0_2_0 = (3 shl 6) or (0 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_3_0_2_1 = (3 shl 6) or (0 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_3_0_2_2 = (3 shl 6) or (0 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_3_0_2_3 = (3 shl 6) or (0 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_3_0_3_0 = (3 shl 6) or (0 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_3_0_3_1 = (3 shl 6) or (0 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_3_0_3_2 = (3 shl 6) or (0 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_3_0_3_3 = (3 shl 6) or (0 shl 4) or (3 shl 2) or 3; // $

    // 3,1
    _MM_SHUFFLE_3_1_0_0 = (3 shl 6) or (1 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_3_1_0_1 = (3 shl 6) or (1 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_3_1_0_2 = (3 shl 6) or (1 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_3_1_0_3 = (3 shl 6) or (1 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_3_1_1_0 = (3 shl 6) or (1 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_3_1_1_1 = (3 shl 6) or (1 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_3_1_1_2 = (3 shl 6) or (1 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_3_1_1_3 = (3 shl 6) or (1 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_3_1_2_0 = (3 shl 6) or (1 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_3_1_2_1 = (3 shl 6) or (1 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_3_1_2_2 = (3 shl 6) or (1 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_3_1_2_3 = (3 shl 6) or (1 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_3_1_3_0 = (3 shl 6) or (1 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_3_1_3_1 = (3 shl 6) or (1 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_3_1_3_2 = (3 shl 6) or (1 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_3_1_3_3 = (3 shl 6) or (1 shl 4) or (3 shl 2) or 3; // $

    // 3,2
    _MM_SHUFFLE_3_2_0_0 = (3 shl 6) or (2 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_3_2_0_1 = (3 shl 6) or (2 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_3_2_0_2 = (3 shl 6) or (2 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_3_2_0_3 = (3 shl 6) or (2 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_3_2_1_0 = (3 shl 6) or (2 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_3_2_1_1 = (3 shl 6) or (2 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_3_2_1_2 = (3 shl 6) or (2 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_3_2_1_3 = (3 shl 6) or (2 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_3_2_2_0 = (3 shl 6) or (2 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_3_2_2_1 = (3 shl 6) or (2 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_3_2_2_2 = (3 shl 6) or (2 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_3_2_2_3 = (3 shl 6) or (2 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_3_2_3_0 = (3 shl 6) or (2 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_3_2_3_1 = (3 shl 6) or (2 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_3_2_3_2 = (3 shl 6) or (2 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_3_2_3_3 = (3 shl 6) or (2 shl 4) or (3 shl 2) or 3; // $

    // 3,3
    _MM_SHUFFLE_3_3_0_0 = (3 shl 6) or (3 shl 4) or (0 shl 2) or 0; // $
    _MM_SHUFFLE_3_3_0_1 = (3 shl 6) or (3 shl 4) or (0 shl 2) or 1; // $
    _MM_SHUFFLE_3_3_0_2 = (3 shl 6) or (3 shl 4) or (0 shl 2) or 2; // $
    _MM_SHUFFLE_3_3_0_3 = (3 shl 6) or (3 shl 4) or (0 shl 2) or 3; // $

    _MM_SHUFFLE_3_3_1_0 = (3 shl 6) or (3 shl 4) or (1 shl 2) or 0; // $
    _MM_SHUFFLE_3_3_1_1 = (3 shl 6) or (3 shl 4) or (1 shl 2) or 1; // $
    _MM_SHUFFLE_3_3_1_2 = (3 shl 6) or (3 shl 4) or (1 shl 2) or 2; // $
    _MM_SHUFFLE_3_3_1_3 = (3 shl 6) or (3 shl 4) or (1 shl 2) or 3; // $

    _MM_SHUFFLE_3_3_2_0 = (3 shl 6) or (3 shl 4) or (2 shl 2) or 0; // $
    _MM_SHUFFLE_3_3_2_1 = (3 shl 6) or (3 shl 4) or (2 shl 2) or 1; // $
    _MM_SHUFFLE_3_3_2_2 = (3 shl 6) or (3 shl 4) or (2 shl 2) or 2; // $
    _MM_SHUFFLE_3_3_2_3 = (3 shl 6) or (3 shl 4) or (2 shl 2) or 3; // $

    _MM_SHUFFLE_3_3_3_0 = (3 shl 6) or (3 shl 4) or (3 shl 2) or 0; // $
    _MM_SHUFFLE_3_3_3_1 = (3 shl 6) or (3 shl 4) or (3 shl 2) or 1; // $
    _MM_SHUFFLE_3_3_3_2 = (3 shl 6) or (3 shl 4) or (3 shl 2) or 2; // $
    _MM_SHUFFLE_3_3_3_3 = (3 shl 6) or (3 shl 4) or (3 shl 2) or 3; // $




function _MM_SHUFFLE(z, y, x, w: byte): byte;
begin
    Result := (z shl 6) or (y shl 4) or (x shl 2) or w;
end;

function XM_PERMUTE_PS(v: TXMVECTOR; constref c: byte): TXMVECTOR;
var
    x, y, z, w: byte;
begin
    z := (c shr 6) and 3;
    y := (c shr 4) and 3;
    x := (c shr 2) and 3;
    w := c and 3;
    Result := XMVectorSwizzle(V, x, y, z, w);
end;



function XMMin(a, b: single): single;
begin
    if a < b then
        Result := a
    else
        Result := b;
end;

function XMMax(a, b: single): single;
begin
    if a > b then
        Result := a
    else
        Result := b;
end;

function XMVectorPermute_0X_0Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    Result := V1;
end;

function XMVectorPermute_1X_1Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    Result := V2;
end;



{$IF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVectorPermute_0X_0Y_1X_1Y(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_movelh_ps(V1, V2);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           MOVLHPS XMM0, XMM1
           MOVUPS  [result], XMM0

end;

function XMVectorPermute_1Z_1W_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_movehl_ps(V1, V2);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           MOVHLPS XMM0, XMM1
           MOVUPS  [result], XMM0
end;

function XMVectorPermute_0X_1X_0Y_1Y(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_unpacklo_ps(V1, V2);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           UNPCKLPS XMM0, XMM1
           MOVUPS  [result], XMM0
end;

function XMVectorPermute_0Z_1Z_0W_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_unpackhi_ps(V1, V2);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           UNPCKHPS XMM0, XMM1
           MOVUPS  [result], XMM0
end;

function XMVectorPermute_0Z_0W_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_castpd_ps(_mm_unpackhi_pd(_mm_castps_pd(V1), _mm_castps_pd(V2)));
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           UNPCKHPD XMM0, XMM1
           MOVUPS  [result], XMM0
end;

{$ELSE}
function XMVectorPermute_0X_0Y_1X_1Y(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_0Y, XM_PERMUTE_1X, XM_PERMUTE_1Y);
end;

function XMVectorPermute_1Z_1W_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1Z, XM_PERMUTE_1W, XM_PERMUTE_0Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_0X_1X_0Y_1Y(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_1X, XM_PERMUTE_0Y, XM_PERMUTE_1Y);
end;

function XMVectorPermute_0Z_1Z_0W_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0Z, XM_PERMUTE_1Z, XM_PERMUTE_0W, XM_PERMUTE_1W);
end;

function XMVectorPermute_0Z_0W_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0Z, XM_PERMUTE_0W, XM_PERMUTE_1Z, XM_PERMUTE_1W);
end;

{$ENDIF}

{$IF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorPermute_1X_0Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $1);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $1
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_0X_1Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $2);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $2
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_1X_1Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $3);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $3
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_0X_0Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $4);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $4
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_1X_0Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $5);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $5
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_0X_1Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $6);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $6
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_1X_1Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $7);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $7
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_0X_0Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $8);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $8
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_1X_0Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $9);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $9
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_0X_1Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $A);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $A
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_1X_1Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $B);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $B
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_0X_0Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $C);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $C
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_1X_0Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $D);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $D
           MOVUPS  [result],XMM0
end;

function XMVectorPermute_0X_1Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Result := _mm_blend_ps(V1, V2, $E);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM1,[V2]
           BLENDPS XMM0, XMM1, $E
           MOVUPS  [result],XMM0
end;

{$ELSE}
function XMVectorPermute_1X_0Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1X, XM_PERMUTE_0Y, XM_PERMUTE_0Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_0X_1Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_1Y, XM_PERMUTE_0Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_1X_1Y_0Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_0Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_0X_0Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_0Y, XM_PERMUTE_1Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_1X_0Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1X, XM_PERMUTE_0Y, XM_PERMUTE_1Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_0X_1Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_1Y, XM_PERMUTE_1Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_1X_1Y_1Z_0W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_1Z, XM_PERMUTE_0W);
end;

function XMVectorPermute_0X_0Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_0Y, XM_PERMUTE_0Z, XM_PERMUTE_1W);
end;

function XMVectorPermute_1X_0Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1X, XM_PERMUTE_0Y, XM_PERMUTE_0Z, XM_PERMUTE_1W);
end;

function XMVectorPermute_0X_1Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_1Y, XM_PERMUTE_0Z, XM_PERMUTE_1W);
end;

function XMVectorPermute_1X_1Y_0Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_0Z, XM_PERMUTE_1W);
end;

function XMVectorPermute_0X_0Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_0Y, XM_PERMUTE_1Z, XM_PERMUTE_1W);
end;

function XMVectorPermute_1X_0Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_1X, XM_PERMUTE_0Y, XM_PERMUTE_1Z, XM_PERMUTE_1W);
end;

function XMVectorPermute_0X_1Y_1Z_1W(V1, V2: TXMVECTOR): TXMVECTOR;
begin
    XMVectorPermute(V1, V2, XM_PERMUTE_0X, XM_PERMUTE_1Y, XM_PERMUTE_1Z, XM_PERMUTE_1W);
end;

{$ENDIF}

// Specialized swizzles
function XMVectorSwizzle_X_Y_Z_W(V: TXMVECTOR): TXMVECTOR;
begin
    Result := V;
end;

{$IF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVectorSwizzle_X_Y_X_Y(V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // _mm_movelh_ps(V,V);
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,[V]
           MOVLHPS XMM0, XMM1
           MOVUPS  [result], XMM0
end;

function XMVectorSwizzle_Z_W_Z_W(V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // _mm_movehl_ps(V,V);
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,[V]
           MOVHLPS XMM0, XMM1
           MOVUPS  [result], XMM0
end;

function XMVectorSwizzle_X_X_Y_Y(V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //_mm_unpacklo_ps(V,V);
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,[V]
           UNPCKLPS  XMM0, XMM1
           MOVUPS  [result], XMM0
end;

function XMVectorSwizzle_Z_Z_W_W(V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //_mm_unpackhi_ps(V,V);
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,[V]
           UNPCKHPS   XMM0, XMM1
           MOVUPS  [result], XMM0
end;
{$ELSE}
function XMVectorSwizzle_X_Y_X_Y(V: TXMVECTOR): TXMVECTOR;
begin
    XMVectorSwizzle(V, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_Y);
end;

function XMVectorSwizzle_Z_W_Z_W(V: TXMVECTOR): TXMVECTOR;
begin
    XMVectorSwizzle(V, XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Z, XM_SWIZZLE_W);
end;

function XMVectorSwizzle_X_X_Y_Y(V: TXMVECTOR): TXMVECTOR;
begin
    XMVectorSwizzle(V, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
end;

function XMVectorSwizzle_Z_Z_W_W(V: TXMVECTOR): TXMVECTOR;
begin
    XMVectorSwizzle(V, XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_W);
end;

{$ENDIF}

{$IF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVectorSwizzle_X_X_Z_Z(V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  _mm_moveldup_ps(V);
           MOVUPS  XMM0,[V]
           MOVSLDUP XMM0, XMM0
           MOVUPS  [result], XMM0
end;

function XMVectorSwizzle_Y_Y_W_W(V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  _mm_movehdup_ps(V);
           MOVUPS  XMM0,[V]
           MOVSHDUP XMM0, XMM0
           MOVUPS  [result], XMM0
end;

{$ELSE}
function XMVectorSwizzle_X_X_Z_Z(V: TXMVECTOR): TXMVECTOR;
begin
    XMVectorSwizzle(V, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_Z, XM_SWIZZLE_Z);
end;

function XMVectorSwizzle_Y_Y_W_W(V: TXMVECTOR): TXMVECTOR;
begin
    XMVectorSwizzle(V, XM_SWIZZLE_Y, XM_SWIZZLE_Y, XM_SWIZZLE_W, XM_SWIZZLE_W);
end;

{$ENDIF}

{$IF DEFINED(_XM_AVX2_INTRINSICS_)}
function XMVectorSwizzle_X_X_X_X(V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // _mm_broadcastss_ps(V);
           MOVUPS  XMM0,[V]
           VBROADCASTSS XMM0, XMM0
           MOVUPS  [result], XMM0
end;

{$ELSE}
function XMVectorSwizzle_X_X_X_X(V: TXMVECTOR): TXMVECTOR;
begin
    XMVectorSwizzle(V, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X);
end;

{$ENDIF}


function XMISNAN(x: UINT32): boolean;
begin
    Result := ((x and $7F800000) = $7F800000) and ((x and $7FFFFF) <> 0);
end;

function XMISINF(x: UINT32): boolean;
begin
    Result := ((x and $7FFFFFFF) = $7F800000);
end;

{$if defined(_XM_SSE_INTRINSICS_)}
procedure XM3UNPACK3INTO4(constref l1, l2, l3: TXMVECTOR); assembler;
var
    V3, V2, V4: TXMVECTOR;
asm
           MOVUPS  XMM0, [l2]
           SHUFPS  XMM0, [l3], _MM_SHUFFLE_0_0_3_2 // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
           MOVUPS  XMM1, [l2]
           SHUFPS  XMM1, [l1], _MM_SHUFFLE_3_3_1_0 // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));\
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_0_2 // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));\
           MOVUPS  XMM2, [l3]
           PSRLDQ  XMM2, 4 //  V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
end;

procedure XM3PACK4INTO3(out v2x: TXMVECTOR); assembler;
var
    V1, V3, V2, V4: TXMVECTOR;
asm
           MOVUPS  XMM0, [v2]
           SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // v2x := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
           MOVUPS  XMM1, [v2]
           SHUFPS  XMM1, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
           MOVUPS  XMM2, [v1]
           SHUFPS  XMM2, [v2], _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
           MOVUPS  XMM3, [v3]
           SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));\
           MOVUPS  XMM4, [v3]
           SHUFPS  XMM4, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));\
end;
{$endif}

{***************************************************************************
 *
 * Macros
 *
 ***************************************************************************}


// Unit conversion

function XMConvertToRadians(fDegrees: single): single; inline;
begin
    Result := fDegrees * (XM_PI / 180.0);
end;



function XMConvertToDegrees(fRadians: single): single; inline;
begin
    Result := fRadians * (180.0 / XM_PI);
end;

// Condition register evaluation proceeding a recording (R) comparison
function XMComparisonAllTrue(CR: uint32): boolean; inline;
begin
    Result := ((CR and XM_CRMASK_CR6TRUE) = XM_CRMASK_CR6TRUE);
end;



function XMComparisonAnyTrue(CR: uint32): boolean; inline;
begin
    Result := ((CR and XM_CRMASK_CR6FALSE) <> XM_CRMASK_CR6FALSE);
end;



function XMComparisonAllFalse(CR: uint32): boolean; inline;
begin
    Result := ((CR and XM_CRMASK_CR6FALSE) = XM_CRMASK_CR6FALSE);
end;



function XMComparisonAnyFalse(CR: uint32): boolean; inline;
begin
    Result := ((CR and XM_CRMASK_CR6TRUE) <> XM_CRMASK_CR6TRUE);
end;



function XMComparisonMixed(CR: uint32): boolean; inline;
begin
    Result := ((CR and XM_CRMASK_CR6) = 0);
end;



function XMComparisonAllInBounds(CR: uint32): boolean; inline;
begin
    Result := ((CR and XM_CRMASK_CR6BOUNDS) = XM_CRMASK_CR6BOUNDS);
end;



function XMComparisonAnyOutOfBounds(CR: uint32): boolean; inline;
begin
    Result := ((CR and XM_CRMASK_CR6BOUNDS) <> XM_CRMASK_CR6BOUNDS);
end;

{****************************************************************************
 *
 * Data conversion
 *
 ****************************************************************************}

// Returns the converted vector, where each component has been divided by two raised to the DivExponent power.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMConvertVectorIntToFloat(constref VInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR;
var
    fScale: single;
    ElementIndex: UINT32;
begin
    fScale := 1.0 / (1 shl DivExponent);
    ElementIndex := 0;
    while ElementIndex < 4 do
    begin
        Result.f32[ElementIndex] := VInt.u32[ElementIndex] * fScale;
        Inc(ElementIndex);
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMConvertVectorIntToFloat(constref VInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR;
begin
    (*
    float fScale = 1.0f / (float)(1U << DivExponent);
    float32x4_t vResult = vcvtq_f32_s32( VInt );
    return vmulq_n_f32( vResult, fScale );
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMConvertVectorIntToFloat(constref VInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR; assembler;
const
    uScale: UINT32 = $3F800000; // 1;
asm
           // Convert to floats
           VMOVUPS XMM1,[VInt]
           VCVTDQ2PS XMM0, XMM1
           // Convert DivExponent into 1.0f/(1<<DivExponent)
           VPBROADCASTD XMM1,[uScale]
           VPBROADCASTD XMM2, [DivExponent]
           PSLLD   XMM2, 23
           VPSUBB  XMM1, XMM1,XMM2
           // Splat the scalar value
           VMULPS  XMM0,XMM0, XMM1
           VMOVUPS   [Result], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
//Returns the converted vector, where each component has been multiplied by two raised to the MulExponent power.
function XMConvertVectorFloatToInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR;
var
    fScale, fTemp: single;
    ElementIndex: uint32 = 0;
    iResult: int32;
begin
    // Get the scalar factor.
    fScale := (1 shl MulExponent);

    while ElementIndex < 4 do
    begin
        fTemp := VFloat.f32[ElementIndex] * fScale;
        if (fTemp <= -(65536.0 * 32768.0)) then
            iResult := (-$7FFFFFFF) - 1
        else if (fTemp > (65536.0 * 32768.0) - 128.0) then
            iResult := $7FFFFFFF
        else
            iResult := Trunc(fTemp);
        Result.u32[ElementIndex] := iResult;
        Inc(ElementIndex);
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMConvertVectorFloatToInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR;
begin
    (* ToDo
   float32x4_t vResult = vmulq_n_f32(VFloat, (float)(1U << MulExponent));
    // In case of positive overflow, detect it
    uint32x4_t vOverflow = vcgtq_f32(vResult,g_XMMaxInt);
    // Float to int conversion
    int32x4_t vResulti = vcvtq_s32_f32(vResult);
    // If there was positive overflow, set to 0x7FFFFFFF
    vResult = vandq_u32(vOverflow,g_XMAbsMask);
    vOverflow = vbicq_u32(vResulti,vOverflow);
    vOverflow = vorrq_u32(vOverflow,vResult);
    return vOverflow;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMConvertVectorFloatToInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR; assembler;
const
    uScale: UINT32 = 1;// $3F800000; // 1;
asm
           VMOVUPS XMM0,[VFloat]
           VPBROADCASTD XMM1,[uScale]
           VPBROADCASTD XMM2, [MulExponent]
           VPSLLVD   XMM1,XMM1, XMM2
           VCVTDQ2PS XMM1, XMM1
           VMULPS  XMM0,XMM0, XMM1
           // Float to int conversion
           // ToDo: check for exceptions, wenn Zahl zu groß ist
           VCVTTPS2DQ XMM3, XMM0
           // In case of positive overflow, detect it
           VMOVUPS XMM2,[g_XMMaxInt]
           VCMPPS  XMM1, XMM3, XMM2, 6 // overflow in xmm1
           // If there was positive overflow, set to 0x7FFFFFFF
           VANDPS  XMM0, XMM1, [g_XMAbsMask]
           VANDNPS XMM1, XMM1, XMM3
           VORPS   XMM1, XMM1,XMM0
           VMOVUPS   [Result], XMM1
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMConvertVectorUIntToFloat(constref VUInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR;
var
    fScale: single;
    ElementIndex: uint32 = 0;
begin
    // ToDo    assert(DivExponent<32);
    fScale := 1.0 / (1 shl DivExponent);
    while ElementIndex < 4 do
    begin
        Result.f32[ElementIndex] := VUInt.u32[ElementIndex] * fScale;
        Inc(ElementIndex);
    end;
end;

 {$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMConvertVectorUIntToFloat(constref VUInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR;
begin
     (*
      float fScale = 1.0f / (float)(1U << DivExponent);
    float32x4_t vResult = vcvtq_f32_u32( VUInt );
    return vmulq_n_f32( vResult, fScale );
     *)
end;

{$ELSE}
function XMConvertVectorUIntToFloat(constref VUInt: TXMVECTOR; constref DivExponent: UINT32): TXMVECTOR; assembler;
const
    uScale: UINT32 = $3F800000; // 1.0
asm
           // For the values that are higher than 0x7FFFFFFF, a fixup is needed
           // Determine which ones need the fix.
           VMOVUPS XMM0,[VUInt]
           VANDPS  XMM1, XMM0, [g_XMNegativeZero] // iMask in xmm1, VUint in xmm0
           // Force all values positive
           VXORPS  XMM3, XMM0, XMM1 // vResult in xmm3
           // Convert to floats
           VCVTDQ2PS XMM3, XMM3
           // Convert 0x80000000 -> 0xFFFFFFFF
           VPSRAD  XMM1,XMM1, 31
           // For only the ones that are too big, add the fixup
           VANDPS  XMM4, XMM1, [g_XMFixUnsigned]  // vMask in xmm4
           VANDPS  XMM3, XMM3, XMM4
           // Convert DivExponent into 1.0f/(1<<DivExponent)
           VPBROADCASTD XMM5,[uScale]
           VPBROADCASTD XMM6, [DivExponent]
           PSLLD   XMM6, 23
           VPSUBB  XMM5, XMM5,XMM6
           // Splat
           VMULPS  XMM0,XMM3, XMM5
           VMOVUPS   [Result], XMM0

   (*

 // For the values that are higher than 0x7FFFFFFF, a fixup is needed
    // Determine which ones need the fix.
    XMVECTOR vMask = _mm_and_ps(VUInt,g_XMNegativeZero);
    // Force all values positive
    XMVECTOR vResult = _mm_xor_ps(VUInt,vMask);
    // Convert to floats
    vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
    // Convert 0x80000000 -> 0xFFFFFFFF
    __m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
    // For only the ones that are too big, add the fixup
    vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
    vResult = _mm_add_ps(vResult,vMask);
    // Convert DivExponent into 1.0f/(1<<DivExponent)
    uint32_t uScale = 0x3F800000U - (DivExponent << 23);
    // Splat
    iMask = _mm_set1_epi32(uScale);
    vResult = _mm_mul_ps(vResult,_mm_castsi128_ps(iMask));
    return vResult;   *)
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMConvertVectorFloatToUInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR;
var
    fScale, fTemp: single;
    ElementIndex, uResult: uint32;
begin
    // Get the scalar factor.
    fScale := (1 shl MulExponent);
    ElementIndex := 0;
    while ElementIndex < 4 do
    begin
        fTemp := VFloat.f32[ElementIndex] * fScale;
        if (fTemp <= 0.0) then
            uResult := 0
        else if (fTemp >= (65536.0 * 65536.0)) then
            uResult := $FFFFFFFF
        else
            uResult := trunc(fTemp);
        Result.u32[ElementIndex] := uResult;
    end;
end;

{$ELSEIF DEFINED (_XM_ARM_NEON_INTRINSICS_)}
function XMConvertVectorFloatToUInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR;
begin
    (*

    float32x4_t vResult = vmulq_n_f32(VFloat,(float)(1U << MulExponent));
    // In case of overflow, detect it
    uint32x4_t vOverflow = vcgtq_f32(vResult,g_XMMaxUInt);
    // Float to int conversion
    uint32x4_t vResulti = vcvtq_u32_f32(vResult);
    // If there was overflow, set to 0xFFFFFFFFU
    vResult = vbicq_u32(vResulti,vOverflow);
    vOverflow = vorrq_u32(vOverflow,vResult);
    return vOverflow;

    *)
end;

{$ELSE}
function XMConvertVectorFloatToUInt(constref VFloat: TXMVECTOR; constref MulExponent: UINT32): TXMVECTOR; { assembler;}
var
    f: single;
begin
    f := 1 shl MulExponent;
    asm
               // vResult = _mm_set_ps1(static_cast<float>(1U << MulExponent));
               MOVSS   XMM1, [f]
               SHUFPS  XMM1, XMM1, 0 // XMM1 = vResult

               MULPS   XMM1, [VFloat] //  vResult = _mm_mul_ps(vResult,VFloat);
               // Clamp to >=0
               MAXPS   XMM1, [g_XMZero] // vResult = _mm_max_ps(vResult,g_XMZero);
               // Any numbers that are too big, set to 0xFFFFFFFFU
               // vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
               MOVUPS  XMM2, [g_XMMaxUInt]
               CMPPS   XMM2, XMM1, 1  // XMM2 = vOverflow

               // vValue = g_XMUnsignedFix;
               MOVUPS  XMM3, [g_XMUnsignedFix]
               // Too large for a signed integer?
               // vMask = _mm_cmpge_ps(vResult,vValue);
               MOVUPS  XMM4, XMM3
               CMPPS   XMM4, XMM1, 2
               // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
               ANDPS   XMM3, XMM4 // vValue = _mm_and_ps(vValue,vMask);
               // Perform fixup only on numbers too large (Keeps low bit precision)
               SUBPS   XMM1, XMM3 // vResult = _mm_sub_ps(vResult,vValue);
               CVTTPS2DQ XMM5, XMM1 // __m128i vResulti = _mm_cvttps_epi32(vResult);
               // Convert from signed to unsigned pnly if greater than 0x80000000
               ANDPS   XMM4, [g_XMNegativeZero] // vMask = _mm_and_ps(vMask,g_XMNegativeZero);
               XORPS   XMM5, XMM4 // vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
               // On those that are too large, set to 0xFFFFFFFF
               ORPS    XMM5, XMM2 // vResult = _mm_or_ps(vResult,vOverflow);
               MOVUPS  [result], XMM5 // return vResult;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetBinaryConstant(constref C0: UINT32; constref C1: UINT32; constref C2: UINT32; constref C3: UINT32): TXMVECTOR;
begin
    Result.u32[0] := (0 - (C0 and 1)) and $3F800000;
    Result.u32[1] := (0 - (C1 and 1)) and $3F800000;
    Result.u32[2] := (0 - (C2 and 1)) and $3F800000;
    Result.u32[3] := (0 - (C3 and 1)) and $3F800000;
end;

{$ELSEIF DEFINED (_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetBinaryConstant(constref C0: UINT32; constref C1: UINT32; constref C2: UINT32; constref C3: UINT32): TXMVECTOR;
begin
    Result.u32[0] := (0 - (C0 and 1)) and $3F800000;
    Result.u32[1] := (0 - (C1 and 1)) and $3F800000;
    Result.u32[2] := (0 - (C2 and 1)) and $3F800000;
    Result.u32[3] := (0 - (C3 and 1)) and $3F800000;
end;

{$ELSE}// XM_SSE_INTRINSICS_
function XMVectorSetBinaryConstant(constref C0: UINT32; constref C1: UINT32; constref C2: UINT32; constref C3: UINT32): TXMVECTOR; assembler;
const
    g_vMask1: TXMVECTORU32 = (u: (1, 1, 1, 1));
asm
           // Move the parms to a vector
           // __m128i vTemp = _mm_set_epi32(C3,C2,C1,C0);
           MOVAPS        XMM0,[c3]
           MOVAPS        XMM1,[c2]
           MOVAPS        XMM2,[c1]
           MOVAPS        XMM3,[c0]
           PUNPCKLDQ   XMM3,XMM1
           PUNPCKLDQ   XMM2,XMM0
           PUNPCKLDQ   XMM3,XMM2  // XMM3 = vTemp


           // Mask off the low bits
           PAND    XMM3, [g_vMask1] // vTemp = _mm_and_si128(vTemp,g_vMask1);
           // 0xFFFFFFFF on true bits
           PCMPEQD XMM3, [g_vMask1] // vTemp = _mm_cmpeq_epi32(vTemp,g_vMask1);
           // 0xFFFFFFFF -> 1.0f, 0x00000000 -> 0.0f
           PAND    XMM3, [g_XMOne] // vTemp = _mm_and_si128(vTemp,g_XMOne);
           MOVAPS  [result],XMM3// return _mm_castsi128_ps(vTemp);
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatConstant(IntConstant: INT32; DivExponent: UINT32): TXMVECTOR;
var
    V: TXMVECTOR;
begin
    assert((IntConstant >= -16) and (IntConstant <= 15));
    assert(DivExponent < 32);

    V.i32[0] := IntConstant;
    V.i32[1] := IntConstant;
    V.i32[2] := IntConstant;
    V.i32[3] := IntConstant;
    Result := XMConvertVectorIntToFloat(V, DivExponent);
end;

{$ELSEIF DEFINED (_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatConstant(IntConstant: INT32; DivExponent: UINT32): TXMVECTOR;
begin
     {
     // Splat the int
    int32x4_t vScale = vdupq_n_s32(IntConstant);
    // Convert to a float
    XMVECTOR vResult = vcvtq_f32_s32(vScale);
    // Convert DivExponent into 1.0f/(1<<DivExponent)
    uint32_t uScale = 0x3F800000U - (DivExponent << 23);
    // Splat the scalar value (It's really a float)
    vScale = vdupq_n_s32(uScale);
    // Multiply by the reciprocal (Perform a right shift by DivExponent)
    vResult = vmulq_f32(vResult,reinterpret_cast<const float32x4_t *>(&vScale)[0]);
    return vResult;
    }
end;

{$ELSE}// XM_SSE_INTRINSICS_
function XMVectorSplatConstant(IntConstant: INT32; DivExponent: UINT32): TXMVECTOR;
begin
    {
   // Splat the int
    __m128i vScale = _mm_set1_epi32(IntConstant);
    // Convert to a float
    XMVECTOR vResult = _mm_cvtepi32_ps(vScale);
    // Convert DivExponent into 1.0f/(1<<DivExponent)
    uint32_t uScale = 0x3F800000U - (DivExponent << 23);
    // Splat the scalar value (It's really a float)
    vScale = _mm_set1_epi32(uScale);
    // Multiply by the reciprocal (Perform a right shift by DivExponent)
    vResult = _mm_mul_ps(vResult,_mm_castsi128_ps(vScale));
    return vResult;
    }
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatConstantInt(IntConstant: INT32): TXMVECTOR;
begin
    assert((IntConstant >= -16) and (IntConstant <= 15));

    Result.u32[0] := IntConstant;
    Result.u32[1] := IntConstant;
    Result.u32[2] := IntConstant;
    Result.u32[3] := IntConstant;
end;

{$ELSEIF DEFINED (_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatConstantInt(IntConstant: INT32): TXMVECTOR;
begin
    {int32x4_t V = vdupq_n_s32( IntConstant );
    return reinterpret_cast<float32x4_t *>(&V)[0];   }
end;

{$ELSE}// XM_SSE_INTRINSICS_
function XMVectorSplatConstantInt(IntConstant: INT32): TXMVECTOR;
begin
   {  __m128i V = _mm_set1_epi32( IntConstant );
    return _mm_castsi128_ps(V); }
end;

{$ENDIF}


{****************************************************************************
 *
 * Vector and matrix load operations
 *
 ****************************************************************************}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadInt(constref pSource: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pSource[0];
    Result.u32[1] := 0;
    Result.u32[2] := 0;
    Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadInt(constref pSource: PUINT32): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t zero = vdupq_n_u32(0);
    return vld1q_lane_u32( pSource, zero, 0 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadInt(constref pSource: PUINT32): TXMVECTOR; assembler;
asm
           //   return _mm_load_ss( reinterpret_cast<const float*>(pSource) );
           MOVSS   XMM0, [pSource];
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat(constref pSource: PSingle): TXMVECTOR;
begin
    Result.f32[0] := pSource[0];
    Result.f32[1] := 0.0;
    Result.f32[2] := 0.0;
    Result.f32[3] := 0.0;

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat(constref pSource: PSingle): TXMVECTOR;
begin
    (* ToDo
    float32x4_t zero = vdupq_n_f32(0);
    return vld1q_lane_f32( pSource, zero, 0 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat(constref pSource: PSingle): TXMVECTOR; assembler;
asm
           //   return _mm_load_ss( pSource );
           MOVSS   XMM0, [pSource];
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadInt2(const pSource: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pSource[0];
    Result.u32[1] := pSource[1];
    Result.u32[2] := 0;
    Result.u32[3] := 0;

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadInt2(const pSource: PUINT32): TXMVECTOR;
begin
    (* ToDo
    uint32x2_t x = vld1_u32( pSource );
    uint32x2_t zero = vdup_n_u32(0);
    return vcombine_u32( x, zero );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadInt2(const pSource: PUINT32): TXMVECTOR; assembler;
asm
           // __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pSource) );
           MOVSS   XMM0, [pSource];
           // __m128 y = _mm_load_ss( reinterpret_cast<const float*>(pSource+1) );
           MOVSS   XMM1, [pSource+1];
           // return _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadInt2A(const PSource: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pSource[0];
    Result.u32[1] := pSource[1];
    Result.u32[2] := 0;
    Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadInt2A(const PSource: PUINT32): TXMVECTOR;
begin
    (* ToDo
    uint32x2_t x = vld1_u32_ex( pSource, 64 );
    uint32x2_t zero = vdup_n_u32(0);
    return vcombine_u32( x, zero );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadInt2A(const PSource: PUINT32): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pSource) );
           MOVQ    XMM0, [PSource]
           // return _mm_castsi128_ps(V);
           MOVAPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat2(const pSource: TXMFLOAT2): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := 0.0;
    Result.f32[3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat2(const pSource: TXMFLOAT2): TXMVECTOR;
begin
    (* ToDo
    float32x2_t x = vld1_f32( reinterpret_cast<const float*>(pSource) );
    float32x2_t zero = vdup_n_f32(0);
    return vcombine_f32( x, zero );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat2(const pSource: TXMFLOAT2): TXMVECTOR; assembler;
asm
           // __m128 x = _mm_load_ss( &pSource->x );
           MOVSS   XMM0, [pSource];
           // __m128 y = _mm_load_ss( &pSource->y );
           MOVSS   XMM1, [pSource+1];
           // return _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat2A(const pSource: TXMFLOAT2A): TXMVECTOR;
begin

    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := 0.0;
    Result.f32[3] := 0.0;

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat2A(const pSource: TXMFLOAT2A): TXMVECTOR;
begin
    (* ToDo
   float32x2_t x = vld1_f32_ex( reinterpret_cast<const float*>(pSource), 64 );
    float32x2_t zero = vdup_n_f32(0);
    return vcombine_f32( x, zero );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat2A(const pSource: TXMFLOAT2A): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pSource) );
           MOVQ    XMM0, [pSource]
           // return _mm_castsi128_ps(V);
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadSInt2(const pSource: TXMINT2): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := 0.0;
    Result.f32[3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadSInt2(const pSource: TXMINT2): TXMVECTOR;
begin
    (* ToDo
     int32x2_t x = vld1_s32( reinterpret_cast<const int32_t*>(pSource) );
    float32x2_t v = vcvt_f32_s32( x );
    float32x2_t zero = vdup_n_f32(0);
    return vcombine_f32( v, zero );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadSInt2(const pSource: TXMINT2): TXMVECTOR; assembler;
asm
           // __m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
           MOVSS   XMM0, TXMINT2([pSource]).x;
           // __m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
           MOVSS   XMM1, TXMINT2([pSource]).Y;
           // __m128 V = _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1
           // return _mm_cvtepi32_ps(_mm_castps_si128(V));
           CVTDQ2PS XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadUInt2(const pSource: TXMUINT2): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := 0.0;
    Result.f32[3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadUInt2(const pSource: TXMUINT2): TXMVECTOR;
begin
    (* ToDo
    uint32x2_t x = vld1_u32( reinterpret_cast<const uint32_t*>(pSource) );
    float32x2_t v = vcvt_f32_u32( x );
    float32x2_t zero = vdup_n_f32(0);
    return vcombine_f32( v, zero );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadUInt2(const pSource: TXMUINT2): TXMVECTOR; assembler;
asm
           // _m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
           MOVSS   XMM0, TXMUINT2([pSource]).X;
           // __m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
           MOVSS   XMM1, TXMUINT2([pSource]).y;

           // __m128 V = _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1

           // For the values that are higher than 0x7FFFFFFF, a fixup is needed
           // Determine which ones need the fix.
           MOVUPS  XMM2, XMM0
           ANDPS   XMM2,[g_XMNegativeZero] // vMask = _mm_and_ps(V,g_XMNegativeZero);
           // Force all values positive
           MOVUPS  XMM3, XMM0
           XORPS   XMM3,XMM2 // vResult = _mm_xor_ps(V,vMask);
           // Convert to floats
           CVTDQ2PS XMM3, XMM3 // vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
           // Convert 0x80000000 -> 0xFFFFFFFF
           // __m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
           MOVUPS  XMM4, XMM2
           PSRAD   XMM4, 31

           // For only the ones that are too big, add the fixup
           ANDPS   XMM4,[g_XMFixUnsigned] // vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
           ADDPS   XMM3, XMM4 // vResult = _mm_add_ps(vResult,vMask);
           MOVUPS  [result],XMM3// return vResult;
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadInt3(const pSource: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pSource[0];
    Result.u32[1] := pSource[1];
    Result.u32[2] := pSource[2];
    Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadInt3(const pSource: PUINT32): TXMVECTOR;
begin
    (* ToDo
    uint32x2_t x = vld1_u32( pSource );
    uint32x2_t zero = vdup_n_u32(0);
    uint32x2_t y = vld1_lane_u32( pSource+2, zero, 0 );
    return vcombine_u32( x, y );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadInt3(const pSource: PUINT32): TXMVECTOR; assembler;
asm
           // __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pSource) );
           MOVSS   XMM0, UINT32([pSource]);
           //__m128 y = _mm_load_ss( reinterpret_cast<const float*>(pSource+1) );
           MOVSS   XMM1, UINT32([pSource+1]);
           //__m128 z = _mm_load_ss( reinterpret_cast<const float*>(pSource+2) );
           MOVSS   XMM2, UINT32([pSource+2]);
           //__m128 xy = _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1
           //return _mm_movelh_ps( xy, z );
           MOVLHPS XMM0, XMM2
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadInt3A(const pSource: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pSource[0];
    Result.u32[1] := pSource[1];
    Result.u32[2] := pSource[2];
    Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadInt3A(const pSource: UINT32): TXMVECTOR;
begin
    (* ToDo
    // Reads an extra integer which is zero'd
    uint32x4_t V = vld1q_u32_ex( pSource, 128 );
    return vsetq_lane_u32( 0, V, 3 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadInt3A(const pSource: PUINT32): TXMVECTOR; assembler;
asm
           // Reads an extra integer which is zero'd
           //  __m128i V = _mm_load_si128( reinterpret_cast<const __m128i*>(pSource) );
           MOVDQA  XMM0, [pSource]
           // V = _mm_and_si128( V, g_XMMask3 );
           PAND    XMM0, [g_XMMask3]
           // return _mm_castsi128_ps(V);
           MOVAPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat3(constref pSource: TXMFLOAT3): TXMVECTOR; inline;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := pSource.z;
    Result.f32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat3(constref pSource: TXMFLOAT3): TXMVECTOR; inline;
begin
    (* ToDo
    float32x2_t x = vld1_f32( reinterpret_cast<const float*>(pSource) );
    float32x2_t zero = vdup_n_f32(0);
    float32x2_t y = vld1_lane_f32( reinterpret_cast<const float*>(pSource)+2, zero, 0 );
    return vcombine_f32( x, y );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat3(constref pSource: TXMFLOAT3): TXMVECTOR; inline; assembler;
asm
           // __m128 x = _mm_load_ss( &pSource->x );
           MOVSS   XMM0, TXMFLOAT3([pSource]).X;
           //    __m128 y = _mm_load_ss( &pSource->y );
           MOVSS   XMM0, TXMFLOAT3([pSource]).y;
           //    __m128 z = _mm_load_ss( &pSource->z );
           MOVSS   XMM0, TXMFLOAT3([pSource]).z;
           // __m128 xy = _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1

           // return _mm_movelh_ps( xy, z );
           MOVLHPS XMM0, XMM2
           MOVUPS  [result],XMM0
end;
{$ENDIF}


function XMLoadFloat3(constref pSource: pSingle): TXMVECTOR;
begin
    Result.f32[0] := pSource[0];
    Result.f32[1] := pSource[1];
    Result.f32[2] := pSource[2];
    Result.f32[3] := 0;
end;

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat3A(const pSource: TXMFLOAT3A): TXMVECTOR;
begin

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat3A(const pSource: TXMFLOAT3A): TXMVECTOR;
begin
    (* ToDo
    // Reads an extra float which is zero'd
    float32x4_t V = vld1q_f32_ex( reinterpret_cast<const float*>(pSource), 128 );
    return vsetq_lane_f32( 0, V, 3 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat3A(const pSource: TXMFLOAT3A): TXMVECTOR; assembler;
asm
           // Reads an extra float which is zero'd
           // __m128 V = _mm_load_ps( &pSource->x );
           MOVAPS  XMM0, TXMFLOAT3A([pSource]).x
           // return _mm_and_ps( V, g_XMMask3 );
           ANDPS   XMM0,[g_XMMask3]
           MOVAPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadSInt3(const pSource: TXMINT3): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := pSource.z;
    Result.f32[3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadSInt3(const pSource: TXMINT3): TXMVECTOR;
begin
    (* ToDo
    int32x2_t x = vld1_s32( reinterpret_cast<const int32_t*>(pSource) );
    int32x2_t zero = vdup_n_s32(0);
    int32x2_t y = vld1_lane_s32( reinterpret_cast<const int32_t*>(pSource)+2, zero, 0 );
    int32x4_t v = vcombine_s32( x, y );
    return vcvtq_f32_s32( v );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadSInt3(const pSource: TXMINT3): TXMVECTOR; assembler;
asm
           // __m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
           MOVSS   XMM0, TXMINT3([pSource]).x
           //__m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
           MOVSS   XMM1, TXMINT3([pSource]).y
           // __m128 z = _mm_load_ss( reinterpret_cast<const float*>(&pSource->z) );
           MOVSS   XMM2, TXMINT3([pSource]).y
           // __m128 xy = _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1

           // __m128 V = _mm_movelh_ps( xy, z );
           MOVLHPS XMM0, XMM2

           // return _mm_cvtepi32_ps(_mm_castps_si128(V));
           CVTDQ2PS XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadUInt3(const pSource: TXMUINT3): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := pSource.z;
    Result.f32[3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadUInt3(const pSource: TXMUINT3): TXMVECTOR;
begin
    (* ToDo
    uint32x2_t x = vld1_u32( reinterpret_cast<const uint32_t*>(pSource) );
    uint32x2_t zero = vdup_n_u32(0);
    uint32x2_t y = vld1_lane_u32( reinterpret_cast<const uint32_t*>(pSource)+2, zero, 0 );
    uint32x4_t v = vcombine_u32( x, y );
    return vcvtq_f32_u32( v );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadUInt3(const pSource: TXMUINT3): TXMVECTOR; assembler;
asm
           //__m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
           MOVSS   XMM0, TXMUINT3([pSource]).x
           // __m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
           MOVSS   XMM1, TXMUINT3([pSource]).y
           //__m128 z = _mm_load_ss( reinterpret_cast<const float*>(&pSource->z) );
           MOVSS   XMM2, TXMUINT3([pSource]).z
           // __m128 xy = _mm_unpacklo_ps( x, y );
           UNPCKLPS XMM0, XMM1
           // __m128 V = _mm_movelh_ps( xy, z );
           MOVLHPS XMM0, XMM2
           // For the values that are higher than 0x7FFFFFFF, a fixup is needed
           // Determine which ones need the fix.
           // vMask = _mm_and_ps(V,g_XMNegativeZero);
           MOVUPS  XMM3,XMM0
           ANDPS   XMM3, [g_XMNegativeZero]
           // Force all values positive
           // vResult = _mm_xor_ps(V,vMask);
           XORPS   XMM0, XMM3
           // Convert to floats
           // vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
           CVTDQ2PS XMM0, XMM0
           // Convert 0x80000000 -> 0xFFFFFFFF
           // __m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
           PSRAD   XMM3, 31
           // For only the ones that are too big, add the fixup
           // vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
           ANDPS   XMM3,[g_XMFixUnsigned]
           // vResult = _mm_add_ps(vResult,vMask);
           ADDPS   XMM0,XMM3
           // return vResult;
           MOVUPS  [Result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadInt4(const pSource: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pSource[0];
    Result.u32[1] := pSource[1];
    Result.u32[2] := pSource[2];
    Result.u32[3] := pSource[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadInt4(const pSource: PUINT32): TXMVECTOR;
begin
    (* ToDo
    return vld1q_u32( pSource );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadInt4(const pSource: PUINT32): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_loadu_si128( reinterpret_cast<const __m128i*>(pSource) );
           MOVDQU  XMM0, [pSource]
           // return _mm_castsi128_ps(V);
           MOVAPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadInt4A(const pSource: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pSource[0];
    Result.u32[1] := pSource[1];
    Result.u32[2] := pSource[2];
    Result.u32[3] := pSource[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadInt4A(const pSource: PUINT32): TXMVECTOR;
begin
    (* ToDo
    return vld1q_u32_ex( pSource, 128 );
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadInt4A(const pSource: PUINT32): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_load_si128( reinterpret_cast<const __m128i*>(pSource) );
           MOVDQA  XMM0, [pSource]
           // return _mm_castsi128_ps(V);
           MOVAPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat4(constref pSource: TXMFLOAT4): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := pSource.z;
    Result.f32[3] := pSource.w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat4(constref pSource: TXMFLOAT4): TXMVECTOR;
begin
    (* ToDo
   return vld1q_f32( reinterpret_cast<const float*>(pSource) );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat4(constref pSource: TXMFLOAT4): TXMVECTOR; assembler;
asm
           // return _mm_loadu_ps( &pSource->x );
           MOVUPS  XMM0, [pSource]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

function XMLoadFloat4(constref pSource: PSingle): TXMVECTOR;
begin
    Result.f32[0] := pSource[0];
    Result.f32[1] := pSource[1];
    Result.f32[2] := pSource[2];
    Result.f32[3] := pSource[3];
end;

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat4A(const pSource: TXMFLOAT4A): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := pSource.z;
    Result.f32[3] := pSource.w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat4A(const pSource: TXMFLOAT4A): TXMVECTOR;
begin
    (* ToDo
     return vld1q_f32_ex( reinterpret_cast<const float*>(pSource), 128 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat4A(const pSource: TXMFLOAT4A): TXMVECTOR; assembler;
asm
           // return _mm_load_ps( &pSource.x );
           MOVAPS  XMM0, [pSource]
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadSInt4(const pSource: TXMINT4): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := pSource.z;
    Result.f32[3] := pSource.w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadSInt4(const pSource: TXMINT4): TXMVECTOR;
begin
    (* ToDo
    int32x4_t v = vld1q_s32( reinterpret_cast<const int32_t*>(pSource) );
    return vcvtq_f32_s32( v );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadSInt4(const pSource: TXMINT4): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_loadu_si128( reinterpret_cast<const __m128i*>(pSource) );
           MOVDQU  XMM0, [pSource]
           // return _mm_cvtepi32_ps(V);
           CVTDQ2PS XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadUInt4(const pSource: TXMUINT4): TXMVECTOR;
begin
    Result.f32[0] := pSource.x;
    Result.f32[1] := pSource.y;
    Result.f32[2] := pSource.z;
    Result.f32[3] := pSource.w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadUInt4(const pSource: TXMUINT4): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t v = vld1q_u32( reinterpret_cast<const uint32_t*>(pSource) );
    return vcvtq_f32_u32( v );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadUInt4(const pSource: TXMUINT4): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_loadu_si128( reinterpret_cast<const __m128i*>(pSource) );
           MOVDQU  XMM0, [pSource]
           // For the values that are higher than 0x7FFFFFFF, a fixup is needed
           // Determine which ones need the fix.
           // vMask = _mm_and_ps(_mm_castsi128_ps(V),g_XMNegativeZero);
           MOVUPS  XMM1,XMM0
           ANDPS   XMM1, [g_XMNegativeZero] // XMM1 = vMask
           // Force all values positive
           // vResult = _mm_xor_ps(_mm_castsi128_ps(V),vMask);
           XORPS   XMM0,XMM1
           // Convert to floats
           // vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
           CVTDQ2PS XMM0, XMM0
           // Convert 0x80000000 -> 0xFFFFFFFF
           //__m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
           PSRAD   XMM1, 31
           // For only the ones that are too big, add the fixup
           // vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
           ANDPS   XMM1, [g_XMFixUnsigned]
           // vResult = _mm_add_ps(vResult,vMask);
           ADDPS   XMM0,XMM1
           // return vResult;
           MOVUPS  [Result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat3x3(const pSource: TXMFLOAT3X3): TXMMATRIX;
begin
    Result.r[0].f32[0] := pSource.m[0, 0];
    Result.r[0].f32[1] := pSource.m[0, 1];
    Result.r[0].f32[2] := pSource.m[0, 2];
    Result.r[0].f32[3] := 0.0;

    Result.r[1].f32[0] := pSource.m[1, 0];
    Result.r[1].f32[1] := pSource.m[1, 1];
    Result.r[1].f32[2] := pSource.m[1, 2];
    Result.r[1].f32[3] := 0.0;

    Result.r[2].f32[0] := pSource.m[2, 0];
    Result.r[2].f32[1] := pSource.m[2, 1];
    Result.r[2].f32[2] := pSource.m[2, 2];
    Result.r[2].f32[3] := 0.0;
    Result.r[3].f32[0] := 0.0;
    Result.r[3].f32[1] := 0.0;
    Result.r[3].f32[2] := 0.0;
    Result.r[3].f32[3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat3x3(const pSource: TXMFLOAT3X3): TXMMATRIX;
begin
    (* ToDo
    float32x4_t v0 = vld1q_f32( &pSource.m[0,0] );
    float32x4_t v1 = vld1q_f32( &pSource.m[1,1] );
    float32x2_t v2 = vcreate_f32( (uint64_t)*(const uint32_t* )&pSource.m[2,2] );
    float32x4_t T = vextq_f32( v0, v1, 3 );

    XMMATRIX M;
    result.r[0] = vandq_u32( v0, g_XMMask3 );
    result.r[1] = vandq_u32( T, g_XMMask3 );
    result.r[2] = vcombine_f32( vget_high_f32(v1), v2 );
    result.r[3] = g_XMIdentityR3;
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat3x3(const pSource: TXMFLOAT3X3): TXMMATRIX; assembler;
asm
           // __m128 Z = _mm_setzero_ps();
           XORPS   XMM0, XMM0

           // __m128 V1 = _mm_loadu_ps( &pSource.m[0,0] );
           MOVUPS  XMM1, TXMFLOAT3X3([pSource])._11
           // __m128 V2 = _mm_loadu_ps( &pSource.m[1,1] );
           MOVUPS  XMM2, TXMFLOAT3X3([pSource])._22
           // __m128 V3 = _mm_load_ss( &pSource.m[2,2] );
           MOVUPS  XMM3, TXMFLOAT3X3([pSource])._33

           // __m128 T1 = _mm_unpackhi_ps( V1, Z );
           MOVUPS  XMM4,XMM1
           UNPCKHPS XMM4, XMM0

           // __m128 T2 = _mm_unpacklo_ps( V2, Z );
           MOVUPS  XMM5,XMM2
           UNPCKHPS XMM5, XMM0
           // __m128 T3 = _mm_shuffle_ps( V3, T2, _MM_SHUFFLE( 0, 1, 0, 0 ) );
           MOVUPS  XMM6,XMM3
           SHUFPS  XMM6, XMM5, _MM_SHUFFLE_0_1_0_0

           // __m128 T4 = _mm_movehl_ps( T2, T3 );
           MOVHLPS XMM5, XMM6 // T4= XMM5

           // __m128 T5 = _mm_movehl_ps( Z, T1 );
           MOVHLPS XMM0, XMM4 // T5 = XMM0

           // result.r[0] = _mm_movelh_ps( V1, T1 );
           MOVHLPS XMM1, XMM4
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r0), XMM1

           // result.r[1] = _mm_add_ps( T4, T5 );
           ADDPS   XMM5,XMM0
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r1), XMM0

           // result.r[2] = _mm_shuffle_ps( V2, V3, _MM_SHUFFLE(1, 0, 3, 2) );
           SHUFPS  XMM2, XMM3, _MM_SHUFFLE_1_0_3_2
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r2), XMM2

           // result.r[3] = g_XMIdentityR3;
           MOVUPS  XMM0, [g_XMIdentityR3]
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r3), XMM0

end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat4x3(const pSource: TXMFLOAT4X3): TXMMATRIX;
begin
    Result.r[0].f32[0] := pSource.m[0, 0];
    Result.r[0].f32[1] := pSource.m[0, 1];
    Result.r[0].f32[2] := pSource.m[0, 2];
    Result.r[0].f32[3] := 0.0;

    Result.r[1].f32[0] := pSource.m[1, 0];
    Result.r[1].f32[1] := pSource.m[1, 1];
    Result.r[1].f32[2] := pSource.m[1, 2];
    Result.r[1].f32[3] := 0.0;

    Result.r[2].f32[0] := pSource.m[2, 0];
    Result.r[2].f32[1] := pSource.m[2, 1];
    Result.r[2].f32[2] := pSource.m[2, 2];
    Result.r[2].f32[3] := 0.0;

    Result.r[3].f32[0] := pSource.m[3, 0];
    Result.r[3].f32[1] := pSource.m[3, 1];
    Result.r[3].f32[2] := pSource.m[3, 2];
    Result.r[3].f32[3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat4x3(const pSource: TXMFLOAT4X3): TXMMATRIX;
begin
    (* ToDo
     float32x4_t v0 = vld1q_f32( &pSource.m[0,0] );
    float32x4_t v1 = vld1q_f32( &pSource.m[1,1] );
    float32x4_t v2 = vld1q_f32( &pSource.m[2,2] );

    float32x4_t T1 = vextq_f32( v0, v1, 3 );
    float32x4_t T2 = vcombine_f32( vget_high_f32(v1), vget_low_f32(v2) );
    float32x4_t T3 = vextq_f32( v2, v2, 1 );

    XMMATRIX M;
    result.r[0] = vandq_u32( v0, g_XMMask3 );
    result.r[1] = vandq_u32( T1, g_XMMask3 );
    result.r[2] = vandq_u32( T2, g_XMMask3 );
    result.r[3] = vsetq_lane_f32( 1.f, T3, 3 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat4x3(const pSource: TXMFLOAT4X3): TXMMATRIX; assembler;
asm
           // Use unaligned load instructions to
           // load the 12 floats
           // vTemp1 = x1,y1,z1,x2
           // vTemp1 = _mm_loadu_ps(&pSource.m[0,0]);
           MOVUPS  XMM1, TXMFLOAT4X3([pSource])._11;
           // vTemp2 = y2,z2,x3,y3
           // vTemp2 = _mm_loadu_ps(&pSource.m[1,1]);
           MOVUPS  XMM2, TXMFLOAT4X3([pSource])._22;
           // vTemp4 = z3,x4,y4,z4
           // vTemp4 = _mm_loadu_ps(&pSource.m[2,2]);
           MOVUPS  XMM4, TXMFLOAT4X3([pSource])._33;
           // vTemp3 = x3,y3,z3,z3
           // vTemp3 = _mm_shuffle_ps(vTemp2,vTemp4,_MM_SHUFFLE(0,0,3,2));
           MOVUPS  XMM3, XMM2
           SHUFPS  XMM3, XMM4, _MM_SHUFFLE_0_0_3_2

           // vTemp2 = y2,z2,x2,x2
           // vTemp2 = _mm_shuffle_ps(vTemp2,vTemp1,_MM_SHUFFLE(3,3,1,0));
           SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
           // vTemp2 = x2,y2,z2,z2
           // vTemp2 = XM_PERMUTE_PS(vTemp2,_MM_SHUFFLE(1,1,0,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
           // vTemp1 = x1,y1,z1,0
           ANDPS   XMM1, [g_XMMask3] // vTemp1 = _mm_and_ps(vTemp1,g_XMMask3);
           // vTemp2 = x2,y2,z2,0
           ANDPS   XMM2, [g_XMMask3] //vTemp2 = _mm_and_ps(vTemp2,g_XMMask3);
           // vTemp3 = x3,y3,z3,0
           ANDPS   XMM3, [g_XMMask3] // vTemp3 = _mm_and_ps(vTemp3,g_XMMask3);
           // vTemp4i = x4,y4,z4,0

           // __m128i vTemp4i = _mm_srli_si128(_mm_castps_si128(vTemp4),32/8);
           PSRLDQ  XMM4, 4
           // vTemp4i = x4,y4,z4,1.0f
           // vTemp4i = _mm_or_si128(vTemp4i,g_XMIdentityR3);
           POR     XMM4, [g_XMIdentityR3]
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r0), XMM1
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r1), XMM2
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r2), XMM3
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r3), XMM4
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat4x3A(const pSource: TXMFLOAT4X3A): TXMMATRIX;
begin
    Result.r[0].f32[0] := pSource.m[0, 0];
    Result.r[0].f32[1] := pSource.m[0, 1];
    Result.r[0].f32[2] := pSource.m[0, 2];
    Result.r[0].f32[3] := 0.0;

    Result.r[1].f32[0] := pSource.m[1, 0];
    Result.r[1].f32[1] := pSource.m[1, 1];
    Result.r[1].f32[2] := pSource.m[1, 2];
    Result.r[1].f32[3] := 0.0;

    Result.r[2].f32[0] := pSource.m[2, 0];
    Result.r[2].f32[1] := pSource.m[2, 1];
    Result.r[2].f32[2] := pSource.m[2, 2];
    Result.r[2].f32[3] := 0.0;

    Result.r[3].f32[0] := pSource.m[3, 0];
    Result.r[3].f32[1] := pSource.m[3, 1];
    Result.r[3].f32[2] := pSource.m[3, 2];
    Result.r[3].f32[3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat4x3A(const pSource: TXMFLOAT4X3A): TXMMATRIX;
begin
    (* ToDo
    float32x4_t v0 = vld1q_f32_ex( &pSource.m[0,0], 128 );
    float32x4_t v1 = vld1q_f32_ex( &pSource.m[1,1], 128 );
    float32x4_t v2 = vld1q_f32_ex( &pSource.m[2,2], 128 );

    float32x4_t T1 = vextq_f32( v0, v1, 3 );
    float32x4_t T2 = vcombine_f32( vget_high_f32(v1), vget_low_f32(v2) );
    float32x4_t T3 = vextq_f32( v2, v2, 1 );

    XMMATRIX M;
    result.r[0] = vandq_u32( v0, g_XMMask3 );
    result.r[1] = vandq_u32( T1, g_XMMask3 );
    result.r[2] = vandq_u32( T2, g_XMMask3 );
    result.r[3] = vsetq_lane_f32( 1.f, T3, 3 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat4x3A(const pSource: TXMFLOAT4X3A): TXMMATRIX; assembler;
asm
           // Use aligned load instructions to
           // load the 12 floats
           // vTemp1 = x1,y1,z1,x2
           // vTemp1 = _mm_load_ps(&pSource.m[0,0]);
           MOVAPS  XMM1, TXMFLOAT4X3A([pSource])._11
           // vTemp2 = y2,z2,x3,y3
           // vTemp2 = _mm_load_ps(&pSource.m[1,1]);
           MOVAPS  XMM2, TXMFLOAT4X3A([pSource])._22
           // vTemp4 = z3,x4,y4,z4
           // vTemp4 = _mm_load_ps(&pSource.m[2,2]);
           MOVAPS  XMM4, TXMFLOAT4X3A([pSource])._33
           // vTemp3 = x3,y3,z3,z3
           // vTemp3 = _mm_shuffle_ps(vTemp2,vTemp4,_MM_SHUFFLE(0,0,3,2));
           MOVUPS  XMM3,XMM2
           SHUFPS  XMM3, XMM4, _MM_SHUFFLE_0_0_3_2
           // vTemp2 = y2,z2,x2,x2
           // vTemp2 = _mm_shuffle_ps(vTemp2,vTemp1,_MM_SHUFFLE(3,3,1,0));
           SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
           // vTemp2 = x2,y2,z2,z2
           // vTemp2 = XM_PERMUTE_PS(vTemp2,_MM_SHUFFLE(1,1,0,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
           // vTemp1 = x1,y1,z1,0
           // vTemp1 = _mm_and_ps(vTemp1,g_XMMask3);
           ANDPS   XMM1, [g_XMMask3]
           // vTemp2 = x2,y2,z2,0
           // vTemp2 = _mm_and_ps(vTemp2,g_XMMask3);
           ANDPS   XMM2,[g_XMMask3]
           // vTemp3 = x3,y3,z3,0
           // vTemp3 = _mm_and_ps(vTemp3,g_XMMask3);
           ANDPS   XMM3,[g_XMMask3]
           // vTemp4i = x4,y4,z4,0
           // __m128i vTemp4i = _mm_srli_si128(_mm_castps_si128(vTemp4),32/8);
           PSRLDQ  XMM4, 4
           // vTemp4i = x4,y4,z4,1.0f
           //vTemp4i = _mm_or_si128(vTemp4i,g_XMIdentityR3);
           POR     XMM4, [g_XMIdentityR3]
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r0), XMM1
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r1), XMM2
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r2), XMM3
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r3), XMM4
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat4x4(const pSource: TXMFLOAT4X4): TXMMATRIX;
begin
    Result.r[0].f32[0] := pSource.m[0, 0];
    Result.r[0].f32[1] := pSource.m[0, 1];
    Result.r[0].f32[2] := pSource.m[0, 2];
    Result.r[0].f32[3] := pSource.m[0, 3];

    Result.r[1].f32[0] := pSource.m[1, 0];
    Result.r[1].f32[1] := pSource.m[1, 1];
    Result.r[1].f32[2] := pSource.m[1, 2];
    Result.r[1].f32[3] := pSource.m[1, 3];

    Result.r[2].f32[0] := pSource.m[2, 0];
    Result.r[2].f32[1] := pSource.m[2, 1];
    Result.r[2].f32[2] := pSource.m[2, 2];
    Result.r[2].f32[3] := pSource.m[2, 3];

    Result.r[3].f32[0] := pSource.m[3, 0];
    Result.r[3].f32[1] := pSource.m[3, 1];
    Result.r[3].f32[2] := pSource.m[3, 2];
    Result.r[3].f32[3] := pSource.m[3, 3];

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat4x4(const pSource: TXMFLOAT4X4): TXMMATRIX;
begin
    (* ToDo
     XMMATRIX M;
    result.r[0] = vld1q_f32( reinterpret_cast<const float*>(&pSource._11) );
    result.r[1] = vld1q_f32( reinterpret_cast<const float*>(&pSource._21) );
    result.r[2] = vld1q_f32( reinterpret_cast<const float*>(&pSource._31) );
    result.r[3] = vld1q_f32( reinterpret_cast<const float*>(&pSource._41) );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat4x4(const pSource: TXMFLOAT4X4): TXMMATRIX; assembler;
asm
           // result.r[0] = _mm_loadu_ps( &pSource._11 );
           MOVUPS  XMM0, TXMFLOAT4X4([pSource])._11
           MOVUPS  TXMMATRIX([result]).r0, XMM0
           // result.r[1] = _mm_loadu_ps( &pSource._21 );
           MOVUPS  XMM0, TXMFLOAT4X4([pSource])._21
           MOVUPS  TXMMATRIX([result]).r1, XMM0
           // result.r[2] = _mm_loadu_ps( &pSource._31 );
           MOVUPS  XMM0, TXMFLOAT4X4([pSource])._31
           MOVUPS  TXMMATRIX([result]).r2, XMM0
           // result.r[3] = _mm_loadu_ps( &pSource._41 );
           MOVUPS  XMM0, TXMFLOAT4X4([pSource])._41
           MOVUPS  TXMMATRIX([result]).r3, XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMLoadFloat4x4A(const pSource: TXMFLOAT4X4A): TXMMATRIX;
begin
    Result.r[0].f32[0] := pSource.m[0, 0];
    Result.r[0].f32[1] := pSource.m[0, 1];
    Result.r[0].f32[2] := pSource.m[0, 2];
    Result.r[0].f32[3] := pSource.m[0, 3];

    Result.r[1].f32[0] := pSource.m[1, 0];
    Result.r[1].f32[1] := pSource.m[1, 1];
    Result.r[1].f32[2] := pSource.m[1, 2];
    Result.r[1].f32[3] := pSource.m[1, 3];

    Result.r[2].f32[0] := pSource.m[2, 0];
    Result.r[2].f32[1] := pSource.m[2, 1];
    Result.r[2].f32[2] := pSource.m[2, 2];
    Result.r[2].f32[3] := pSource.m[2, 3];

    Result.r[3].f32[0] := pSource.m[3, 0];
    Result.r[3].f32[1] := pSource.m[3, 1];
    Result.r[3].f32[2] := pSource.m[3, 2];
    Result.r[3].f32[3] := pSource.m[3, 3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMLoadFloat4x4A(const pSource: TXMFLOAT4X4A): TXMMATRIX;
begin
    (* ToDo
      XMMATRIX M;
    result.r[0] = vld1q_f32_ex( reinterpret_cast<const float*>(&pSource._11), 128 );
    result.r[1] = vld1q_f32_ex( reinterpret_cast<const float*>(&pSource._21), 128 );
    result.r[2] = vld1q_f32_ex( reinterpret_cast<const float*>(&pSource._31), 128 );
    result.r[3] = vld1q_f32_ex( reinterpret_cast<const float*>(&pSource._41), 128 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMLoadFloat4x4A(const pSource: TXMFLOAT4X4A): TXMMATRIX; assembler;
asm
           // result.r[0] = _mm_load_ps( &pSource._11 );
           MOVAPS  XMM0, TXMFLOAT4X4([pSource])._11
           MOVAPS  TXMMATRIX([result]).r0, XMM0

           // result.r[1] = _mm_load_ps( &pSource._21 );
           MOVAPS  XMM0, TXMFLOAT4X4([pSource])._21
           MOVAPS  TXMMATRIX([result]).r1, XMM0
           //  result.r[2] = _mm_load_ps( &pSource._31 );
           MOVAPS  XMM0, TXMFLOAT4X4([pSource])._31
           MOVAPS  TXMMATRIX([result]).r2, XMM0
           // result.r[3] = _mm_load_ps( &pSource._41 );
           MOVAPS  XMM0, TXMFLOAT4X4([pSource])._41
           MOVAPS  TXMMATRIX([result]).r3, XMM0

end;
{$ENDIF}


{****************************************************************************
 *
 * Vector and matrix store operations
 *
 ****************************************************************************}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreInt(out pDestination: UINT32; V: TXMVECTOR);
begin
    pDestination := XMVectorGetIntX(V);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreInt(out pDestination: PUINT32; V: TXMVECTOR);
begin
    (* ToDo
     vst1q_lane_u32( pDestination, *reinterpret_cast<const uint32x4_t*>(&V), 0 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreInt(out pDestination: UINT32; constref V: TXMVECTOR); assembler;
asm
           // _mm_store_ss( reinterpret_cast<float*>(pDestination), V );
           MOVUPS  XMM0, [V]
           MOVSS   [pDestination], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat(out pDestination: single; V: TXMVECTOR);
begin
    pDestination := XMVectorGetX(V);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat(out pDestination: single; V: TXMVECTOR);
begin
    (* ToDo
   vst1q_lane_f32( pDestination, V, 0 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat(out pDestination: single; V: TXMVECTOR); assembler;
asm
           // _mm_store_ss( pDestination, V );
           MOVUPS  XMM0, [V]
           MOVSS   [pDestination], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreInt2(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    pDestination[0] := V.u32[0];
    pDestination[1] := V.u32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreInt2(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    (* ToDo
    uint32x2_t VL = vget_low_u32(V);
    vst1_u32( pDestination, VL );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreInt2(out pDestination: PUINT32; constref V: TXMVECTOR); assembler;
asm
           // T = XM_PERMUTE_PS( V, _MM_SHUFFLE( 1, 1, 1, 1 ) );
           MOVUPS  XMM0, [V]
           MOVUPS  XMM1, XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination[0]), V );
           MOVSS   pDestination[0], XMM0
           // _mm_store_ss( reinterpret_cast<float*>(&pDestination[1]), T );
           MOVSS   pDestination[1], XMM1
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreInt2A(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    pDestination[0] := V.u32[0];
    pDestination[1] := V.u32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreInt2A(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    (* ToDo
     uint32x2_t VL = vget_low_u32(V);
    vst1_u32_ex( pDestination, VL, 64 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreInt2A(out pDestination: PUINT32; constref V: TXMVECTOR); assembler;
asm
           // _mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
           MOVUPS  XMM0, [V]
           MOVQ    [pDestination], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat2(out pDestination: TXMFLOAT2; constref V: TXMVECTOR);
begin
    pDestination.x := V.f32[0];
    pDestination.y := V.f32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat2(out pDestination: TXMFLOAT2; constref V: TXMVECTOR);
begin
    (* ToDo
    float32x2_t VL = vget_low_f32(V);
    vst1_f32( reinterpret_cast<float*>(pDestination), VL );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat2(out pDestination: TXMFLOAT2; constref V: TXMVECTOR); assembler;
asm
           // T = XM_PERMUTE_PS( V, _MM_SHUFFLE( 1, 1, 1, 1 ) );
           MOVUPS  XMM0, [V]
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // _mm_store_ss( &pDestination.x, V );
           MOVSS   TXMFLOAT2([pDestination]).x, XMM0
           // _mm_store_ss( &pDestination.y, T );
           MOVSS   TXMFLOAT2([pDestination]).y, XMM1
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat2A(out pDestination: TXMFLOAT2A; constref V: TXMVECTOR);
begin
    pDestination.x := v.f32[0];
    pDestination.y := V.f32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat2A(out pDestination: TXMFLOAT2A; constref V: TXMVECTOR);
begin
    (* ToDo
    float32x2_t VL = vget_low_f32(V);
    vst1_f32_ex( reinterpret_cast<float*>(pDestination), VL, 64 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat2A(out pDestination: TXMFLOAT2A; constref V: TXMVECTOR); assembler;
asm
           //_mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
           MOVUPS  XMM0,[V]
           MOVQ    [pDestination], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreSInt2(out pDestination: TXMINT2; constref V: TXMVECTOR);
begin
    pDestination.x := trunc(V.f32[0]);
    pDestination.y := trunc(V.f32[1]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreSInt2(out pDestination: TXMINT2; constref V: TXMVECTOR);
begin
    (* ToDo
     int32x2_t v = vget_low_s32(V);
    v = vcvt_s32_f32( v );
    vst1_s32( reinterpret_cast<int32_t*>(pDestination), v );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreSInt2(out pDestination: TXMINT2; constref V: TXMVECTOR); assembler;
asm
           // In case of positive overflow, detect it
           // vOverflow = _mm_cmpgt_ps(V,g_XMMaxInt);
           MOVUPS  XMM0,[g_XMMaxInt]
           CMPPS   XMM0, [V], 1
           // Float to int conversion
           //_m128i vResulti = _mm_cvttps_epi32(V);
           CVTTPS2DQ XMM1, [V]
           // If there was positive overflow, set to 0x7FFFFFFF
           // vResult = _mm_and_ps(vOverflow,g_XMAbsMask);
           MOVUPS  XMM2,XMM0
           ANDPS   XMM2,[g_XMAbsMask]

           //vOverflow = _mm_andnot_ps(vOverflow,_mm_castsi128_ps(vResulti));
           ANDNPS  XMM0, XMM1
           // vOverflow = _mm_or_ps(vOverflow,vResult);
           ORPS    XMM0, XMM2
           // Write two ints
           // T = XM_PERMUTE_PS( vOverflow, _MM_SHUFFLE( 1, 1, 1, 1 ) );
           MOVUPS  XMM3,XMM0
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_1_1_1_1
           // _mm_store_ss( reinterpret_cast<float*>(&pDestination.x), vOverflow );
           MOVSS   TXMINT2([pDestination]).x, XMM0
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination.y), T );
           MOVSS   TXMINT2([pDestination]).y, XMM3
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreUInt2(out pDestination: TXMUINT2; constref V: TXMVECTOR);
begin
    pDestination.x := trunc(V.f32[0]);
    pDestination.y := trunc(V.f32[1]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreUInt2(out pDestination: TXMUINT2; constref V: TXMVECTOR);
begin
    (* ToDo
    float32x2_t v = vget_low_f32(V);
    uint32x2_t iv = vcvt_u32_f32( v );
    vst1_u32( reinterpret_cast<uint32_t*>(pDestination), iv );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreUInt2(out pDestination: TXMUINT2; constref V: TXMVECTOR); assembler;
asm
           // Clamp to >=0
           // vResult = _mm_max_ps(V,g_XMZero);
           MOVUPS  XMM0, [V]
           MAXPS   XMM0, [g_XMZero]
           // Any numbers that are too big, set to 0xFFFFFFFFU
           // vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
           MOVUPS  XMM1,[g_XMMaxUInt]
           CMPPS   XMM1, XMM0, 1
           // vValue = g_XMUnsignedFix;
           MOVUPS  XMM2, [g_XMUnsignedFix]
           // Too large for a signed integer?
           // vMask = _mm_cmpge_ps(vResult,vValue);
           MOVUPS  XMM3, XMM2
           CMPPS   XMM3, XMM0, 2
           // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
           // vValue = _mm_and_ps(vValue,vMask);
           ANDPS   XMM2, XMM3
           // Perform fixup only on numbers too large (Keeps low bit precision)
           // vResult = _mm_sub_ps(vResult,vValue);
           SUBPS   XMM0, XMM2
           // __m128i vResulti = _mm_cvttps_epi32(vResult);
           CVTTPS2DQ XMM4, XMM0
           // Convert from signed to unsigned pnly if greater than 0x80000000
           // vMask = _mm_and_ps(vMask,g_XMNegativeZero);
           ANDPS   XMM3, [g_XMNegativeZero]
           // vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
           XORPS   XMM4, XMM3
           // On those that are too large, set to 0xFFFFFFFF
           // vResult = _mm_or_ps(vResult,vOverflow);
           ORPS    XMM4, XMM1
           // Write two uints
           // T = XM_PERMUTE_PS( vResult, _MM_SHUFFLE( 1, 1, 1, 1 ) );
           MOVUPS  XMM5,XMM4
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_1_1_1_1
           // _mm_store_ss( reinterpret_cast<float*>(&pDestination.x), vResult );
           MOVSS   TXMUINT2([pDestination]).x, XMM4
           // _mm_store_ss( reinterpret_cast<float*>(&pDestination.y), T );
           MOVSS   TXMUINT2([pDestination]).y, XMM5
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreInt3(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    pDestination[0] := V.u32[0];
    pDestination[1] := V.u32[1];
    pDestination[2] := V.u32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreInt3(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    (* ToDo
    uint32x2_t VL = vget_low_u32(V);
    vst1_u32( pDestination, VL );
    vst1q_lane_u32( pDestination+2, *reinterpret_cast<const uint32x4_t*>(&V), 2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreInt3(out pDestination: PUINT32; constref V: TXMVECTOR); assembler;
asm
           // T1 = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_1_1_1
           // T2 = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM1, [V]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2
           // _mm_store_ss( reinterpret_cast<float*>(pDestination), V );
           MOVUPS  XMM2, [V]
           MOVSS   pDestination[0], XMM2
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination[1]), T1 );
           MOVSS   pDestination[1], XMM0
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination[2]), T2 );
           MOVSS   pDestination[2], XMM1
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreInt3A(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    pDestination[0] := V.u32[0];
    pDestination[1] := V.u32[1];
    pDestination[2] := V.u32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreInt3A(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    (* ToDo
    uint32x2_t VL = vget_low_u32(V);
    vst1_u32_ex( pDestination, VL, 64 );
    vst1q_lane_u32( pDestination+2, *reinterpret_cast<const uint32x4_t*>(&V), 2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreInt3A(out pDestination: PUINT32; constref V: TXMVECTOR); assembler;
asm
           // T = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM0, [V]
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2
           //_mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );

           MOVQ    [pDestination], XMM0
           // _mm_store_ss( reinterpret_cast<float*>(&pDestination[2]), T );
           MOVSS   pDestination[2], XMM1
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat3(out pDestination: TXMFLOAT3; constref V: TXMVECTOR); inline;
begin
    pDestination.x := V.f32[0];
    pDestination.y := V.f32[1];
    pDestination.z := V.f32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat3(out pDestination: TXMFLOAT3; constref V: TXMVECTOR); inline;
begin
    (* ToDo
   float32x2_t VL = vget_low_f32(V);
    vst1_f32( reinterpret_cast<float*>(pDestination), VL );
    vst1q_lane_f32( reinterpret_cast<float*>(pDestination)+2, V, 2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat3(out pDestination: TXMFLOAT3; constref V: TXMVECTOR); inline; assembler;
asm
           // T1 = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM0, [V]
           MOVUPS  XMM1,XMM0
           MOVUPS  XMM2,XMM0
           SHUFPS  XMM0,XMM0, _MM_SHUFFLE_1_1_1_1
           // T2 = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2
           // _mm_store_ss( &pDestination.x, V );
           MOVSS   TXMFLOAT3([pDestination]).x, XMM2
           // _mm_store_ss( &pDestination.y, T1 );
           MOVSS   TXMFLOAT3([pDestination]).y, XMM0
           // _mm_store_ss( &pDestination.z, T2 );
           MOVSS   TXMFLOAT3([pDestination]).z, XMM1
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat3A(out pDestination: TXMFLOAT3A; constref V: TXMVECTOR);
begin
    pDestination.x := V.f32[0];
    pDestination.y := V.f32[1];
    pDestination.z := V.f32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat3A(out pDestination: TXMFLOAT3A; constref V: TXMVECTOR);
begin
    (* ToDo
     float32x2_t VL = vget_low_f32(V);
    vst1_f32_ex( reinterpret_cast<float*>(pDestination), VL, 64 );
    vst1q_lane_f32( reinterpret_cast<float*>(pDestination)+2, V, 2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat3A(out pDestination: TXMFLOAT3A; constref V: TXMVECTOR); assembler;
asm
           // T = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM0,[v]
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2
           // _mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
           MOVQ    TXMFLOAT3A([pDestination]), XMM1
           //_mm_store_ss( &pDestination.z, T );
           MOVSS   TXMFLOAT3A([pDestination]).z, XMM1
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreSInt3(out pDestination: TXMINT3; constref V: TXMVECTOR);
begin
    pDestination.x := trunc(V.f32[0]);
    pDestination.y := trunc(V.f32[1]);
    pDestination.z := trunc(V.f32[2]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreSInt3(out pDestination: TXMINT3; constref V: TXMVECTOR);
begin
    (* ToDo
    int32x4_t v = vcvtq_s32_f32(V);
    int32x2_t vL = vget_low_s32(v);
    vst1_s32( reinterpret_cast<int32_t*>(pDestination), vL );
    vst1q_lane_s32( reinterpret_cast<int32_t*>(pDestination)+2, v, 2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreSInt3(out pDestination: TXMINT3; constref V: TXMVECTOR); assembler;
asm
           // In case of positive overflow, detect it
           // vOverflow = _mm_cmpgt_ps(V,g_XMMaxInt);
           MOVUPS  XMM0,[g_XMMaxInt]
           MOVUPS  XMM1,[V]
           CMPPS   XMM0, XMM1, 1  // vOverflow = XMM0
           // Float to int conversion
           //__m128i vResulti = _mm_cvttps_epi32(V);
           CVTTPS2DQ XMM1, XMM1  // vResulti = XMM1
           // If there was positive overflow, set to 0x7FFFFFFF
           // vResult = _mm_and_ps(vOverflow,g_XMAbsMask);
           MOVUPS  XMM2,XMM0  // vResult = XMM2
           ANDPS   XMM2, [g_XMAbsMask]
           // vOverflow = _mm_andnot_ps(vOverflow,_mm_castsi128_ps(vResulti));
           ANDNPS  XMM0, XMM1
           // vOverflow = _mm_or_ps(vOverflow,vResult);
           ORPS    XMM0, XMM2
           // Write 3 uints
           // T1 = XM_PERMUTE_PS(vOverflow,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM3,XMM0
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_1_1_1_1 // T1 = XMM3
           // T2 = XM_PERMUTE_PS(vOverflow,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM4,XMM0
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_2_2_2_2 // T2 = XMM4
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination.x), vOverflow );
           MOVSS   TXMINT3([pDestination]).x, XMM0
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination.y), T1 );
           MOVSS   TXMINT3([pDestination]).y, XMM3
           // _mm_store_ss( reinterpret_cast<float*>(&pDestination.z), T2 );
           MOVSS   TXMINT3([pDestination]).z, XMM4
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreUInt3(out pDestination: TXMUINT3; constref V: TXMVECTOR);
begin
    pDestination.x := trunc(V.f32[0]);
    pDestination.y := trunc(V.f32[1]);
    pDestination.z := trunc(V.f32[2]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreUInt3(out pDestination: TXMUINT3; constref V: TXMVECTOR);
begin
    (* ToDo
    uint32x4_t v = vcvtq_u32_f32(V);
    uint32x2_t vL = vget_low_u32(v);
    vst1_u32( reinterpret_cast<uint32_t*>(pDestination), vL );
    vst1q_lane_u32( reinterpret_cast<uint32_t*>(pDestination)+2, v, 2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreUInt3(out pDestination: TXMUINT3; constref V: TXMVECTOR); assembler;
asm
           // Clamp to >=0
           // vResult = _mm_max_ps(V,g_XMZero);
           MOVUPS  XMM0,[V]
           MAXPS   XMM0, [g_XMZero] // vResult = XMM0
           // Any numbers that are too big, set to 0xFFFFFFFFU
           // vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
           MOVUPS  XMM1,[g_XMMaxUInt]
           CMPPS   XMM1, XMM0, 1  // vOverflow = XMM1
           // vValue = g_XMUnsignedFix;
           MOVUPS  XMM2, [g_XMUnsignedFix]
           // Too large for a signed integer?
           // vMask = _mm_cmpge_ps(vResult,vValue);
           MOVUPS  XMM3,XMM2
           CMPPS   XMM3, XMM0, 2
           // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
           // vValue = _mm_and_ps(vValue,vMask);
           ANDPS   XMM2,XMM3
           // Perform fixup only on numbers too large (Keeps low bit precision)
           // vResult = _mm_sub_ps(vResult,vValue);
           SUBPS   XMM0, XMM2
           //__m128i vResulti = _mm_cvttps_epi32(vResult);
           CVTTPS2DQ XMM0, XMM0
           // Convert from signed to unsigned pnly if greater than 0x80000000
           // vMask = _mm_and_ps(vMask,g_XMNegativeZero);
           ANDPS   XMM3, [g_XMNegativeZero]
           // vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
           XORPS   XMM0, XMM3
           // On those that are too large, set to 0xFFFFFFFF
           // vResult = _mm_or_ps(vResult,vOverflow);
           ORPS    XMM0, XMM1
           // Write 3 uints
           // T1 = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // T2 = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM2,XMM0
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination.x), vResult );
           MOVSS   TXMUINT3([pDestination]).x, XMM0
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination.y), T1 );
           MOVSS   TXMUINT3([pDestination]).y, XMM1
           //_mm_store_ss( reinterpret_cast<float*>(&pDestination.z), T2 );
           MOVSS   TXMUINT3([pDestination]).z, XMM2
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreInt4(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    pDestination[0] := V.u32[0];
    pDestination[1] := V.u32[1];
    pDestination[2] := V.u32[2];
    pDestination[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreInt4(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    (* ToDo
    vst1q_u32( pDestination, V );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreInt4(out pDestination: PUINT32; constref V: TXMVECTOR); assembler;
asm
           // _mm_storeu_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
           MOVUPS  XMM0, [V]
           MOVDQU  [pDestination], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreInt4A(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    pDestination[0] := V.u32[0];
    pDestination[1] := V.u32[1];
    pDestination[2] := V.u32[2];
    pDestination[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreInt4A(out pDestination: PUINT32; constref V: TXMVECTOR);
begin
    (* ToDo
    vst1q_u32_ex( pDestination, V, 128 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreInt4A(out pDestination: PUINT32; constref V: TXMVECTOR); assembler;
asm
           //_mm_store_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
           MOVUPS  XMM0, [V]
           MOVDQA  [pDestination], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat4(out pDestination: TXMFLOAT4; constref V: TXMVECTOR);
begin
    pDestination.x := V.f32[0];
    pDestination.y := V.f32[1];
    pDestination.z := V.f32[2];
    pDestination.w := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat4(out pDestination: TXMFLOAT4; constref V: TXMVECTOR);
begin
    (* ToDo
    vst1q_f32( reinterpret_cast<float*>(pDestination), V );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat4(out pDestination: TXMFLOAT4; constref V: TXMVECTOR); assembler;
asm
           // _mm_storeu_ps( &pDestination.x, V );
           MOVUPS  XMM0, [V]
           MOVUPS  [pDestination], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat4A(out pDestination: TXMFLOAT4A; constref V: TXMVECTOR);
begin
    pDestination.x := V.f32[0];
    pDestination.y := V.f32[1];
    pDestination.z := V.f32[2];
    pDestination.w := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat4A(out pDestination: TXMFLOAT4A; constref V: TXMVECTOR);
begin
    (* ToDo
   vst1q_f32_ex( reinterpret_cast<float*>(pDestination), V, 128 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat4A(out pDestination: TXMFLOAT4A; constref V: TXMVECTOR); assembler;
asm
           // _mm_store_ps( &pDestination.x, V );
           MOVUPS  XMM0, [V]
           MOVAPS  [pDestination], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreSInt4(out pDestination: TXMINT4; constref V: TXMVECTOR);
begin
    pDestination.x := trunc(V.f32[0]);
    pDestination.y := trunc(V.f32[1]);
    pDestination.z := trunc(V.f32[2]);
    pDestination.w := trunc(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreSInt4(out pDestination: TXMINT4; constref V: TXMVECTOR);
begin
    (* ToDo
    int32x4_t v = vcvtq_s32_f32(V);
    vst1q_s32( reinterpret_cast<int32_t*>(pDestination), v );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreSInt4(out pDestination: TXMINT4; constref V: TXMVECTOR); assembler;
asm
           // In case of positive overflow, detect it
           // vOverflow = _mm_cmpgt_ps(V,g_XMMaxInt);
           MOVUPS  XMM0, [g_XMMaxInt]
           CMPPS   XMM0, [V], 1
           // Float to int conversion
           //__m128i vResulti = _mm_cvttps_epi32(V);
           CVTTPS2DQ XMM1, [V]
           // If there was positive overflow, set to 0x7FFFFFFF
           // vResult = _mm_and_ps(vOverflow,g_XMAbsMask);
           MOVUPS  XMM2, XMM0
           ANDPS   XMM2, [g_XMAbsMask]
           // vOverflow = _mm_andnot_ps(vOverflow,_mm_castsi128_ps(vResulti));
           ANDNPS  XMM0, XMM1
           // vOverflow = _mm_or_ps(vOverflow,vResult);
           ORPS    XMM0, XMM2
           // _mm_storeu_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(vOverflow) );
           MOVDQU  [pDestination], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreUInt4(out pDestination: TXMUINT4; constref V: TXMVECTOR);
begin
    pDestination.x := trunc(V.f32[0]);
    pDestination.y := trunc(V.f32[1]);
    pDestination.z := trunc(V.f32[2]);
    pDestination.w := trunc(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreUInt4(out pDestination: TXMUINT4; constref V: TXMVECTOR);
begin
    (* ToDo
    uint32x4_t v = vcvtq_u32_f32(V);
    vst1q_u32( reinterpret_cast<uint32_t*>(pDestination), v );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreUInt4(out pDestination: TXMUINT4; constref V: TXMVECTOR); assembler;
asm
           // Clamp to >=0
           // vResult = _mm_max_ps(V,g_XMZero);
           MOVUPS  XMM0, [V]
           MAXPS   XMM0, [g_XMZero]
           // Any numbers that are too big, set to 0xFFFFFFFFU
           // vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
           MOVUPS  XMM1,[g_XMMaxUInt]
           CMPPS   XMM1, XMM0, 1
           // vValue = g_XMUnsignedFix;
           MOVUPS  XMM2,[g_XMUnsignedFix]
           // Too large for a signed integer?
           // vMask = _mm_cmpge_ps(vResult,vValue);
           MOVUPS  XMM3, XMM2
           CMPPS   XMM3, XMM0, 2
           // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
           // vValue = _mm_and_ps(vValue,vMask);
           ANDPS   XMM2, XMM3
           // Perform fixup only on numbers too large (Keeps low bit precision)
           // vResult = _mm_sub_ps(vResult,vValue);
           SUBPS   XMM0, XMM2
           // __m128i vResulti = _mm_cvttps_epi32(vResult);
           CVTTPS2DQ XMM0, XMM0
           // Convert from signed to unsigned pnly if greater than 0x80000000
           // vMask = _mm_and_ps(vMask,g_XMNegativeZero);
           ANDPS   XMM3, [g_XMNegativeZero]
           // vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
           XORPS   XMM0, XMM3
           // On those that are too large, set to 0xFFFFFFFF
           // vResult = _mm_or_ps(vResult,vOverflow);
           ORPS    XMM0, XMM1
           // _mm_storeu_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(vResult) );
           MOVDQU  [pDestination], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat3x3(out pDestination: TXMFLOAT3X3; constref M: TXMMATRIX);
begin
    pDestination.m[0, 0] := M.r[0].f32[0];
    pDestination.m[0, 1] := M.r[0].f32[1];
    pDestination.m[0, 2] := M.r[0].f32[2];

    pDestination.m[1, 0] := M.r[1].f32[0];
    pDestination.m[1, 1] := M.r[1].f32[1];
    pDestination.m[1, 2] := M.r[1].f32[2];

    pDestination.m[2, 0] := M.r[2].f32[0];
    pDestination.m[2, 1] := M.r[2].f32[1];
    pDestination.m[2, 2] := M.r[2].f32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat3x3(out pDestination: TXMFLOAT3X3; constref M: TXMMATRIX);
begin
    (* ToDo
     float32x4_t T1 = vextq_f32( M.r[0], M.r[1], 1 );
    float32x4_t T2 = vbslq_f32( g_XMMask3, M.r[0], T1 );
    vst1q_f32( &pDestination.m[0,0], T2 );

    T1 = vextq_f32( M.r[1], M.r[1], 1 );
    T2 = vcombine_f32( vget_low_f32(T1), vget_low_f32(M.r[2]) );
    vst1q_f32( &pDestination.m[1,1], T2 );

    vst1q_lane_f32( &pDestination.m[2,2], M.r[2], 2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat3x3(out pDestination: TXMFLOAT3X3; constref M: TXMMATRIX); assembler;
asm
           // vTemp1 = M.r[0];
           MOVUPS  XMM1, TXMMATRIX([M]).r0
           // vTemp2 = M.r[1];
           MOVUPS  XMM2, TXMMATRIX([M]).r1
           // vTemp3 = M.r[2];
           MOVUPS  XMM3, TXMMATRIX([M]).r2
           // vWork = _mm_shuffle_ps(vTemp1,vTemp2,_MM_SHUFFLE(0,0,2,2));
           MOVUPS  XMM4, XMM1
           SHUFPS  XMM4, XMM2, _MM_SHUFFLE_0_0_2_2
           // vTemp1 = _mm_shuffle_ps(vTemp1,vWork,_MM_SHUFFLE(2,0,1,0));
           SHUFPS  XMM1, XMM4, _MM_SHUFFLE_2_0_1_0
           // _mm_storeu_ps(&pDestination.m[0,0],vTemp1);
           MOVUPS  TXMFLOAT3X3([pDestination])._11, XMM1
           // vTemp2 = _mm_shuffle_ps(vTemp2,vTemp3,_MM_SHUFFLE(1,0,2,1));
           SHUFPS  XMM2, XMM3, _MM_SHUFFLE_1_0_2_1
           // _mm_storeu_ps(&pDestination.m[1,1],vTemp2);
           MOVUPS  TXMFLOAT3X3([pDestination])._22, XMM2
           // vTemp3 = XM_PERMUTE_PS(vTemp3,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_2_2_2_2
           // _mm_store_ss(&pDestination.m[2,2],vTemp3);
           MOVSS   TXMFLOAT3X3([pDestination])._33, XMM3
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat4x3(out pDestination: TXMFLOAT4X3; constref M: TXMMATRIX);
begin
    pDestination.m[0, 0] := M.r[0].f32[0];
    pDestination.m[0, 1] := M.r[0].f32[1];
    pDestination.m[0, 2] := M.r[0].f32[2];

    pDestination.m[1, 0] := M.r[1].f32[0];
    pDestination.m[1, 1] := M.r[1].f32[1];
    pDestination.m[1, 2] := M.r[1].f32[2];

    pDestination.m[2, 0] := M.r[2].f32[0];
    pDestination.m[2, 1] := M.r[2].f32[1];
    pDestination.m[2, 2] := M.r[2].f32[2];

    pDestination.m[3, 0] := M.r[3].f32[0];
    pDestination.m[3, 1] := M.r[3].f32[1];
    pDestination.m[3, 2] := M.r[3].f32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat4x3(out pDestination: TXMFLOAT4X3; constref M: TXMMATRIX);
begin
    (* ToDo
    float32x4_t T1 = vextq_f32( M.r[0], M.r[1], 1 );
    float32x4_t T2 = vbslq_f32( g_XMMask3, M.r[0], T1 );
    vst1q_f32( &pDestination.m[0,0], T2 );

    T1 = vextq_f32( M.r[1], M.r[1], 1 );
    T2 = vcombine_f32( vget_low_f32(T1), vget_low_f32(M.r[2]) );
    vst1q_f32( &pDestination.m[1,1], T2 );

    T1 = vdupq_lane_f32( vget_high_f32( M.r[2] ), 0 );
    T2 = vextq_f32( T1, M.r[3], 3 );
    vst1q_f32( &pDestination.m[2,2], T2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat4x3(out pDestination: TXMFLOAT4X3; constref M: TXMMATRIX); assembler;
asm
           // vTemp1 = M.r[0];
           MOVUPS  XMM1, TXMMATRIX([M]).r0
           // vTemp2 = M.r[1];
           MOVUPS  XMM2, TXMMATRIX([M]).r1
           // vTemp3 = M.r[2];
           MOVUPS  XMM3, TXMMATRIX([M]).r2
           // vTemp4 = M.r[3];
           MOVUPS  XMM4, TXMMATRIX([M]).r3
           // vTemp2x = _mm_shuffle_ps(vTemp2,vTemp3,_MM_SHUFFLE(1,0,2,1));
           MOVUPS  XMM5,XMM2
           SHUFPS  XMM5, XMM3, _MM_SHUFFLE_1_0_2_1
           // vTemp2 = _mm_shuffle_ps(vTemp2,vTemp1,_MM_SHUFFLE(2,2,0,0));
           SHUFPS  XMM2, XMM1, _MM_SHUFFLE_2_2_0_0
           // vTemp1 = _mm_shuffle_ps(vTemp1,vTemp2,_MM_SHUFFLE(0,2,1,0));
           SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0
           // vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,_MM_SHUFFLE(0,0,2,2));
           SHUFPS  XMM3, XMM4, _MM_SHUFFLE_0_0_2_2
           // vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,_MM_SHUFFLE(2,1,2,0));
           SHUFPS  XMM3, XMM4, _MM_SHUFFLE_2_1_2_0
           // _mm_storeu_ps(&pDestination.m[0,0],vTemp1);
           MOVUPS  TXMFLOAT4X3([pDestination])._11, XMM1
           // _mm_storeu_ps(&pDestination.m[1,1],vTemp2x);
           MOVUPS  TXMFLOAT4X3([pDestination])._22, XMM5
           // _mm_storeu_ps(&pDestination.m[2,2],vTemp3);
           MOVUPS  TXMFLOAT4X3([pDestination])._33, XMM3
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat4x3A(out pDestination: TXMFLOAT4X3A; constref M: TXMMATRIX);
begin
    pDestination.m[0, 0] := M.r[0].f32[0];
    pDestination.m[0, 1] := M.r[0].f32[1];
    pDestination.m[0, 2] := M.r[0].f32[2];

    pDestination.m[1, 0] := M.r[1].f32[0];
    pDestination.m[1, 1] := M.r[1].f32[1];
    pDestination.m[1, 2] := M.r[1].f32[2];

    pDestination.m[2, 0] := M.r[2].f32[0];
    pDestination.m[2, 1] := M.r[2].f32[1];
    pDestination.m[2, 2] := M.r[2].f32[2];

    pDestination.m[3, 0] := M.r[3].f32[0];
    pDestination.m[3, 1] := M.r[3].f32[1];
    pDestination.m[3, 2] := M.r[3].f32[2];

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat4x3A(out pDestination: TXMFLOAT4X3A; constref M: TXMMATRIX);
begin
    (* ToDo
    float32x4_t T1 = vextq_f32( M.r[0], M.r[1], 1 );
    float32x4_t T2 = vbslq_f32( g_XMMask3, M.r[0], T1 );
    vst1q_f32_ex( &pDestination.m[0,0], T2, 128 );

    T1 = vextq_f32( M.r[1], M.r[1], 1 );
    T2 = vcombine_f32( vget_low_f32(T1), vget_low_f32(M.r[2]) );
    vst1q_f32_ex( &pDestination.m[1,1], T2, 128 );

    T1 = vdupq_lane_f32( vget_high_f32( M.r[2] ), 0 );
    T2 = vextq_f32( T1, M.r[3], 3 );
    vst1q_f32_ex( &pDestination.m[2,2], T2, 128 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat4x3A(out pDestination: TXMFLOAT4X3A; constref M: TXMMATRIX);
asm
           // x1,y1,z1,w1
           // vTemp1 = M.r[0];
           MOVUPS  XMM1, TXMMATRIX([M]).r0
           // x2,y2,z2,w2
           // vTemp2 = M.r[1];
           MOVUPS  XMM2, TXMMATRIX([M]).r1
           // x3,y3,z3,w3
           // vTemp3 = M.r[2];
           MOVUPS  XMM3, TXMMATRIX([M]).r2
           // x4,y4,z4,w4
           // vTemp4 = M.r[3];
           MOVUPS  XMM4, TXMMATRIX([M]).r3
           // z1,z1,x2,y2
           // vTemp = _mm_shuffle_ps(vTemp1,vTemp2,_MM_SHUFFLE(1,0,2,2));
           MOVUPS  XMM5, XMM1
           SHUFPS  XMM5, XMM2, _MM_SHUFFLE_1_0_2_2
           // y2,z2,x3,y3 (Final)
           // vTemp2 = _mm_shuffle_ps(vTemp2,vTemp3,_MM_SHUFFLE(1,0,2,1));
           SHUFPS  XMM2, XMM3, _MM_SHUFFLE_1_0_2_1
           // x1,y1,z1,x2 (Final)
           // vTemp1 = _mm_shuffle_ps(vTemp1,vTemp,_MM_SHUFFLE(2,0,1,0));
           SHUFPS  XMM1, XMM5, _MM_SHUFFLE_2_0_1_0
           // z3,z3,x4,x4
           // vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,_MM_SHUFFLE(0,0,2,2));
           SHUFPS  XMM3, XMM4, _MM_SHUFFLE_0_0_2_2
           // z3,x4,y4,z4 (Final)
           // vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,_MM_SHUFFLE(2,1,2,0));
           SHUFPS  XMM3, XMM4, _MM_SHUFFLE_2_1_2_0
           // Store in 3 operations
           // _mm_store_ps(&pDestination.m[0,0],vTemp1);
           MOVAPS  TXMFLOAT4X3A([pDestination])._11, XMM1
           // _mm_store_ps(&pDestination.m[1,1],vTemp2);
           MOVAPS  TXMFLOAT4X3A([pDestination])._22, XMM2
           // _mm_store_ps(&pDestination.m[2,2],vTemp3);
           MOVAPS  TXMFLOAT4X3A([pDestination])._33, XMM3
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat4x4(out pDestination: TXMFLOAT4X4; constref M: TXMMATRIX);
begin
    pDestination.m[0, 0] := M.r[0].f32[0];
    pDestination.m[0, 1] := M.r[0].f32[1];
    pDestination.m[0, 2] := M.r[0].f32[2];
    pDestination.m[0, 3] := M.r[0].f32[3];

    pDestination.m[1, 0] := M.r[1].f32[0];
    pDestination.m[1, 1] := M.r[1].f32[1];
    pDestination.m[1, 2] := M.r[1].f32[2];
    pDestination.m[1, 3] := M.r[1].f32[3];

    pDestination.m[2, 0] := M.r[2].f32[0];
    pDestination.m[2, 1] := M.r[2].f32[1];
    pDestination.m[2, 2] := M.r[2].f32[2];
    pDestination.m[2, 3] := M.r[2].f32[3];

    pDestination.m[3, 0] := M.r[3].f32[0];
    pDestination.m[3, 1] := M.r[3].f32[1];
    pDestination.m[3, 2] := M.r[3].f32[2];
    pDestination.m[3, 3] := M.r[3].f32[3];

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat4x4(out pDestination: TXMFLOAT4X4; constref M: TXMMATRIX);
begin
    (* ToDo
     vst1q_f32( reinterpret_cast<float*>(&pDestination._11), M.r[0] );
    vst1q_f32( reinterpret_cast<float*>(&pDestination._21), M.r[1] );
    vst1q_f32( reinterpret_cast<float*>(&pDestination._31), M.r[2] );
    vst1q_f32( reinterpret_cast<float*>(&pDestination._41), M.r[3] );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat4x4(out pDestination: TXMFLOAT4X4; constref M: TXMMATRIX);
asm
           // _mm_storeu_ps( &pDestination._11, M.r[0] );
           MOVUPS  XMM0, TXMMATRIX([M]).r0
           MOVUPS  TXMFLOAT4X4([pDestination])._11, XMM0
           // _mm_storeu_ps( &pDestination._21, M.r[1] );
           MOVUPS  XMM0, TXMMATRIX([M]).r1
           MOVUPS  TXMFLOAT4X4([pDestination])._21, XMM0
           // _mm_storeu_ps( &pDestination._31, M.r[2] );
           MOVUPS  XMM0, TXMMATRIX([M]).r2
           MOVUPS  TXMFLOAT4X4([pDestination])._31, XMM0
           // _mm_storeu_ps( &pDestination._41, M.r[3] );
           MOVUPS  XMM0, TXMMATRIX([M]).r3
           MOVUPS  TXMFLOAT4X4([pDestination])._41, XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMStoreFloat4x4A(out pDestination: TXMFLOAT4X4A; constref M: TXMMATRIX);
begin
    pDestination.m[0, 0] := M.r[0].f32[0];
    pDestination.m[0, 1] := M.r[0].f32[1];
    pDestination.m[0, 2] := M.r[0].f32[2];
    pDestination.m[0, 3] := M.r[0].f32[3];

    pDestination.m[1, 0] := M.r[1].f32[0];
    pDestination.m[1, 1] := M.r[1].f32[1];
    pDestination.m[1, 2] := M.r[1].f32[2];
    pDestination.m[1, 3] := M.r[1].f32[3];

    pDestination.m[2, 0] := M.r[2].f32[0];
    pDestination.m[2, 1] := M.r[2].f32[1];
    pDestination.m[2, 2] := M.r[2].f32[2];
    pDestination.m[2, 3] := M.r[2].f32[3];

    pDestination.m[3, 0] := M.r[3].f32[0];
    pDestination.m[3, 1] := M.r[3].f32[1];
    pDestination.m[3, 2] := M.r[3].f32[2];
    pDestination.m[3, 3] := M.r[3].f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMStoreFloat4x4A(out pDestination: TXMFLOAT4X4A; constref M: TXMMATRIX);
begin
    (* ToDo
    vst1q_f32_ex( reinterpret_cast<float*>(&pDestination._11), M.r[0], 128 );
    vst1q_f32_ex( reinterpret_cast<float*>(&pDestination._21), M.r[1], 128 );
    vst1q_f32_ex( reinterpret_cast<float*>(&pDestination._31), M.r[2], 128 );
    vst1q_f32_ex( reinterpret_cast<float*>(&pDestination._41), M.r[3], 128 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMStoreFloat4x4A(out pDestination: TXMFLOAT4X4A; constref M: TXMMATRIX);
asm
           // _mm_store_ps( &pDestination._11, M.r[0] );
           MOVUPS  XMM0, TXMMATRIX([M]).r0
           MOVAPS  TXMFLOAT4X4A([pDestination])._11, XMM0
           // _mm_store_ps( &pDestination._21, M.r[1] );
           MOVUPS  XMM0, TXMMATRIX([M]).r1
           MOVAPS  TXMFLOAT4X4A([pDestination])._21, XMM0
           // _mm_store_ps( &pDestination._31, M.r[2] );
           MOVUPS  XMM0, TXMMATRIX([M]).r2
           MOVAPS  TXMFLOAT4X4A([pDestination])._31, XMM0
           // _mm_store_ps( &pDestination._41, M.r[3] );
           MOVUPS  XMM0, TXMMATRIX([M]).r3
           MOVAPS  TXMFLOAT4X4A([pDestination])._41, XMM0
end;
{$ENDIF}


{****************************************************************************
 *
 * General Vector
 *
 ****************************************************************************}

//------------------------------------------------------------------------------
// Assignment operations
//------------------------------------------------------------------------------


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Return a vector with all elements equaling zero
function XMVectorZero: TXMVECTOR;
begin
    Result.f32[0] := 0;
    Result.f32[1] := 0;
    Result.f32[2] := 0;
    Result.f32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorZero: TXMVECTOR;
begin
    (* ToDo
     return vdupq_n_f32(0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorZero: TXMVECTOR; assembler;
asm
           XORPS   XMM0, XMM0
           MOVUPS   [Result], XMM0
end;
{$ENDIF}


//------------------------------------------------------------------------------
// Initialize a vector with four floating point values
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSet(constref x, y, z, w: single): TXMVECTOR;
begin
    Result.f32[0] := x;
    Result.f32[1] := y;
    Result.f32[2] := z;
    Result.f32[3] := w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSet(constref x, y, z, w: single): TXMVECTOR;
begin
    (*
float32x2_t V0 = vcreate_f32(((uint64_t) * (const uint32_t * )&x) | ((uint64_t)( *(const uint32_t * )&y) << 32));
    float32x2_t V1 = vcreate_f32(((uint64_t) * (const uint32_t * )&z) | ((uint64_t)( *(const uint32_t * )&w) << 32));
    return vcombine_f32(V0, V1);
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSet(constref x, y, z, w: single): TXMVECTOR; assembler; nostackframe;
asm
           // ToDo: return _mm_set_ps( w, z, y, x );
           PUSH    EAX
           MOV     EAX, [x]
           MOV     TXMVECTOR(result).f32[0], EAX
           MOV     EAX, [y]
           MOV     TXMVECTOR(result).f32[1], EAX
           MOV     EAX, [z]
           MOV     TXMVECTOR(result).f32[2], EAX
           MOV     EAX, [w]
           MOV     TXMVECTOR(result).f32[3], EAX
           POP     EAX
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Initialize a vector with four integer values
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetInt(x, y, z, w: UINT32): TXMVECTOR;
begin
    Result.u32[0] := x;
    Result.u32[1] := y;
    Result.u32[2] := z;
    Result.u32[3] := w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetInt(x, y, z, w: UINT32): TXMVECTOR;
begin
    (*
     uint32x2_t V0 = vcreate_u32(((uint64_t)x) | ((uint64_t)y << 32));
    uint32x2_t V1 = vcreate_u32(((uint64_t)z) | ((uint64_t)w << 32));
    return vcombine_u32(V0, V1);
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetInt(x, y, z, w: UINT32): TXMVECTOR;
asm
           // __m128i V = _mm_set_epi32( w, z, y, x );
           MOVAPS        XMM0,[x]
           MOVAPS        XMM1,[y]
           MOVAPS        XMM2,[z]
           MOVAPS        XMM3,[w]
           PUNPCKLDQ   XMM3,XMM1
           PUNPCKLDQ   XMM2,XMM0
           PUNPCKLDQ   XMM3,XMM2
           // return _mm_castsi128_ps(V);
           MOVUPS  [result],XMM3
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Initialize a vector with a replicated floating point value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReplicate(constref Value: single): TXMVECTOR;
begin
    Result.f32[0] := Value;
    Result.f32[1] := Value;
    Result.f32[2] := Value;
    Result.f32[3] := Value;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReplicate(constref Value: single): TXMVECTOR;
begin
    (* ToDo
     return vld1q_dup_f32( pValue );
*)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorReplicate(constref Value: single): TXMVECTOR; assembler;
asm
           VBROADCASTSS XMM0, [Value]
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReplicate(constref Value: single): TXMVECTOR; assembler;
asm
           // return _mm_set_ps1( Value );
           MOVSS   XMM0, [Value]
           SHUFPS  XMM0, XMM0, 0
           MOVUPS   [Result], XMM0
end;
{$ENDIF}




//------------------------------------------------------------------------------
// Initialize a vector with a replicated floating point value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReplicatePtr(pValue: PSingle): TXMVECTOR;
begin
    Result.f32[0] := pValue^;
    Result.f32[1] := pValue^;
    Result.f32[2] := pValue^;
    Result.f32[3] := pValue^;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReplicatePtr(pValue: PSingle): TXMVECTOR;
begin
    (*
      return vld1q_dup_f32( pValue );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorReplicatePtr(pValue: PSingle): TXMVECTOR; assembler;
asm
           // return _mm_broadcast_ss( pValue );
           VBROADCASTSS XMM0, [pValue]
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReplicatePtr(pValue: PSingle): TXMVECTOR; assembler;
asm
           // return _mm_load_ps1( pValue );
           MOVSS   XMM0, [pValue]
           SHUFPS  XMM0, XMM0, 0
           MOVUPS   [Result], XMM0
end;
{$ENDIF}




// Initialize a vector with a replicated integer value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReplicateInt(constref Value: UINT32): TXMVECTOR;
begin
    Result.u32[0] := Value;
    Result.u32[1] := Value;
    Result.u32[2] := Value;
    Result.u32[3] := Value;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReplicateInt(constref Value: UINT32): TXMVECTOR;
begin
    (*
     return vdupq_n_u32( Value );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorReplicateInt(constref Value: UINT32): TXMVECTOR; assembler;
asm
           VPBROADCASTD XMM0, [Value]
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReplicateInt(constref Value: UINT32): TXMVECTOR; assembler;
asm
           //      __m128i vTemp = _mm_set1_epi32( Value );
           MOVUPS  XMM0,[value]
           // return _mm_castsi128_ps(vTemp);
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Initialize a vector with a replicated integer value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReplicateIntPtr(pValue: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := pValue^;
    Result.u32[1] := pValue^;
    Result.u32[2] := pValue^;
    Result.u32[3] := pValue^;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReplicateIntPtr(pValue: PUINT32): TXMVECTOR;
begin
    (*
     return vld1q_dup_u32(pValue);
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorReplicateIntPtr(pValue: PUINT32): TXMVECTOR; assembler;
asm
           VPBROADCASTD XMM0, [pValue]
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReplicateIntPtr(pValue: PUINT32): TXMVECTOR; assembler;
asm
           // return _mm_load_ps1(reinterpret_cast<const float *>(pValue));
           MOVSS   XMM0, [pValue]
           SHUFPS  XMM0, XMM0, 0
           MOVUPS  [result],XMM0
end;
{$ENDIF}


// ToDo Checken XMVectorReplicate - XMVectorReplicateIntPtr mit orginal source

// Initialize a vector with all bits set (true mask)
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorTrueInt: TXMVECTOR;
begin
    Result.u32[0] := $FFFFFFFF;
    Result.u32[1] := $FFFFFFFF;
    Result.u32[2] := $FFFFFFFF;
    Result.u32[3] := $FFFFFFFF;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorTrueInt: TXMVECTOR;
begin
    (*
    return vdupq_n_s32(-1);
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorTrueInt: TXMVECTOR; assembler;
const
    c: uint32 = $FFFFFFFF;
asm
           VPBROADCASTD XMM0, [c]
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorTrueInt: TXMVECTOR; assembler; nostackframe; // ??????
const
    v: array[0..3] of UINT32 = ($FFFFFFFF, $FFFFFFFF, $FFFFFFFF, $FFFFFFFF);
asm
           //__m128i V = _mm_set1_epi32(-1);
           MOVAPS  XMM0, [v]
           // return _mm_castsi128_ps(V);
           MOVAPS  [Result],XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Initialize a vector with all bits clear (false mask)
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorFalseInt: TXMVECTOR;
begin
    Result.u32[0] := $0;
    Result.u32[1] := $0;
    Result.u32[2] := $0;
    Result.u32[3] := $0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorFalseInt: TXMVECTOR;
begin
    (* ToDo
    return vdupq_n_u32(0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorFalseInt: TXMVECTOR; assembler;
asm
           XORPS   XMM0, XMM0
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Returns a vector, all of whose components are equal to the x component of V.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatX(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := v.f32[0];
    Result.f32[1] := v.f32[0];
    Result.f32[2] := v.f32[0];
    Result.f32[3] := v.f32[0];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatX(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vdupq_lane_f32( vget_low_f32( V ), 0 );
*)
end;

{$ELSEIF DEFINED(_XM_AVX2_INTRINSICS_)}
function XMVectorSplatX(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           VBROADCASTSS XMM0, [v]
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatX(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[V]
           MOVAPS  XMM1, XMM0
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_0_0_0_0
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

// Returns a vector, all of whose components are equal to the y component of V.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatY(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := v.f32[1];
    Result.f32[1] := v.f32[1];
    Result.f32[2] := v.f32[1];
    Result.f32[3] := v.f32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatY(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
  return vdupq_lane_f32( vget_low_f32( V ), 1 );
*)
end;

{$ELSEIF DEFINED(_XM_AVX2_INTRINSICS_)}
function XMVectorSplatY(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           VMOVUPS XMM0,[v]
           VPERMILPS XMM0, XMM0, $55
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatY(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[V]
           MOVAPS  XMM1, XMM0
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_1_1_1_1
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Replicate the z component of the vector
// Returns a vector, all of whose components are equal to the z component of V.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatZ(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := v.f32[2];
    Result.f32[1] := v.f32[2];
    Result.f32[2] := v.f32[2];
    Result.f32[3] := v.f32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatZ(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   return vdupq_lane_f32( vget_high_f32( V ), 0 );
*)
end;

{$ELSEIF DEFINED(_XM_AVX2_INTRINSICS_)}
function XMVectorSplatZ(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           VMOVUPS XMM0,[v]
           VPERMILPS XMM0, XMM0, $AA
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatZ(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[V]
           MOVAPS  XMM1, XMM0
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_2_2_2_2
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Returns a vector, all of whose components are equal to the w component of V.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatW(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := v.f32[3];
    Result.f32[1] := v.f32[3];
    Result.f32[2] := v.f32[3];
    Result.f32[3] := v.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatW(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vdupq_lane_f32( vget_high_f32( V ), 1 );
*)
end;

{$ELSEIF DEFINED(_XM_AVX2_INTRINSICS_)}
function XMVectorSplatW(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           VMOVUPS XMM0,[v]
           VPERMILPS XMM0, XMM0, $FF
           VMOVUPS   [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatW(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[V]
           MOVAPS  XMM1, XMM0
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_3_3_3
           MOVUPS   [Result], XMM0
end;
{$ENDIF}


// Return a vector of 1.0f,1.0f,1.0f,1.0f
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatOne: TXMVECTOR; inline;
begin
    Result.f32[0] := 1.0;
    Result.f32[1] := 1.0;
    Result.f32[2] := 1.0;
    Result.f32[3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatOne: TXMVECTOR; inline;
begin
    (* ToDo
     return vdupq_n_f32(1.0f);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatOne: TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[g_XMOne]
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Return a vector of INF,INF,INF,INF
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSplatInfinity: TXMVECTOR;
begin
    Result.u32[0] := $7F800000;
    Result.u32[1] := $7F800000;
    Result.u32[2] := $7F800000;
    Result.u32[3] := $7F800000;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatInfinity: TXMVECTOR;
begin
    (* ToDo
    return vdupq_n_u32(0x7F800000);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatInfinity: TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[g_XMInfinity]
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Return a vector of Q_NAN,Q_NAN,Q_NAN,Q_NAN
function XMVectorSplatQNaN: TXMVECTOR;
begin
    Result.u32[0] := $7FC00000;
    Result.u32[1] := $7FC00000;
    Result.u32[2] := $7FC00000;
    Result.u32[3] := $7FC00000;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatQNaN: TXMVECTOR;
begin
    (* ToDo
    return vdupq_n_u32(0x7FC00000);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatQNaN: TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[g_XMQNaN]
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
//------------------------------------------------------------------------------
// Return a vector of 1.192092896e-7f,1.192092896e-7f,1.192092896e-7f,1.192092896e-7f
function XMVectorSplatEpsilon: TXMVECTOR;
begin
    Result.u32[0] := $34000000;
    Result.u32[1] := $34000000;
    Result.u32[2] := $34000000;
    Result.u32[3] := $34000000;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatEpsilon: TXMVECTOR;
begin
    (* ToDo
    return vdupq_n_u32(0x34000000);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatEpsilon: TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[g_XMEpsilon]
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Return a vector of -0.0f (0x80000000),-0.0f,-0.0f,-0.0f
{$IF DEFINED(_XM_NO_INTRINSICS_)}
//------------------------------------------------------------------------------
// Return a vector of -0.0  (0x80000000),-0.0 ,-0.0 ,-0.0
function XMVectorSplatSignMask: TXMVECTOR;
begin
    Result.u32[0] := $80000000;
    Result.u32[1] := $80000000;
    Result.u32[2] := $80000000;
    Result.u32[3] := $80000000;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSplatSignMask: TXMVECTOR;
begin
    (* ToDo
    return vdupq_n_u32(0x80000000U);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSplatSignMask: TXMVECTOR; assembler;
const
    v: array [0..3] of UINT32 = ($80000000, $80000000, $80000000, $80000000);
asm
           MOVUPS  XMM0,[v]
           MOVUPS   [Result], XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Return a floating point value via an index. This is not a recommended
// function to use due to performance loss.
function XMVectorGetByIndex(V: TXMVECTOR; i: size_t): single;
begin
    assert(i < 4);
    Result := V.f32[i];
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
//------------------------------------------------------------------------------
// Return the X component in an FPU register.
function XMVectorGetX(constref V: TXMVECTOR): single;
begin
    Result := V.f32[0];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetX(constref V: TXMVECTOR): single;
begin
    (* ToDo
      return vgetq_lane_f32(V, 0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetX(constref V: TXMVECTOR): single; assembler;
asm
           MOVUPS  XMM0, [v]
           MOVSS   [result], XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Return the Y component in an FPU register.
function XMVectorGetY(constref V: TXMVECTOR): single;
begin
    Result := V.f32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetY(constref V: TXMVECTOR): single;
begin
    (*  return vgetq_lane_f32(V, 1); *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetY(constref V: TXMVECTOR): single; assembler;
asm
           MOVUPS  XMM0, [v]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_1_1_1
           MOVSS   [result], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Return the Z component in an FPU register.
function XMVectorGetZ(constref V: TXMVECTOR): single;
begin
    Result := V.f32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetZ(constref V: TXMVECTOR): single;
begin
    (*  return vgetq_lane_f32(V, 2); *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetZ(constref V: TXMVECTOR): single; assembler;
asm
           MOVUPS  XMM0, [v]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           MOVSS   [result], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Return the W component in an FPU register.
function XMVectorGetW(constref V: TXMVECTOR): single;
begin
    Result := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetW(constref V: TXMVECTOR): single;
begin
    (*  return vgetq_lane_f32(V, 3); *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetW(constref V: TXMVECTOR): single; assembler;
asm
           MOVUPS  XMM0, [v]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_3_3_3
           MOVSS   [result], XMM0
end;
{$ENDIF}



//------------------------------------------------------------------------------
// Store a component indexed by i into a 32 bit  single  location in memory.
procedure XMVectorGetByIndexPtr(out f: single; V: TXMVECTOR; i: size_t);
begin
    assert(i < 4);
    f := V.f32[i];
end;

// Store the X component into a 32 bit  single  location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetXPtr(out x: single; constref V: TXMVECTOR);
begin
    x := V.f32[0];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetXPtr(out x: single; constref V: TXMVECTOR);
begin
    (*  return vgetq_lane_f32(V, 0); *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetXPtr(out x: single; constref V: TXMVECTOR); assembler;
asm
           // ToDo: _mm_store_ss(x,V);
           MOVUPS  XMM0, [v]
           MOVSS   [x], XMM0
end;
{$ENDIF}


// Store the Y component into a 32 bit  single  location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetYPtr(out y: single; constref V: TXMVECTOR);
begin
    y := V.f32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetYPtr(out y: single; constref V: TXMVECTOR);
begin
    (*  return vgetq_lane_f32(V, 1); *)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
procedure XMVectorGetYPtr(out y: single; constref V: TXMVECTOR); assembler;
asm
           //  *((int*)y) = _mm_extract_ps( V, 1 );
           MOVUPS  XMM0, [v]
           EXTRACTPS [y], XMM0, 32
end;
{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetYPtr(out y: single; constref V: TXMVECTOR); assembler;
asm
           MOVUPS  XMM0, [v]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_1_1_1
           MOVSS   [y], XMM0
end;
{$ENDIF}

// Store the Z component into a 32 bit  single  location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetZPtr(out z: single; constref V: TXMVECTOR);
begin
    z := V.f32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetZPtr(out z: single; constref V: TXMVECTOR);
begin
    (* ToDo
     vst1q_lane_f32(z,V,2);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
procedure XMVectorGetZPtr(out z: single; constref V: TXMVECTOR); assembler;
asm

           // *((int*)z) = _mm_extract_ps( V, 2 );
           MOVUPS  XMM0 ,[V]
           EXTRACTPS EAX, XMM0, 2
           MOV     [z], EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetZPtr(out z: single; constref V: TXMVECTOR); assembler;
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM0, [v]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // _mm_store_ss(z,vResult);
           MOVSS   [z], XMM0
end;
{$ENDIF}


// Store the W component into a 32 bit  single  location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetWPtr(out w: single; constref V: TXMVECTOR);
begin
    w := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetWPtr(out w: single; constref V: TXMVECTOR);
begin
    (* ToDo
    vst1q_lane_f32(w,V,3);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
procedure XMVectorGetWPtr(out w: single; constref V: TXMVECTOR); assembler;
asm
           //((int*)w) = _mm_extract_ps( V, 3 );
           MOVUPS  XMM0 ,[V]
           EXTRACTPS EAX, XMM0, 3
           MOV     [w], EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetWPtr(out w: single; constref V: TXMVECTOR); assembler;
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_3_3_3
           // _mm_store_ss(w,vResult);
           MOVSS   [w], XMM0
end;
{$ENDIF}



// Return an integer value via an index. This is not a recommended
// function to use due to performance loss.
function XMVectorGetIntByIndex(constref V: TXMVECTOR; constref i: size_t): UINT32;
begin
    assert(i < 4);
    Result := V.u32[i];
end;

// Return the X component in an integer register.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGetIntX(constref V: TXMVECTOR): UINT32;
begin
    Result := V.u32[0];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetIntX(constref V: TXMVECTOR): UINT32;
begin
    (* ToDo
     return vgetq_lane_u32(V, 0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetIntX(constref V: TXMVECTOR): UINT32; assembler;
asm
           // return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_castps_si128(V)));
           MOVUPS  XMM0, [V]
           MOVD    [result], XMM0
end;
{$ENDIF}



// Return the Y component in an integer register.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGetIntY(constref V: TXMVECTOR): UINT32;
begin
    Result := V.u32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetIntY(constref V: TXMVECTOR): UINT32;
begin
    (* ToDo
    return vgetq_lane_u32(V, 1);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorGetIntY(constref V: TXMVECTOR): UINT32; assembler;
asm
           // V1 = _mm_castps_si128( V );
           MOVUPS  XMM0, [V]
           // return static_cast<uint32_t>( _mm_extract_epi32( V1, 1 ) );
           PEXTRD  EAX, XMM0, 1
           MOV     [result],EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetIntY(constref V: TXMVECTOR): UINT32; assembler;
asm
           // __m128i vResulti = _mm_shuffle_epi32(_mm_castps_si128(V),_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM0,[V]
           PSHUFD  XMM0, XMM0, _MM_SHUFFLE_1_1_1_1
           // return static_cast<uint32_t>(_mm_cvtsi128_si32(vResulti));
           MOVD    [result], XMM0
end;
{$ENDIF}



// Return the Z component in an integer register.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGetIntZ(constref V: TXMVECTOR): UINT32;
begin
    Result := V.u32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetIntZ(constref V: TXMVECTOR): UINT32;
begin
    (* ToDo
    return vgetq_lane_u32(V, 2);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorGetIntZ(constref V: TXMVECTOR): UINT32; assembler;
asm
           // V1 = _mm_castps_si128( V );
           MOVUPS  XMM0,[V]
           // return static_cast<uint32_t>( _mm_extract_epi32( V1, 2 ) );
           PEXTRD  EAX, XMM0, 2
           MOV     [result],EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetIntZ(constref V: TXMVECTOR): UINT32; assembler;
asm
           // __m128i vResulti = _mm_shuffle_epi32(_mm_castps_si128(V),_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM0,[V]
           PSHUFD  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // return static_cast<uint32_t>(_mm_cvtsi128_si32(vResulti));
           MOVD    [result], XMM0
end;
{$ENDIF}



// Return the W component in an integer register.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGetIntW(constref V: TXMVECTOR): UINT32;
begin
    Result := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGetIntW(constref V: TXMVECTOR): UINT32;
begin
    (* ToDo
    return vgetq_lane_u32(V, 3);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorGetIntW(constref V: TXMVECTOR): UINT32; assembler;
asm
           // V1 = _mm_castps_si128( V );
           MOVUPS  XMM0,[V]
           // static_cast<uint32_t>( _mm_extract_epi32( V1, 3 ) );
           PEXTRD  EAX, XMM0, 3
           MOV     [result],EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGetIntW(constref V: TXMVECTOR): UINT32; assembler;
asm
           // __m128i vResulti = _mm_shuffle_epi32(_mm_castps_si128(V),_MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM0,[V]
           PSHUFD  XMM0, XMM0, _MM_SHUFFLE_3_3_3_3
           // return static_cast<uint32_t>(_mm_cvtsi128_si32(vResulti));
           MOVD    [result], XMM0
end;
{$ENDIF}



// Store a component indexed by i into a 32 bit integer location in memory.
procedure XMVectorGetIntByIndexPtr(out x: UINT32; constref V: TXMVECTOR; constref i: size_t);
begin
    assert(i < 4);
    x := V.u32[i];
end;


// Store the X component into a 32 bit integer location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetIntXPtr(out x: UINT32; constref V: TXMVECTOR);
begin
    x := V.u32[0];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetIntXPtr(out x: UINT32; constref V: TXMVECTOR);
begin
    (* ToDo
     vst1q_lane_u32(x,*reinterpret_cast<const uint32x4_t*>(&V),0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetIntXPtr(out x: UINT32; constref V: TXMVECTOR); assembler;
asm
           //_mm_store_ss(reinterpret_cast<float *>(x),V);
           MOVUPS  XMM0,[V]
           MOVSS   [x], XMM0
end;
{$ENDIF}



// Store the Y component into a 32 bit integer location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetIntYPtr(out y: UINT32; constref V: TXMVECTOR);
begin
    y := V.u32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetIntYPtr(out y: UINT32; constref V: TXMVECTOR);
begin
    (* ToDo
     vst1q_lane_u32(y,*reinterpret_cast<const uint32x4_t*>(&V),1);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
procedure XMVectorGetIntYPtr(out y: UINT32; constref V: TXMVECTOR); assembler;
asm
           // V1 = _mm_castps_si128( V );
           MOVUPS  XMM0,[V]
           // *y = static_cast<uint32_t>( _mm_extract_epi32( V1, 1 ) );
           PEXTRD  EAX, XMM0, 1
           MOV     [y],EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetIntYPtr(out y: UINT32; constref V: TXMVECTOR); assembler;
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_1_1_1
           //    _mm_store_ss(reinterpret_cast<float *>(y),vResult);
           MOVSS   [y], XMM0
end;
{$ENDIF}



// Store the Z component into a 32 bit integer location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetIntZPtr(out z: UINT32; constref V: TXMVECTOR);
begin
    z := V.u32[2];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetIntZPtr(out z: UINT32; constref V: TXMVECTOR);
begin
    (* ToDo
     vst1q_lane_u32(z,*reinterpret_cast<const uint32x4_t*>(&V),2);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
procedure XMVectorGetIntZPtr(out z: UINT32; constref V: TXMVECTOR); assembler;
asm
           // V1 = _mm_castps_si128( V );
           MOVUPS  XMM0,[V]
           // *z = static_cast<uint32_t>( _mm_extract_epi32( V1, 2 ) );
           PEXTRD  EAX, XMM0, 2
           MOV     [z],EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetIntZPtr(out z: UINT32; constref V: TXMVECTOR); assembler;
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           //_mm_store_ss(reinterpret_cast<float *>(z),vResult);
           MOVSS   [z], XMM0
end;
{$ENDIF}



// Store the W component into a 32 bit integer location in memory.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorGetIntWPtr(out w: UINT32; constref V: TXMVECTOR);
begin
    w := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorGetIntWPtr(out w: UINT32; constref V: TXMVECTOR);
begin
    (* ToDo
   vst1q_lane_u32(w,*reinterpret_cast<const uint32x4_t*>(&V),3);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
procedure XMVectorGetIntWPtr(out w: UINT32; constref V: TXMVECTOR); assembler;
asm
           // V1 = _mm_castps_si128( V );
           MOVUPS  XMM0 ,[V]
           // *w = static_cast<uint32_t>( _mm_extract_epi32( V1, 3 ) );
           PEXTRD  EAX, XMM0, 3
           MOV     [w],EAX
end;
{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorGetIntWPtr(out w: UINT32; constref V: TXMVECTOR); assembler;
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_3_3_3
           // _mm_store_ss(reinterpret_cast<float *>(w),vResult);
           MOVSS   [w], XMM0
end;
{$ENDIF}




//------------------------------------------------------------------------------
// Set a single indexed floating point component
function XMVectorSetByIndex(constref V: TXMVECTOR; constref f: single; constref i: size_t): TXMVECTOR;
begin
    assert(i < 4);
    Result := V;
    Result.f32[i] := f;
end;


//------------------------------------------------------------------------------
// Sets the X component of a vector to a passed floating point value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetX(constref V: TXMVECTOR; constref x: single): TXMVECTOR;
begin
    Result.f32[0] := x;
    Result.f32[1] := V.f32[1];
    Result.f32[2] := V.f32[2];
    Result.f32[3] := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetX(constref V: TXMVECTOR; constref x: single): TXMVECTOR;
begin
    (* ToDo
     return vsetq_lane_f32(x,V,0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetX(constref V: TXMVECTOR; constref x: single): TXMVECTOR; assembler;
asm
           //  vResult = _mm_set_ss(x);
           MOVSS   XMM1, [x]
           // vResult = _mm_move_ss(V,vResult);
           MOVUPS  XMM0,[V]
           MOVSS   XMM0,XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the Y component of a vector to a passed floating point value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetY(constref V: TXMVECTOR; constref y: single): TXMVECTOR;
begin
    Result.f32[0] := V.f32[0];
    Result.f32[1] := y;
    Result.f32[2] := V.f32[2];
    Result.f32[3] := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetY(constref V: TXMVECTOR; constref y: single): TXMVECTOR;
begin
    (* ToDo
    return vsetq_lane_f32(y,V,1);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorSetY(constref V: TXMVECTOR; constref y: single): TXMVECTOR; assembler;
asm
           // vResult = _mm_set_ss(y);
           MOVSS   XMM1,[y]
           // vResult = _mm_insert_ps( V, vResult, 0x10 );
           MOVUPS  XMM0,[V]
           INSERTPS XMM0, XMM1, $10
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetY(constref V: TXMVECTOR; constref y: single): TXMVECTOR; assembler;
asm
           // Swap y and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,2,0,1));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // Convert input to vector
           // vTemp = _mm_set_ss(y);
           MOVSS   XMM1,[y]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0, XMM1
           // Swap y and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,2,0,1));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the Z component of a vector to a passed floating point value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetZ(constref V: TXMVECTOR; constref z: single): TXMVECTOR;
begin
    Result.f32[0] := V.f32[0];
    Result.f32[1] := V.f32[1];
    Result.f32[2] := z;
    Result.f32[3] := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetZ(constref V: TXMVECTOR; constref z: single): TXMVECTOR;
begin
    (* ToDo
     return vsetq_lane_f32(z,V,2);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorSetZ(constref V: TXMVECTOR; constref z: single): TXMVECTOR; assembler;
asm
           // vResult = _mm_set_ss(z);
           MOVSS   XMM1,[z]
           //vResult = _mm_insert_ps( V, vResult, $20 );
           MOVUPS  XMM0,[V]
           INSERTPS XMM0, XMM1, $20
           //return vResult;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetZ(constref V: TXMVECTOR; constref z: single): TXMVECTOR; assembler;
asm
           // Swap z and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,0,1,2));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // Convert input to vector
           // vTemp = _mm_set_ss(z);
           MOVSS   XMM1,[z]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0,XMM1
           // Swap z and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,0,1,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the W component of a vector to a passed floating point value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetW(constref V: TXMVECTOR; constref w: single): TXMVECTOR;
begin
    Result.f32[0] := V.f32[0];
    Result.f32[1] := V.f32[1];
    Result.f32[2] := V.f32[2];
    Result.f32[3] := w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetW(constref V: TXMVECTOR; constref w: single): TXMVECTOR;
begin
    (* ToDo
     return vsetq_lane_f32(w,V,3);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorSetW(constref V: TXMVECTOR; constref w: single): TXMVECTOR; assembler;
asm
           // vResult = _mm_set_ss(w);
           MOVSS   XMM1,[w]
           // vResult = _mm_insert_ps( V, vResult, $30 );
           MOVUPS  XMM0,[V]
           INSERTPS XMM0, XMM1, $30
           // return vResult;
           MOVUPS  [result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetW(constref V: TXMVECTOR; constref w: single): TXMVECTOR; assembler;
asm
           // Swap w and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,2,1,3));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // Convert input to vector
           // vTemp = _mm_set_ss(w);
           MOVSS   XMM1,[w]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0, XMM1
           // Swap w and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(0,2,1,3));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}




//------------------------------------------------------------------------------

// Sets a component of a vector to a floating point value passed by pointer
function XMVectorSetByIndexPtr(constref V: TXMVECTOR; constref f: Psingle; constref i: size_t): TXMVECTOR;
begin
    assert(i < 4);
    Result := V;
    Result.f32[i] := f^;
end;

// Sets the X component of a vector to a floating point value passed by p
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetXPtr(constref V: TXMVECTOR; constref x: Psingle): TXMVECTOR;
begin
    Result.f32[0] := x^;
    Result.f32[1] := V.f32[1];
    Result.f32[2] := V.f32[2];
    Result.f32[3] := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetXPtr(constref V: TXMVECTOR; constref x: Psingle): TXMVECTOR;
begin
    (* ToDo
    return vld1q_lane_f32(x,V,0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetXPtr(constref V: TXMVECTOR; constref x: Psingle): TXMVECTOR; assembler;
asm
           // XMVECTOR vResult = _mm_load_ss(x);
           MOVSS   XMM1,[x]
           // vResult = _mm_move_ss(V,vResult);
           MOVUPS  XMM0, [V]
           MOVSS   XMM0,XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}
//------------------------------------------------------------------------------




// Sets the Y component of a vector to a floating point value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetYPtr(constref V: TXMVECTOR; constref y: Psingle): TXMVECTOR;
begin
    Result.f32[0] := V.f32[0];
    Result.f32[1] := y^;
    Result.f32[2] := V.f32[2];
    Result.f32[3] := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetYPtr(constref V: TXMVECTOR; constref y: Psingle): TXMVECTOR;
begin
    (* ToDo
     return vld1q_lane_f32(y,V,1);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetYPtr(constref V: TXMVECTOR; constref y: Psingle): TXMVECTOR; assembler;
asm
           // Swap y and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,2,0,1));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // Convert input to vector
           // vTemp = _mm_load_ss(y);
           MOVSS   XMM1, [y]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0, XMM1
           // Swap y and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,2,0,1));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the Z component of a vector to a floating point value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetZPtr(constref V: TXMVECTOR; constref z: Psingle): TXMVECTOR;
begin
    Result.f32[0] := V.f32[0];
    Result.f32[1] := V.f32[1];
    Result.f32[2] := z^;
    Result.f32[3] := V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetZPtr(constref V: TXMVECTOR; constref z: Psingle): TXMVECTOR;
begin
    (* ToDo
     return vld1q_lane_f32(z,V,2);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetZPtr(constref V: TXMVECTOR; constref z: Psingle): TXMVECTOR; assembler;
asm
           // Swap z and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,0,1,2));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // Convert input to vector
           // vTemp = _mm_load_ss(z);
           MOVSS   XMM1, [z]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0, XMM1
           // Swap z and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,0,1,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the W component of a vector to a floating point value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetWPtr(constref V: TXMVECTOR; constref w: Psingle): TXMVECTOR;
begin
    Result.f32[0] := V.f32[0];
    Result.f32[1] := V.f32[1];
    Result.f32[2] := V.f32[2];
    Result.f32[3] := w^;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetWPtr(constref V: TXMVECTOR; constref w: Psingle): TXMVECTOR;
begin
    (* ToDo
    return vld1q_lane_f32(w,V,3);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetWPtr(constref V: TXMVECTOR; constref w: Psingle): TXMVECTOR; assembler;
asm
           // Swap w and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,2,1,3));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // Convert input to vector
           // vTemp = _mm_load_ss(w);
           MOVSS   XMM1,[w]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0, XMM1
           // Swap w and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(0,2,1,3));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}




//------------------------------------------------------------------------------

// Sets a component of a vector to an integer passed by value
function XMVectorSetIntByIndex(constref V: TXMVECTOR; constref x: UINT32; constref i: size_t): TXMVECTOR;
begin
    assert(i < 4);
    Result := V;
    Result.u32[i] := x;
end;

// Sets the X component of a vector to an integer passed by value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntX(constref V: TXMVECTOR; constref x: UINT32): TXMVECTOR;
begin
    Result.u32[0] := x;
    Result.u32[1] := V.u32[1];
    Result.u32[2] := V.u32[2];
    Result.u32[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntX(constref V: TXMVECTOR; constref x: UINT32): TXMVECTOR;
begin
    (* ToDo
    return vsetq_lane_u32(x,V,0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntX(constref V: TXMVECTOR; constref x: UINT32): TXMVECTOR; assembler;
asm
           // __m128i vTemp = _mm_cvtsi32_si128(x);
           MOVD    XMM1, [x]
           // vResult = _mm_move_ss(V,_mm_castsi128_ps(vTemp));
           MOVUPS  XMM0,[V]
           MOVSS   XMM0,XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the Y component of a vector to an integer passed by value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntY(constref V: TXMVECTOR; constref y: UINT32): TXMVECTOR;
begin
    Result.u32[0] := V.u32[0];
    Result.u32[1] := y;
    Result.u32[2] := V.u32[2];
    Result.u32[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntY(constref V: TXMVECTOR; constref y: UINT32): TXMVECTOR;
begin
    (* ToDo
    return vsetq_lane_u32(y,V,1);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorSetIntY(constref V: TXMVECTOR; constref y: UINT32): TXMVECTOR; assembler;
asm
           //  __m128i vResult = _mm_castps_si128( V );
           MOVUPS  XMM0,[V]
           //vResult = _mm_insert_epi32( vResult, static_cast<int>(y), 1 );
           PINSRD  XMM0, [y], 1
           //return _mm_castsi128_ps( vResult );
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntY(constref V: TXMVECTOR; constref y: UINT32): TXMVECTOR; assembler;
asm
           // Swap y and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,2,0,1));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // Convert input to vector
           // __m128i vTemp = _mm_cvtsi32_si128(y);
           MOVD    XMM1, [y]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,_mm_castsi128_ps(vTemp));
           MOVSS   XMM0,XMM1
           // Swap y and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,2,0,1));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the Z component of a vector to an integer passed by value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntZ(constref V: TXMVECTOR; constref z: UINT32): TXMVECTOR;
begin
    Result.u32[0] := V.u32[0];
    Result.u32[1] := V.u32[1];
    Result.u32[2] := z;
    Result.u32[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntZ(constref V: TXMVECTOR; constref z: UINT32): TXMVECTOR;
begin
    (* ToDo
    return vsetq_lane_u32(z,V,2);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorSetIntZ(constref V: TXMVECTOR; constref z: UINT32): TXMVECTOR; assembler;
asm
           // __m128i vResult = _mm_castps_si128( V );
           MOVUPS  XMM0,[V]
           // vResult = _mm_insert_epi32( vResult, static_cast<int>(z), 2 );
           PINSRD  XMM0, [z], 2
           // return _mm_castsi128_ps( vResult );
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntZ(constref V: TXMVECTOR; constref z: UINT32): TXMVECTOR; assembler;
asm
           // Swap z and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,0,1,2));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // Convert input to vector
           // __m128i vTemp = _mm_cvtsi32_si128(z);
           MOVD    XMM1, [z]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,_mm_castsi128_ps(vTemp));
           MOVSS   XMM0,XMM1
           // Swap z and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,0,1,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}


// Sets the W component of a vector to an integer passed by value
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntW(constref V: TXMVECTOR; constref w: UINT32): TXMVECTOR;
begin
    Result.u32[0] := V.u32[0];
    Result.u32[1] := V.u32[1];
    Result.u32[2] := V.u32[2];
    Result.u32[3] := w;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntW(constref V: TXMVECTOR; constref w: UINT32): TXMVECTOR;
begin
    (* ToDo
    return vsetq_lane_u32(w,V,3);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorSetIntW(constref V: TXMVECTOR; constref w: UINT32): TXMVECTOR; assembler;
asm
           // vResult = _mm_castps_si128( V );
           MOVUPS  XMM0,[V]
           // vResult = _mm_insert_epi32( vResult, static_cast<int>(w), 3 );
           PINSRD  XMM0, [w], 3
           // return _mm_castsi128_ps( vResult );
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntW(constref V: TXMVECTOR; constref w: UINT32): TXMVECTOR; assembler;
asm
           // Swap w and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,2,1,3));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // Convert input to vector
           // __m128i vTemp = _mm_cvtsi32_si128(w);
           MOVD    XMM1, [w]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,_mm_castsi128_ps(vTemp));
           MOVSS   XMM0,XMM1
           // Swap w and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(0,2,1,3));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



//------------------------------------------------------------------------------

// Sets a component of a vector to an integer value passed by pointer
function XMVectorSetIntByIndexPtr(constref V: TXMVECTOR; constref x: PUINT32; constref i: size_t): TXMVECTOR;
begin
    assert(i < 4);
    Result := V;
    Result.u32[i] := x^;
end;

// Sets the X component of a vector to an integer value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntXPtr(constref V: TXMVECTOR; constref x: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := x^;
    Result.u32[1] := V.u32[1];
    Result.u32[2] := V.u32[2];
    Result.u32[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntXPtr(constref V: TXMVECTOR; constref x: PUINT32): TXMVECTOR;
begin
    (* ToDo
    return vld1q_lane_u32(x,*reinterpret_cast<const uint32x4_t *>(&V),0);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntXPtr(constref V: TXMVECTOR; constref x: PUINT32): TXMVECTOR; assembler;
asm
           // vTemp = _mm_load_ss(reinterpret_cast<const float *>(x));
           MOVSS   XMM1, [x]
           MOVUPS  XMM0,[V]
           // vResult = _mm_move_ss(V,vTemp);
           MOVSS   XMM0,XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}
//------------------------------------------------------------------------------




// Sets the Y component of a vector to an integer value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntYPtr(constref V: TXMVECTOR; constref y: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := V.u32[0];
    Result.u32[1] := y^;
    Result.u32[2] := V.u32[2];
    Result.u32[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntYPtr(constref V: TXMVECTOR; constref y: PUINT32): TXMVECTOR;
begin
    (* ToDo
    return vld1q_lane_u32(y,*reinterpret_cast<const uint32x4_t *>(&V),1);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntYPtr(constref V: TXMVECTOR; constref y: PUINT32): TXMVECTOR; assembler;
asm
           // Swap y and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,2,0,1));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // Convert input to vector
           // vTemp = _mm_load_ss(reinterpret_cast<const float *>(y));
           MOVSS   XMM1,[y]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0,XMM1
           // Swap y and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,2,0,1));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the Z component of a vector to an integer value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntZPtr(constref V: TXMVECTOR; constref z: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := V.u32[0];
    Result.u32[1] := V.u32[1];
    Result.u32[2] := z^;
    Result.u32[3] := V.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntZPtr(constref V: TXMVECTOR; constref z: PUINT32): TXMVECTOR;
begin
    (* ToDo
     return vld1q_lane_u32(z,*reinterpret_cast<const uint32x4_t *>(&V),2);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntZPtr(constref V: TXMVECTOR; constref z: PUINT32): TXMVECTOR; assembler;
asm
           // Swap z and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,0,1,2));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // Convert input to vector
           // vTemp = _mm_load_ss(reinterpret_cast<const float *>(z));
           MOVSS   XMM1,[z]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0,XMM1
           // Swap z and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,0,1,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



// Sets the W component of a vector to an integer value passed by pointer
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSetIntWPtr(constref V: TXMVECTOR; constref w: PUINT32): TXMVECTOR;
begin
    Result.u32[0] := V.u32[0];
    Result.u32[1] := V.u32[1];
    Result.u32[2] := V.u32[2];
    Result.u32[3] := w^;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSetIntWPtr(constref V: TXMVECTOR; constref w: PUINT32): TXMVECTOR;
begin
    (* ToDo
    return vld1q_lane_u32(w,*reinterpret_cast<const uint32x4_t *>(&V),3);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSetIntWPtr(constref V: TXMVECTOR; constref w: PUINT32): TXMVECTOR; assembler;
asm
           // Swap w and x
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,2,1,3));
           MOVUPS  XMM0,[V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // Convert input to vector
           // vTemp = _mm_load_ss(reinterpret_cast<const float *>(w));
           MOVSS   XMM1,[w]
           // Replace the x component
           // vResult = _mm_move_ss(vResult,vTemp);
           MOVSS   XMM0,XMM1
           // Swap w and x again
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(0,2,1,3));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_2_1_3
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}




{$IF DEFINED(_XM_NO_INTRINSICS_)}
//------------------------------------------------------------------------------
function XMVectorSwizzle(constref V: TXMVECTOR; constref SwizzleX, SwizzleY, SwizzleZ, SwizzleW: UINT32): TXMVECTOR;
var
    c: byte;
begin
    {$WARNING 'This code is not implemented in ASM, it would be inefficient cause no pre-compiling is possible'}
    assert((SwizzleX < 4) and (SwizzleY < 4) and (SwizzleZ < 4) and (SwizzleW < 4));
    Result.f32[0] := V.f32[SwizzleX];
    Result.f32[1] := V.f32[SwizzleY];
    Result.f32[2] := V.f32[SwizzleZ];
    Result.f32[3] := V.f32[SwizzleW];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSwizzle(V: TXMVECTOR; E0: UINT32; E1: UINT32; E2: UINT32; E3: UINT32): TXMVECTOR;
begin
    (*
     static const uint32_t ControlElement[ 4 ] =
    {
        0x03020100, // XM_SWIZZLE_X
        0x07060504, // XM_SWIZZLE_Y
        0x0B0A0908, // XM_SWIZZLE_Z
        0x0F0E0D0C, // XM_SWIZZLE_W
    };

    int8x8x2_t tbl;
    tbl.val[0] = vget_low_f32(V);
    tbl.val[1] = vget_high_f32(V);

    uint32x2_t idx = vcreate_u32( ((uint64_t)ControlElement[E0]) | (((uint64_t)ControlElement[E1]) << 32) );
    const uint8x8_t rL = vtbl2_u8( tbl, idx );

    idx = vcreate_u32( ((uint64_t)ControlElement[E2]) | (((uint64_t)ControlElement[E3]) << 32) );
    const uint8x8_t rH = vtbl2_u8( tbl, idx );

    return vcombine_f32( rL, rH );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorSwizzle(constref V: TXMVECTOR; constref SwizzleX, SwizzleY, SwizzleZ, SwizzleW: UINT32): TXMVECTOR; inline;
var
    elem: array[0..3] of UINT32;
begin
    elem[0] := SwizzleX;
    elem[1] := SwizzleY;
    elem[2] := SwizzleZ;
    elem[3] := SwizzleW;
    asm
               // unsigned int elem[4] = { E0, E1, E2, E3 };
               //_m128i vControl = _mm_loadu_si128( reinterpret_cast<const __m128i *>(&elem[0]) );
               MOVDQU  XMM0, [elem]
               // return _mm_permutevar_ps( V, vControl );
               MOVUPS  XMM1,[V]
               VPERMILPS XMM0, XMM1, XMM0
               MOVUPS  [result],XMM0
    end;
end;

{$ELSE}
//------------------------------------------------------------------------------
function XMVectorSwizzle(constref V: TXMVECTOR; constref SwizzleX, SwizzleY, SwizzleZ, SwizzleW: UINT32): TXMVECTOR;
begin
    assert((SwizzleX < 4) and (SwizzleY < 4) and (SwizzleZ < 4) and (SwizzleW < 4));

    Result.f32[0] := V.f32[SwizzleX];
    Result.f32[1] := V.f32[SwizzleY];
    Result.f32[2] := V.f32[SwizzleZ];
    Result.f32[3] := V.f32[SwizzleW];
end;

{$ENDIF}



{$IF DEFINED(_XM_SSE_INTRINSICS_)}
(*
function PermuteHelper(v1, v2: TXMVECTOR; Shuffle: uint32; WhichX, WhichY, WhichZ, WhichW: boolean): TXMVECTOR;
var
    selectMask: TXMVECTORU32;
    shuffled1, shuffled2: TXMVECTOR;
    masked1, masked2: TXMVECTOR;
begin

    if not WhichX and not WhichY and not WhichZ and not WhichW then
    begin
        // Fast path for permutes that only read from the first vector.
        Result := XM_PERMUTE_PS(v1, Shuffle);
        Exit;
    end

    else if WhichX and WhichY and WhichZ and WhichZ then
    begin
        // Fast path for permutes that only read from the second vector.
        Result := XM_PERMUTE_PS(v2, Shuffle);
        Exit;
    end
    else if not WhichX and not WhichY and WhichZ and WhichZ then
    begin
        // Fast path for permutes that read XY from the first vector, ZW from the second.
        asm
                   // result:= _mm_shuffle_ps(v1, v2, Shuffle);
               { MOVUPS XMM0,[V1]
    MOVUPS XMM1,[V2]
    shufps xmm0, xmm1, Shuffle
    MOVUPS [result], XMM0    }
        end;
    end
    else if WhichX and WhichY and not WhichZ and not WhichW then
    begin
        // Fast path for permutes that read XY from the second vector, ZW from the first.
        // ToDo    result:= _mm_shuffle_ps(v2, v1, Shuffle);
    end
    else
    begin
        if WhichX then
            selectMask.u[0] := $FFFFFFFF
        else
            selectMask.u[0] := 0;
        if WhichY then
            selectMask.u[1] := $FFFFFFFF
        else
            selectMask.u[1] := 0;
        if WhichZ then
            selectMask.u[2] := $FFFFFFFF
        else
            selectMask.u[2] := 0;
        if WhichW then
            selectMask.u[3] := $FFFFFFFF
        else
            selectMask.u[3] := 0;

        shuffled1 := XM_PERMUTE_PS(v1, Shuffle);
        shuffled2 := XM_PERMUTE_PS(v2, Shuffle);
             { ToDo
             masked1 := _mm_andnot_ps(selectMask, shuffled1);
              andnps xmm, xmm
             masked2 := _mm_and_ps(selectMask, shuffled2);

            result:= _mm_or_ps(masked1, masked2); }
            orps xmm, xmm
    end;
end;
*)
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorPermute(V1, V2: TXMVECTOR; PermuteX, PermuteY, PermuteZ, PermuteW: uint32): TXMVECTOR; inline;
var
    WhichX, WhichY, WhichZ, WhichW: boolean;
    Shuffle: uint32;
begin
    Shuffle := _MM_SHUFFLE(PermuteW and 3, PermuteZ and 3, PermuteY and 3, PermuteX and 3);
    WhichX := (PermuteX > 3);
    WhichY := (PermuteY > 3);
    WhichZ := (PermuteZ > 3);
    WhichW := (PermuteW > 3);
    //  Result := PermuteHelper(V1, V2, Shuffle, WhichX, WhichY, WhichZ, WhichW);
      {$WARNING 'Not implemented cause code inefficient'}
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_) AND NOT defined(_XM_NO_INTRINSICS_)}
function XMVectorPermute(V1, V2: TXMVECTOR; PermuteX, PermuteY, PermuteZ, PermuteW: uint32): TXMVECTOR; inline;
begin
    (*
     static const uint32_t ControlElement[ 8 ] =
    {
        0x03020100, // XM_PERMUTE_0X
        0x07060504, // XM_PERMUTE_0Y
        0x0B0A0908, // XM_PERMUTE_0Z
        0x0F0E0D0C, // XM_PERMUTE_0W
        0x13121110, // XM_PERMUTE_1X
        0x17161514, // XM_PERMUTE_1Y
        0x1B1A1918, // XM_PERMUTE_1Z
        0x1F1E1D1C, // XM_PERMUTE_1W
    };

    int8x8x4_t tbl;
    tbl.val[0] = vget_low_f32(V1);
    tbl.val[1] = vget_high_f32(V1);
    tbl.val[2] = vget_low_f32(V2);
    tbl.val[3] = vget_high_f32(V2);

    uint32x2_t idx = vcreate_u32( ((uint64_t)ControlElement[PermuteX]) | (((uint64_t)ControlElement[PermuteY]) << 32) );
    const uint8x8_t rL = vtbl4_u8( tbl, idx );

    idx = vcreate_u32( ((uint64_t)ControlElement[PermuteZ]) | (((uint64_t)ControlElement[PermuteW]) << 32) );
    const uint8x8_t rH = vtbl4_u8( tbl, idx );

    return vcombine_f32( rL, rH );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorPermute(V1: TXMVECTOR; V2: TXMVECTOR; PermuteX: UINT32; PermuteY: UINT32; PermuteZ: UINT32; PermuteW: UINT32): TXMVECTOR; inline;
const
    three: TXMVECTORU32 = (u: (3, 3, 3, 3));
var
    elem: array[0..3] of uint32;
begin
    elem[0] := PermuteX;
    elem[1] := PermuteY;
    elem[2] := PermuteZ;
    elem[3] := PermuteW;


    asm

               //  vControl = _mm_load_si128( reinterpret_cast<const __m128i *>(&elem[0]) );
               MOVUPS  XMM0,[elem]
               //  vSelect = _mm_cmpgt_epi32( vControl, three );
               MOVUPS  XMM1,XMM0
               PCMPGTD XMM1, [three]
               //vControl = _mm_castps_si128( _mm_and_ps( _mm_castsi128_ps( vControl ), three ) );
               ANDPS   XMM0, [three]
               //  shuffled1 = _mm_permutevar_ps( V1, vControl );
               MOVUPS  XMM2, [V1]
               VPERMILPS XMM2, XMM2, XMM0
               //  shuffled2 = _mm_permutevar_ps( V2, vControl );
               MOVUPS  XMM3, [V2]
               VPERMILPS XMM3, XMM3, XMM0
               //  masked1 = _mm_andnot_ps( _mm_castsi128_ps( vSelect ), shuffled1 );
               ANDNPS  XMM2, XMM1
               //  masked2 = _mm_and_ps( _mm_castsi128_ps( vSelect ), shuffled2 );
               ANDPS   XMM3,XMM1
               // return _mm_or_ps( masked1, masked2 );
               ORPS    XMM2, XMM3
               MOVUPS  [result],XMM2
    end;
end;

{$ELSE}
function XMVectorPermute(V1: TXMVECTOR; V2: TXMVECTOR; PermuteX: UINT32; PermuteY: UINT32; PermuteZ: UINT32; PermuteW: UINT32): TXMVECTOR; inline;
var
    aPtr: array [0..1] of TXMVECTOR;
    i0, i1, i2, i3: UINT32;
    vi0, vi1, vi2, vi3: UINT32;
begin
    assert((PermuteX <= 7) and (PermuteY <= 7) and (PermuteZ <= 7) and (PermuteW <= 7));
    aPtr[0] := V1;
    aPtr[1] := V2;

    i0 := PermuteX and 3;
    vi0 := PermuteX shr 2;
    Result.f32[0] := aPtr[vi0].f32[i0];

    i1 := PermuteY and 3;
    vi1 := PermuteY shr 2;
    Result.f32[1] := aPtr[vi1].f32[i1];

    i2 := PermuteZ and 3;
    vi2 := PermuteZ shr 2;
    Result.f32[2] := aPtr[vi2].f32[i2];

    i3 := PermuteW and 3;
    vi3 := PermuteW shr 2;
    Result.f32[3] := aPtr[vi3].f32[i3];
end;

{$ENDIF}


//------------------------------------------------------------------------------
// Define a control vector to be used in XMVectorSelect
// operations.  The four integers specified in XMVectorSelectControl
// serve as indices to select between components in two vectors.
// The first index controls selection for the first component of
// the vectors involved in a select operation, the second index
// controls selection for the second component etc.  A value of
// zero for an index causes the corresponding component from the first
// vector to be selected whereas a one causes the component from the
// second vector to be selected instead.

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSelectControl(constref VectorIndex0: UINT32; constref VectorIndex1: UINT32; constref VectorIndex2: UINT32; constref VectorIndex3: UINT32): TXMVECTOR;
const
    ControlElement: array [0..1] of UINT32 = ($00000000, $FFFFFFFF);
begin
    assert(VectorIndex0 < 2);
    assert(VectorIndex1 < 2);
    assert(VectorIndex2 < 2);
    assert(VectorIndex3 < 2);

    Result.u32[0] := ControlElement[VectorIndex0];
    Result.u32[1] := ControlElement[VectorIndex1];
    Result.u32[2] := ControlElement[VectorIndex2];
    Result.u32[3] := ControlElement[VectorIndex3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSelectControl(constref VectorIndex0: UINT32; constref VectorIndex1: UINT32; constref VectorIndex2: UINT32; constref VectorIndex3: UINT32): TXMVECTOR;
begin
    (* ToDo
     int32x2_t V0 = vcreate_s32(((uint64_t)VectorIndex0) | ((uint64_t)VectorIndex1 << 32));
    int32x2_t V1 = vcreate_s32(((uint64_t)VectorIndex2) | ((uint64_t)VectorIndex3 << 32));
    int32x4_t vTemp = vcombine_s32(V0, V1);
    // Any non-zero entries become 0xFFFFFFFF else 0
    return vcgtq_s32(vTemp,g_XMZero);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSelectControl(constref VectorIndex0: UINT32; constref VectorIndex1: UINT32; constref VectorIndex2: UINT32; constref VectorIndex3: UINT32): TXMVECTOR; assembler;
asm
           // x=Index0,y=Index1,z=Index2,w=Index3
           //__m128i vTemp = _mm_set_epi32(VectorIndex3,VectorIndex2,VectorIndex1,VectorIndex0);
           MOVAPS        XMM0,[VectorIndex3]
           MOVAPS        XMM1,[VectorIndex2]
           MOVAPS        XMM2,[VectorIndex1]
           MOVAPS        XMM3,[VectorIndex0]
           PUNPCKLDQ   XMM3,XMM1
           PUNPCKLDQ   XMM2,XMM0
           PUNPCKLDQ   XMM3,XMM2
           // Any non-zero entries become 0xFFFFFFFF else 0
           //  vTemp = _mm_cmpgt_epi32(vTemp,g_XMZero);
           PCMPGTD XMM3,[g_XMZero]
           // return _mm_castsi128_ps(vTemp);
           MOVUPS  [result],XMM3
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSelect(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Control: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := (V1.u32[0] and not Control.u32[0]) or (V2.u32[0] and Control.u32[0]);
    Result.u32[1] := (V1.u32[1] and not Control.u32[1]) or (V2.u32[1] and Control.u32[1]);
    Result.u32[2] := (V1.u32[2] and not Control.u32[2]) or (V2.u32[2] and Control.u32[2]);
    Result.u32[3] := (V1.u32[3] and not Control.u32[3]) or (V2.u32[3] and Control.u32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSelect(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Control: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vbslq_f32( Control, V2, V1 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSelect(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Control: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp1 = _mm_andnot_ps(Control,V1);
           MOVUPS  XMM0, [Control]
           ANDNPS  XMM0, [V1]
           // vTemp2 = _mm_and_ps(V2,Control);
           MOVUPS  XMM1, [V2]
           ANDPS   XMM1 , [Control]
           // return _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM0, XMM1
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorMergeXY(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := V1.u32[0];
    Result.u32[1] := V2.u32[0];
    Result.u32[2] := V1.u32[1];
    Result.u32[3] := V2.u32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorMergeXY(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vzipq_f32( V1, V2 ).val[0];
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorMergeXY(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  return _mm_unpacklo_ps( V1, V2 );
           MOVUPS  XMM0, [V1]
           UNPCKLPS XMM0, [V2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorMergeZW(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := V1.u32[2];
    Result.u32[1] := V2.u32[2];
    Result.u32[2] := V1.u32[3];
    Result.u32[3] := V2.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorMergeZW(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vzipq_f32( V1, V2 ).val[1];
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorMergeZW(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_unpackhi_ps( V1, V2 );
           MOVUPS  XMM0, [V1]
           UNPCKHPS XMM0, [V2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}



function XMVectorShiftLeft(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Elements: UINT32): TXMVECTOR;
begin
    assert(Elements < 4);
    Result := XMVectorPermute(V1, V2, Elements, ((Elements) + 1), ((Elements) + 2), ((Elements) + 3));
end;


function XMVectorRotateLeft(constref V: TXMVECTOR; constref Elements: UINT32): TXMVECTOR;
begin
    assert(Elements < 4);
    Result := XMVectorSwizzle(V, Elements and 3, (Elements + 1) and 3, (Elements + 2) and 3, (Elements + 3) and 3);
end;


function XMVectorRotateRight(constref V: TXMVECTOR; constref Elements: UINT32): TXMVECTOR;
begin
    assert(Elements < 4);
    Result := XMVectorSwizzle(V, (4 - (Elements)) and 3, (5 - (Elements)) and 3, (6 - (Elements)) and 3, (7 - (Elements)) and 3);
end;

function XMVectorInsert(constref VD: TXMVECTOR; constref VS: TXMVECTOR; constref VSLeftRotateElements: UINT32; constref Select0: UINT32; constref Select1: UINT32; constref Select2: UINT32; constref Select3: UINT32): TXMVECTOR;
var
    Control: TXMVECTOR;
begin
    Control := XMVectorSelectControl(Select0 and 1, Select1 and 1, Select2 and 1, Select3 and 1);
    Result := XMVectorSelect(VD, XMVectorRotateLeft(VS, VSLeftRotateElements), Control);
end;

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] = V2.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.f32[1] = V2.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.f32[2] = V2.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.f32[3] = V2.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vceqq_f32( V1, V2 );
*)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVectorEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]
           VCMPPS  XMM0, XMM1, XMM2, 0
           VMOVUPS [Result], XMM0
end;
{$ELSE}//  _XM_SSE_INTRINSICS_
function XMVectorEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           MOVUPS  XMM0,[v1]
           MOVUPS  XMM2,[v2]
           CMPPS   XMM0, XMM2, 0
           MOVUPS  [Result], XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    ux, uy, uz, uw, CR: UINT32;
begin
    if (V1.f32[0] = V2.f32[0]) then
        ux := $FFFFFFFF
    else
        ux := 0;
    if (V1.f32[1] = V2.f32[1]) then
        uy := $FFFFFFFF
    else
        uy := 0;
    if (V1.f32[2] = V2.f32[2]) then
        uz := $FFFFFFFF
    else
        uz := 0;
    if (V1.f32[3] = V2.f32[3]) then
        uw := $FFFFFFFF
    else
        uw := 0;
    CR := 0;
    if (ux and uy and uz and uw) <> 0 then
    begin
        // All elements are greater
        CR := XM_CRMASK_CR6TRUE;
    end
    else if (not (ux or uy or uz or uw) <> 0) then
    begin
        // All elements are not greater
        CR := XM_CRMASK_CR6FALSE;
    end;
    pCR := CR;

    Result.f32[0] := ux;
    Result.f32[0] := uy;
    Result.f32[0] := uz;
    Result.f32[0] := uw;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (*
     uint32x4_t vResult = vceqq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        // All elements are equal
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        // All elements are not equal
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return vResult;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
var
    CR: uint32 = 0;
asm
           // vTemp = _mm_cmpeq_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 0
           MOVUPS  [result],XMM0
           // int iTest = _mm_movemask_ps(vTemp);
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $F
           CMP     EAX, $F
           JE      @Finished
           // All elements are not greater
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [pCR],EDX
end;
{$ENDIF}


//------------------------------------------------------------------------------
// Treat the components of the vectors as unsigned integers and
// compare individual bits between the two.  This is useful for
// comparing control vectors and result vectors returned from
// other comparison operations.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.u32[0] = V2.u32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.u32[1] = V2.u32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.u32[2] = V2.u32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.u32[3] = V2.u32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vceqq_u32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_cmpeq_epi32( _mm_castps_si128(V1),_mm_castps_si128(V2) );
           MOVUPS  XMM0,[V1]
           PCMPEQD XMM0, [V2]
           //  return _mm_castsi128_ps(V);
           MOVUPS  [result],XMM0
end;
{$ENDIF}




{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorEqualIntR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    Control: TXMVECTOR;
begin
    Control := XMVectorEqualInt(V1, V2);

    pCR := 0;
    if (XMVector4EqualInt(Control, XMVectorTrueInt())) then
    begin
        // All elements are equal
        pCR := pCR or XM_CRMASK_CR6TRUE;
    end
    else if (XMVector4EqualInt(Control, XMVectorFalseInt())) then
    begin
        // All elements are not equal
        pCR := pCR or XM_CRMASK_CR6FALSE;
    end;
    Result := Control;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorEqualIntR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t vResult = vceqq_u32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        // All elements are equal
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        // All elements are not equal
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorEqualIntR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_cmpeq_epi32( _mm_castps_si128(V1),_mm_castps_si128(V2) );
           MOVUPS  XMM0,[V1]
           PCMPEQD XMM0, [V2]
           MOVUPS  [result],XMM0
           // int iTemp = _mm_movemask_ps(_mm_castsi128_ps(V));
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $F
           CMP     EAX, $F
           JE      @Finished
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [pCR],EDX
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorNearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): TXMVECTOR;
var
    fDeltax, fDeltay, fDeltaz, fDeltaw: single;
begin
    fDeltax := V1.f32[0] - V2.f32[0];
    fDeltay := V1.f32[1] - V2.f32[1];
    fDeltaz := V1.f32[2] - V2.f32[2];
    fDeltaw := V1.f32[3] - V2.f32[3];

    fDeltax := abs(fDeltax);
    fDeltay := abs(fDeltay);
    fDeltaz := abs(fDeltaz);
    fDeltaw := abs(fDeltaw);

    if (fDeltax <= Epsilon.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (fDeltay <= Epsilon.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (fDeltaz <= Epsilon.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (fDeltaw <= Epsilon.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorNearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    XMVECTOR vDelta = vsubq_f32(V1,V2);
    return vacleq_f32( vDelta, Epsilon );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorNearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Get the difference
           // vDelta = _mm_sub_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           SUBPS   XMM0,[V2]
           // Get the absolute value of the difference
           // vTemp = _mm_setzero_ps();
           XORPS   XMM1,XMM1
           // vTemp = _mm_sub_ps(vTemp,vDelta);
           SUBPS   XMM1,XMM0
           // vTemp = _mm_max_ps(vTemp,vDelta);
           MAXPS   XMM1, XMM0
           // vTemp = _mm_cmple_ps(vTemp,Epsilon);
           CMPPS   XMM1, [Epsilon], 6
           // return vTemp;
           MOVUPS  [result],XMM1
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorNotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] <> V2.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.f32[1] <> V2.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.f32[2] <> V2.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.f32[3] <> V2.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorNotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vmvnq_u32(vceqq_f32(V1, V2));
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorNotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_cmpneq_ps( V1, V2 );
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 4
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorNotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.u32[0] <> V2.u32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.u32[1] <> V2.u32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.u32[2] <> V2.u32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.u32[3] <> V2.u32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorNotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   return vmvnq_u32(vceqq_u32(V1, V2));
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorNotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
asm
           // __m128i V = _mm_cmpeq_epi32( _mm_castps_si128(V1),_mm_castps_si128(V2) );
           MOVUPS  XMM0,[V1]
           PCMPEQD XMM0, [V2]
           // return _mm_xor_ps(_mm_castsi128_ps(V),g_XMNegOneMask);
           XORPS   XMM0,[g_XMNegOneMask]
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGreater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] > V2.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.f32[1] > V2.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.f32[2] > V2.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.f32[3] > V2.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGreater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   return vcgtq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGreater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
asm
           // return _mm_cmpgt_ps( V1, V2 );
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 1

           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGreaterR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    ux, uy, uz, uw, cr: UINT32;
begin
    if (V1.f32[0] > V2.f32[0]) then
        ux := $FFFFFFFF
    else
        ux := 0;
    if (V1.f32[1] > V2.f32[1]) then
        uy := $FFFFFFFF
    else
        uy := 0;
    if (V1.f32[2] > V2.f32[2]) then
        uz := $FFFFFFFF
    else
        uz := 0;
    if (V1.f32[3] > V2.f32[3]) then
        uw := $FFFFFFFF
    else
        uw := 0;
    CR := 0;
    if (ux and uy and uz and uw) <> 0 then
    begin
        // All elements are greater
        CR := XM_CRMASK_CR6TRUE;
    end
    else if (not (ux or uy or uz or uw) <> 0) then
    begin
        // All elements are not greater
        CR := XM_CRMASK_CR6FALSE;
    end;
    pCR := CR;
    Result.u32[0] := ux;
    Result.u32[1] := uy;
    Result.u32[2] := uz;
    Result.u32[3] := uw;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGreaterR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t vResult = vcgtq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        // All elements are greater
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        // All elements are not greater
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGreaterR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_cmpgt_ps(V1,V2);
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 1
           MOVUPS  [result],XMM0
           // uint32_t CR = 0;
           //  int iTest = _mm_movemask_ps(vTemp);
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $F
           CMP     EAX, $F
           JE      @Finished
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [pCR],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] >= V2.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.f32[1] >= V2.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.f32[2] >= V2.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.f32[3] >= V2.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vcgeq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
asm
           // return _mm_cmpge_ps( V1, V2 );
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 2
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorGreaterOrEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    ux, uy, uz, uw, Cr: UINT32;
begin
    if (V1.f32[0] >= V2.f32[0]) then
        ux := $FFFFFFFF
    else
        ux := 0;
    if (V1.f32[1] >= V2.f32[1]) then
        uy := $FFFFFFFF
    else
        uy := 0;
    if (V1.f32[2] >= V2.f32[2]) then
        uz := $FFFFFFFF
    else
        uz := 0;
    if (V1.f32[3] >= V2.f32[3]) then
        uw := $FFFFFFFF
    else
        uw := 0;
    CR := 0;
    if (ux and uy and uz and uw) <> 0 then
    begin
        // All elements are greater
        CR := XM_CRMASK_CR6TRUE;
    end
    else if (not (ux or uy or uz or uw) <> 0) then
    begin
        // All elements are not greater
        CR := XM_CRMASK_CR6FALSE;
    end;
    pCR := CR;
    Result.u32[0] := ux;
    Result.u32[1] := uy;
    Result.u32[2] := uz;
    Result.u32[3] := uw;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorGreaterOrEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t vResult = vcgeq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        // All elements are greater or equal
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        // All elements are not greater or equal
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorGreaterOrEqualR(out pCR: UINT32; constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
asm
           // vTemp = _mm_cmpge_ps(V1,V2);
           MOVUPS  XMM0,[v2]
           CMPPS   XMM0, [v1], 2
           MOVUPS  [result],XMM0
           //uint32_t CR = 0;
           //int iTest = _mm_movemask_ps(vTemp);
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $F
           CMP     EAX, $F
           JE      @Finished
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [pCR],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorLess(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] < V2.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.f32[1] < V2.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.f32[2] < V2.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.f32[3] < V2.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorLess(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vcltq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorLess(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_cmplt_ps( V1, V2 );
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 1
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorLessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] <= V2.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V1.f32[1] <= V2.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V1.f32[2] <= V2.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V1.f32[3] <= V2.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorLessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vcleq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorLessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_cmple_ps( V1, V2 );
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 2
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorInBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR;
begin
    if (V.f32[0] <= Bounds.f32[0]) and (V.f32[0] >= -Bounds.f32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if (V.f32[1] <= Bounds.f32[1]) and (V.f32[1] >= -Bounds.f32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if (V.f32[2] <= Bounds.f32[2]) and (V.f32[2] >= -Bounds.f32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if (V.f32[3] <= Bounds.f32[3]) and (V.f32[3] >= -Bounds.f32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorInBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Test if less than or equal
    XMVECTOR vTemp1 = vcleq_f32(V,Bounds);
    // Negate the bounds
    XMVECTOR vTemp2 = vnegq_f32(Bounds);
    // Test if greater or equal (Reversed)
    vTemp2 = vcleq_f32(vTemp2,V);
    // Blend answers
    vTemp1 = vandq_u32(vTemp1,vTemp2);
    return vTemp1;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorInBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR;
asm
           // Test if less than or equal
           // vTemp1 = _mm_cmple_ps(V,Bounds);
           MOVUPS  XMM0, [V]
           MOVUPS  XMM1, [Bounds]
           CMPPS   XMM0,XMM1 , 2
           // Negate the bounds
           // vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
           MULPS   XMM1,[g_XMNegativeOne]
           // Test if greater or equal (Reversed)
           // vTemp2 = _mm_cmple_ps(vTemp2,V);
           CMPPS   XMM1,[V] , 2
           // Blend answers
           // vTemp1 = _mm_and_ps(vTemp1,vTemp2);
           ANDPS   XMM0,XMM1
           // return vTemp1;
           MOVUPS  [result], XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorInBoundsR(out pCR: UINT32; constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR;
var
    ux, uy, uz, uw, cr: UINT32;
begin
    if ((V.f32[0] <= Bounds.f32[0]) and (V.f32[0] >= -Bounds.f32[0])) then
        ux := $FFFFFFFF
    else
        ux := 0;
    if ((V.f32[1] <= Bounds.f32[1]) and (V.f32[1] >= -Bounds.f32[1])) then
        uy := $FFFFFFFF
    else
        uy := 0;
    if ((V.f32[2] <= Bounds.f32[2]) and (V.f32[2] >= -Bounds.f32[2])) then
        uz := $FFFFFFFF
    else
        uz := 0;
    if ((V.f32[3] <= Bounds.f32[3]) and (V.f32[3] >= -Bounds.f32[3])) then
        uw := $FFFFFFFF
    else
        uw := 0;

    CR := 0;
    if (ux and uy and uz and uw) <> 0 then
    begin
        // All elements are in bounds
        CR := XM_CRMASK_CR6BOUNDS;
    end;
    pCR := CR;
    Result.u32[0] := ux;
    Result.u32[1] := uy;
    Result.u32[2] := uz;
    Result.u32[3] := uw;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorInBoundsR(out pCR: UINT32; constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Test if less than or equal
    XMVECTOR vTemp1 = vcleq_f32(V,Bounds);
    // Negate the bounds
    XMVECTOR vTemp2 = vnegq_f32(Bounds);
    // Test if greater or equal (Reversed)
    vTemp2 = vcleq_f32(vTemp2,V);
    // Blend answers
    vTemp1 = vandq_u32(vTemp1,vTemp2);
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vTemp1), vget_high_u8(vTemp1));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        // All elements are in bounds
        CR = XM_CRMASK_CR6BOUNDS;
    }
    *pCR = CR;
    return vTemp1;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorInBoundsR(out pCR: UINT32; constref V: TXMVECTOR; constref Bounds: TXMVECTOR): TXMVECTOR;
asm
           // Test if less than or equal
           //  vTemp1 = _mm_cmple_ps(V,Bounds);
           MOVUPS  XMM0,[V]
           CMPPS   XMM0, [Bounds], 2
           // Negate the bounds
           // vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
           MOVUPS  XMM1,[Bounds]
           MULPS   XMM1,[g_XMNegativeOne]
           // Test if greater or equal (Reversed)
           // vTemp2 = _mm_cmple_ps(vTemp2,V);
           CMPPS   XMM1, [V], 2
           // Blend answers
           // vTemp1 = _mm_and_ps(vTemp1,vTemp2);
           ANDPS   XMM0, XMM1
           MOVUPS  [result],XMM0
           MOVMSKPS EAX, XMM0

           // All elements are in bounds
           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $F
           CMP     EAX, $F
           JE      @Finished
           MOV     EDX,0
           @Finished:
           MOV     [pCR],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorIsNaN(constref V: TXMVECTOR): TXMVECTOR;
begin
    if XMISNAN(V.u32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if XMISNAN(V.u32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if XMISNAN(V.u32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if XMISNAN(V.u32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorIsNaN(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Test against itself. NaN is always not equal
    uint32x4_t vTempNan = vceqq_f32( V, V );
    // Flip results
    return vmvnq_u32( vTempNan );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorIsNaN(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Test against itself. NaN is always not equal
           // return _mm_cmpneq_ps(V,V);
           MOVUPS  XMM0,[V]
           CMPPS   XMM0, XMM0, 4
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorIsInfinite(constref V: TXMVECTOR): TXMVECTOR;
begin
    if XMISINF(V.u32[0]) then
        Result.u32[0] := $FFFFFFFF
    else
        Result.u32[0] := 0;
    if XMISINF(V.u32[1]) then
        Result.u32[1] := $FFFFFFFF
    else
        Result.u32[1] := 0;
    if XMISINF(V.u32[2]) then
        Result.u32[2] := $FFFFFFFF
    else
        Result.u32[2] := 0;
    if XMISINF(V.u32[3]) then
        Result.u32[3] := $FFFFFFFF
    else
        Result.u32[3] := 0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorIsInfinite(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Mask off the sign bit
    uint32x4_t vTemp = vandq_u32(V,g_XMAbsMask);
    // Compare to infinity
    vTemp = vceqq_f32(vTemp,g_XMInfinity);
    // If any are infinity, the signs are true.
    return vTemp;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorIsInfinite(constref V: TXMVECTOR): TXMVECTOR;
asm
           // Mask off the sign bit
           // __m128 vTemp = _mm_and_ps(V,g_XMAbsMask);
           MOVUPS  XMM0, [V]
           ANDPS   XMM0,[g_XMAbsMask]
           // Compare to infinity
           // vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 0
           // If any are infinity, the signs are true.
           // return vTemp;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Rounding and clamping operations
//------------------------------------------------------------------------------

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorMin(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] < V2.f32[0]) then
        Result.f32[0] := V1.f32[0]
    else
        Result.f32[0] := V2.f32[0];
    if (V1.f32[1] < V2.f32[1]) then
        Result.f32[1] := V1.f32[1]
    else
        Result.f32[1] := V2.f32[1];
    if (V1.f32[2] < V2.f32[2]) then
        Result.f32[2] := V1.f32[2]
    else
        Result.f32[2] := V2.f32[2];
    if (V1.f32[3] < V2.f32[3]) then
        Result.f32[3] := V1.f32[3]
    else
        Result.f32[3] := V2.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorMin(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vminq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorMin(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  return _mm_min_ps( V1, V2 );
           MOVUPS  XMM0,[V1]
           MINPS   XMM0, [V2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorMax(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    if (V1.f32[0] > V2.f32[0]) then
        Result.f32[0] := V1.f32[0]
    else
        Result.f32[0] := V2.f32[0];
    if (V1.f32[1] > V2.f32[1]) then
        Result.f32[1] := V1.f32[1]
    else
        Result.f32[1] := V2.f32[1];
    if (V1.f32[2] > V2.f32[2]) then
        Result.f32[2] := V1.f32[2]
    else
        Result.f32[2] := V2.f32[2];
    if (V1.f32[3] > V2.f32[3]) then
        Result.f32[3] := V1.f32[3]
    else
        Result.f32[3] := V2.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorMax(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
     (* ToDo
     return vmaxq_f32( V1, V2 );
 *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorMax(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           //   return _mm_max_ps( V1, V2 );
           MOVUPS  XMM0, [V1]
           MAXPS   XMM0, [V2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

// Round to nearest (even) a.k.a. banker's rounding
function round_to_nearest(x: single): single; inline;
var
    i: single;
    int_part: single;
begin
    i := floor(x);
    x := x - i;
    if (x < 0.5) then
    begin
        Result := i;
        Exit;
    end;
    if (x > 0.5) then
    begin
        Result := i + 1.0;
        Exit;
    end;

    int_part := round(i / 2.0);
    if ((2.0 * int_part) = i) then
    begin
        Result := i;
        Exit;
    end;
    Result := i + 1.0;
end;

// ToDo float_control(precise, on)
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorRound(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := round_to_nearest(V.f32[0]);
    Result.f32[1] := round_to_nearest(V.f32[1]);
    Result.f32[2] := round_to_nearest(V.f32[2]);
    Result.f32[3] := round_to_nearest(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorRound(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    return vrndnq_f32(V);
#else
    uint32x4_t sign = vandq_u32( V, g_XMNegativeZero );
    uint32x4_t sMagic = vorrq_u32( g_XMNoFraction, sign );
    float32x4_t R1 = vaddq_f32( V, sMagic );
    R1 = vsubq_f32( R1, sMagic );
    float32x4_t R2 = vabsq_f32( V );
    uint32x4_t mask = vcleq_f32( R2, g_XMNoFraction );
    XMVECTOR vResult = vbslq_f32( mask, R1, V );
    return vResult;
#endif
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorRound(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_round_ps( V, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC );
           MOVUPS  XMM0, [V]
           ROUNDPS XMM0, XMM0, _MM_FROUND_TO_NEAREST_INT or _MM_FROUND_NO_EXC
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorRound(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128 sign = _mm_and_ps( V, g_XMNegativeZero );
           MOVUPS  XMM0, [V]
           MOVUPS  XMM1,XMM0
           MOVUPS  XMM2,XMM0
           ANDPS   XMM0, [g_XMNegativeZero]
           // __m128 sMagic = _mm_or_ps( g_XMNoFraction, sign );
           ORPS    XMM0, [g_XMNoFraction]
           // __m128 R1 = _mm_add_ps( V, sMagic );
           ADDPS   XMM1, XMM0
           // R1 = _mm_sub_ps( R1, sMagic );
           SUBPS   XMM1, XMM0
           //__m128 R2 = _mm_and_ps( V, g_XMAbsMask );
           ANDPS   XMM2, [g_XMAbsMask]
           // __m128 mask = _mm_cmple_ps( R2, g_XMNoFraction );
           MOVUPS  XMM3,XMM2
           CMPPS   XMM3, [g_XMNoFraction], 2
           // R2 = _mm_andnot_ps(mask,V);
           MOVUPS  XMM2,XMM3
           ANDNPS  XMM2, [V]
           // R1 = _mm_and_ps(R1,mask);
           ANDPS   XMM1, XMM3
           // vResult = _mm_xor_ps(R1, R2);
           XORPS   XMM1,XMM2
           // return vResult;
           MOVUPS  [result],XMM1
end;
{$ENDIF}
// ToDo #pragma float_control(pop)

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorTruncate(constref V: TXMVECTOR): TXMVECTOR;
var
    i: uint32;
begin

    // Avoid C4701 -> Pascal never does such shit
    Result.f32[0] := 0.0;

    for i := 0 to 3 do
    begin
        if (XMISNAN(V.u32[i])) then
        begin
            Result.u32[i] := $7FC00000;
        end
        else if (abs(V.f32[i]) < 8388608.0) then
        begin
            Result.f32[i] := V.f32[i];
        end
        else
        begin
            Result.f32[i] := V.f32[i];

        end;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorTruncate(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    return vrndq_f32(V);
#else
    float32x4_t vTest = vabsq_f32( V );
    vTest = vcltq_f32( vTest, g_XMNoFraction );

    int32x4_t vInt = vcvtq_s32_f32( V );
    XMVECTOR vResult = vcvtq_f32_s32( vInt );

    // All numbers less than 8388608 will use the round to int
    // All others, use the ORIGINAL value
    return vbslq_f32( vTest, vResult, V );
#endif
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorTruncate(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_round_ps( V, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC );
           MOVUPS  XMM0,[V]
           ROUNDPS XMM0, XMM0, _MM_FROUND_TO_ZERO or _MM_FROUND_NO_EXC
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorTruncate(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // To handle NAN, INF and numbers greater than 8388608, use masking
           // Get the abs value
           //__m128i vTest = _mm_and_si128(_mm_castps_si128(V),g_XMAbsMask);
           MOVUPS  XMM0, [V]
           PAND    XMM0, [g_XMAbsMask]
           // Test for greater than 8388608 (All floats with NO fractionals, NAN and INF
           // vTest = _mm_cmplt_epi32(vTest,g_XMNoFraction);
           PCMPGTD XMM0, [g_XMNoFraction]
           // Convert to int and back to float for rounding with truncation
           // __m128i vInt = _mm_cvttps_epi32(V);
           CVTTPS2DQ XMM1, [V]
           // Convert back to floats
           // vResult = _mm_cvtepi32_ps(vInt);
           CVTDQ2PS XMM1, XMM1
           // All numbers less than 8388608 will use the round to int
           // vResult = _mm_and_ps(vResult,_mm_castsi128_ps(vTest));
           ANDPS   XMM1,XMM0
           // All others, use the ORIGINAL value
           // vTest = _mm_andnot_si128(vTest,_mm_castps_si128(V));
           PANDN   XMM0, [V]
           // vResult = _mm_or_ps(vResult,_mm_castsi128_ps(vTest));
           ORPS    XMM1, XMM0
           // return vResult;
           MOVUPS  [result],XMM1
end;
{$ENDIF}




{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorFloor(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := floor(V.f32[0]);
    Result.f32[1] := floor(V.f32[1]);
    Result.f32[2] := floor(V.f32[2]);
    Result.f32[3] := floor(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorFloor(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    return vrndmq_f32(V);
#else
    float32x4_t vTest = vabsq_f32( V );
    vTest = vcltq_f32( vTest, g_XMNoFraction );
    // Truncate
    int32x4_t vInt = vcvtq_s32_f32( V );
    XMVECTOR vResult = vcvtq_f32_s32( vInt );
    XMVECTOR vLarger = vcgtq_f32( vResult, V );
    // 0 -> 0, 0xffffffff -> -1.0f
    vLarger = vcvtq_f32_s32( vLarger );
    vResult = vaddq_f32( vResult, vLarger );
    // All numbers less than 8388608 will use the round to int
    // All others, use the ORIGINAL value
    return vbslq_f32( vTest, vResult, V );
#endif
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorFloor(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_floor_ps( V );
           MOVUPS  XMM0,[V]
           ROUNDPS XMM0, XMM0, _MM_FROUND_TO_NEAREST_INT
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorFloor(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // To handle NAN, INF and numbers greater than 8388608, use masking
           // __m128i vTest = _mm_and_si128(_mm_castps_si128(V),g_XMAbsMask);
           MOVUPS  XMM0, [V]
           PAND    XMM0, [g_XMAbsMask]
           // vTest = _mm_cmplt_epi32(vTest,g_XMNoFraction);
           PCMPGTD XMM0, [g_XMNoFraction]
           // Truncate
           //__m128i vInt = _mm_cvttps_epi32(V);
           CVTTPS2DQ XMM1, [V]
           // vResult = _mm_cvtepi32_ps(vInt);
           CVTDQ2PS XMM1, XMM1
           // __m128 vLarger = _mm_cmpgt_ps( vResult, V );
           MOVUPS  XMM2,[V]
           CMPPS   XMM2, XMM1, 1  // vLarger = XMM2
           // 0 -> 0, 0xffffffff -> -1.0f
           // vLarger = _mm_cvtepi32_ps( _mm_castps_si128( vLarger ) );
           CVTDQ2PS XMM2, XMM2
           // vResult = _mm_add_ps( vResult, vLarger );
           ADDPS   XMM1,XMM2
           // All numbers less than 8388608 will use the round to int
           // vResult = _mm_and_ps(vResult,_mm_castsi128_ps(vTest));
           ANDPS   XMM1, XMM0
           // All others, use the ORIGINAL value
           // vTest = _mm_andnot_si128(vTest,_mm_castps_si128(V));
           PANDN   XMM0, [V]
           // vResult = _mm_or_ps(vResult,_mm_castsi128_ps(vTest));
           ORPS    XMM1, XMM0
           // return vResult;
           MOVUPS  [result],XMM1
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorCeiling(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := ceil(V.f32[0]);
    Result.f32[1] := ceil(V.f32[1]);
    Result.f32[2] := ceil(V.f32[2]);
    Result.f32[3] := ceil(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorCeiling(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    return vrndpq_f32(V);
#else
    float32x4_t vTest = vabsq_f32( V );
    vTest = vcltq_f32( vTest, g_XMNoFraction );
    // Truncate
    int32x4_t vInt = vcvtq_s32_f32( V );
    XMVECTOR vResult = vcvtq_f32_s32( vInt );
    XMVECTOR vSmaller = vcltq_f32( vResult, V );
    // 0 -> 0, 0xffffffff -> -1.0f
    vSmaller = vcvtq_f32_s32( vSmaller );
    vResult = vsubq_f32( vResult, vSmaller );
    // All numbers less than 8388608 will use the round to int
    // All others, use the ORIGINAL value
    return vbslq_f32( vTest, vResult, V );
#endif
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVectorCeiling(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // _mm_ceil_ps( V );
           MOVUPS  XMM0, [V]
           ROUNDPS XMM0, XMM0, _MM_FROUND_TO_POS_INF
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorCeiling(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // To handle NAN, INF and numbers greater than 8388608, use masking
           // __m128i vTest = _mm_and_si128(_mm_castps_si128(V),g_XMAbsMask);
           MOVUPS  XMM0,[V]
           PAND    XMM0, [g_XMAbsMask]
           // vTest = _mm_cmplt_epi32(vTest,g_XMNoFraction);
           PCMPGTD XMM0, [g_XMNoFraction]
           // Truncate
           // __m128i vInt = _mm_cvttps_epi32(V);
           CVTTPS2DQ XMM1, [V]
           // vResult = _mm_cvtepi32_ps(vInt);
           CVTDQ2PS XMM1, XMM1
           // __m128 vSmaller = _mm_cmplt_ps( vResult, V );
           MOVUPS  XMM2,XMM1
           CMPPS   XMM2, [V], 1 // vSmaller = XMM2
           // 0 -> 0, 0xffffffff -> -1.0f
           // vSmaller = _mm_cvtepi32_ps( _mm_castps_si128( vSmaller ) );
           CVTDQ2PS XMM2, XMM2
           // vResult = _mm_sub_ps( vResult, vSmaller );
           SUBPS   XMM1, XMM2
           // All numbers less than 8388608 will use the round to int
           // vResult = _mm_and_ps(vResult,_mm_castsi128_ps(vTest));
           ANDPS   XMM1, XMM0
           // All others, use the ORIGINAL value
           // vTest = _mm_andnot_si128(vTest,_mm_castps_si128(V));
           PANDN   XMM0, [V]
           // vResult = _mm_or_ps(vResult,_mm_castsi128_ps(vTest));
           ORPS    XMM1, XMM0
           // return vResult;
           MOVUPS  [result],XMM1
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorClamp(constref V: TXMVECTOR; constref Min: TXMVECTOR; constref Max: TXMVECTOR): TXMVECTOR;
begin
    assert(XMVector4LessOrEqual(Min, Max));
    Result := XMVectorMax(Min, V);
    Result := XMVectorMin(Max, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorClamp(constref V: TXMVECTOR; constref Min: TXMVECTOR; constref Max: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    XMVECTOR vResult;
    vResult = vmaxq_f32(Min,V);
    vResult = vminq_f32(vResult,Max);
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorClamp(constref V: TXMVECTOR; constref Min: TXMVECTOR; constref Max: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vResult = _mm_max_ps(Min,V);
           MOVUPS  XMM0, [Min]
           MAXPS   XMM0, [V]
           // vResult = _mm_min_ps(vResult,Max);
           MINPS   XMM0, [Max]
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSaturate(constref V: TXMVECTOR): TXMVECTOR;
var
    Zero: TXMVECTOR;
begin
    Zero := XMVectorZero();
    Result := XMVectorClamp(V, Zero, g_XMOne.v);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSaturate(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Set <0 to 0
    XMVECTOR vResult = vmaxq_f32(V, vdupq_n_f32(0) );
    // Set>1 to 1
    return vminq_f32(vResult, vdupq_n_f32(1.0f) );s
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSaturate(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Set <0 to 0
           // vResult = _mm_max_ps(V,g_XMZero);
           MOVUPS  XMM0,[V]
           MAXPS   XMM0, [g_XMZero]
           // Set>1 to 1
           // return _mm_min_ps(vResult,g_XMOne);
           MINPS   XMM0, [g_XMOne]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Bitwise logical operations
//------------------------------------------------------------------------------

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorAndInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := V1.u32[0] and V2.u32[0];
    Result.u32[1] := V1.u32[1] and V2.u32[1];
    Result.u32[2] := V1.u32[2] and V2.u32[2];
    Result.u32[3] := V1.u32[3] and V2.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorAndInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vandq_u32(V1,V2);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorAndInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_and_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           ANDPS   XMM0, [V2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorAndCInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := V1.u32[0] and not V2.u32[0];
    Result.u32[1] := V1.u32[1] and not V2.u32[1];
    Result.u32[2] := V1.u32[2] and not V2.u32[2];
    Result.u32[3] := V1.u32[3] and not V2.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorAndCInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   return vbicq_u32(V1,V2);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorAndCInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_andnot_si128( _mm_castps_si128(V2), _mm_castps_si128(V1) );
           MOVUPS  XMM0,[v1]
           PANDN   XMM0, [V2]
           //  return _mm_castsi128_ps(V);
           MOVUPS  [result], XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorOrInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := V1.u32[0] or V2.u32[0];
    Result.u32[1] := V1.u32[1] or V2.u32[1];
    Result.u32[2] := V1.u32[2] or V2.u32[2];
    Result.u32[3] := V1.u32[3] or V2.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorOrInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vorrq_u32(V1,V2);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorOrInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_or_si128( _mm_castps_si128(V1), _mm_castps_si128(V2) );
           MOVUPS  XMM0,[V1]
           POR     XMM0, [V2]
           // return _mm_castsi128_ps(V);
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorNorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := not (V1.u32[0] or V2.u32[0]);
    Result.u32[1] := not (V1.u32[1] or V2.u32[1]);
    Result.u32[2] := not (V1.u32[2] or V2.u32[2]);
    Result.u32[3] := not (V1.u32[3] or V2.u32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorNorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t Result = vorrq_u32(V1,V2);
    return vbicq_u32(g_XMNegOneMask, Result);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorNorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i Result;
           // Result = _mm_or_si128( _mm_castps_si128(V1), _mm_castps_si128(V2) );
           MOVUPS  XMM0,[V1]
           POR     XMM0, [V2]
           // Result = _mm_andnot_si128( Result,g_XMNegOneMask);
           PANDN   XMM0, [g_XMNegOneMask]
           // return _mm_castsi128_ps(Result);
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorXorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.u32[0] := V1.u32[0] xor V2.u32[0];
    Result.u32[1] := V1.u32[1] xor V2.u32[1];
    Result.u32[2] := V1.u32[2] xor V2.u32[2];
    Result.u32[3] := V1.u32[3] xor V2.u32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorXorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return veorq_u32(V1,V2);
     *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorXorInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i V = _mm_xor_si128( _mm_castps_si128(V1), _mm_castps_si128(V2) );
           MOVUPS  XMM0, [V1]
           PXOR    XMM0, [V2]
           // return _mm_castsi128_ps(V);
           MOVUPS  [result],XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorNegate(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := -V.f32[0];
    Result.f32[1] := -V.f32[1];
    Result.f32[2] := -V.f32[2];
    Result.f32[3] := -V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorNegate(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vnegq_f32(V);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorNegate(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Z = _mm_setzero_ps();
           XORPS   XMM0,XMM0
           // return _mm_sub_ps( Z, V );
           SUBPS   XMM0,[V]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := V1.f32[0] + V2.f32[0];
    Result.f32[1] := V1.f32[1] + V2.f32[1];
    Result.f32[2] := V1.f32[2] + V2.f32[2];
    Result.f32[3] := V1.f32[3] + V2.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vaddq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_add_ps( V1, V2 );
           MOVUPS  XMM0,[V1]
           ADDPS   XMM0,[V2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSum(constref V: TXMVECTOR): TXMVECTOR;
var
    f: single;
begin
    f := V.f32[0] + V.f32[1] + V.f32[2] + V.f32[3];
    Result.f32[0] := f;
    Result.f32[1] := f;
    Result.f32[2] := f;
    Result.f32[3] := f;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSum(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    XMVECTOR vTemp = vpaddq_f32(V, V);
    return vpaddq_f32(vTemp,vTemp);
#else
    float32x2_t v1 = vget_low_f32(V);
    float32x2_t v2 = vget_high_f32(V);
    v1 = vadd_f32(v1, v2);
    v1 = vpadd_f32(v1, v1);
    return vcombine_f32(v1, v1);
#endif
*)
end;

{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVectorSum(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_hadd_ps(V, V);
           MOVUPS  XMM0,[V]
           HADDPS  XMM0, XMM0
           // return _mm_hadd_ps(vTemp,vTemp);
           HADDPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSum(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = XM_PERMUTE_PS(V, _MM_SHUFFLE(2, 3, 0, 1));
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_3_0_1
           // vTemp2 = _mm_add_ps(V, vTemp);
           ADDPS   XMM1, XMM0
           // vTemp = XM_PERMUTE_PS(vTemp2, _MM_SHUFFLE(1, 0, 3, 2));
           MOVUPS  XMM0, XMM1
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_3_2
           // return _mm_add_ps(vTemp, vTemp2);
           ADDPS   XMM0, XMM1
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorAddAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    Zero, Mask, Offset: TXMVECTOR;
begin
    Zero := XMVectorZero();

    // Add the given angles together.  If the range of V1 is such
    // that -Pi <= V1 < Pi and the range of V2 is such that
    // -2Pi <= V2 <= 2Pi, then the range of the resulting angle
    // will be -Pi <= Result < Pi.
    Result := XMVectorAdd(V1, V2);

    Mask := XMVectorLess(Result, g_XMNegativePi.v);
    Offset := XMVectorSelect(Zero, g_XMTwoPi.v, Mask);

    Mask := XMVectorGreaterOrEqual(Result, g_XMPi.v);
    Offset := XMVectorSelect(Offset, g_XMNegativeTwoPi.v, Mask);

    Result := XMVectorAdd(Result, Offset);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorAddAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Adjust the angles
    XMVECTOR vResult = vaddq_f32(V1,V2);
    // Less than Pi?
    uint32x4_t vOffset = vcltq_f32(vResult,g_XMNegativePi);
    vOffset = vandq_u32(vOffset,g_XMTwoPi);
    // Add 2Pi to all entries less than -Pi
    vResult = vaddq_f32(vResult,vOffset);
    // Greater than or equal to Pi?
    vOffset = vcgeq_f32(vResult,g_XMPi);
    vOffset = vandq_u32(vOffset,g_XMTwoPi);
    // Sub 2Pi to all entries greater than Pi
    vResult = vsubq_f32(vResult,vOffset);
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorAddAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Adjust the angles
           // vResult = _mm_add_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           ADDPS   XMM0,[V2]

           // Less than Pi?
           // vOffset = _mm_cmplt_ps(vResult,g_XMNegativePi);
           MOVUPS  XMM1,XMM0
           CMPPS   XMM1, [g_XMNegativePi], 1

           // vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
           ANDPS   XMM1, [g_XMTwoPi]
           // Add 2Pi to all entries less than -Pi
           // vResult = _mm_add_ps(vResult,vOffset);
           ADDPS   XMM0, XMM1
           // Greater than or equal to Pi?
           // vOffset = _mm_cmpge_ps(vResult,g_XMPi);
           MOVUPS  XMM1,XMM0
           CMPPS   XMM1, [g_XMPi], 2
           // vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
           ANDPS   XMM1, [g_XMTwoPi]
           // Sub 2Pi to all entries greater than Pi
           // vResult = _mm_sub_ps(vResult,vOffset);
           SUBPS   XMM0, XMM1
           // return vResult;
           MOVUPS  [result], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := V1.f32[0] - V2.f32[0];
    Result.f32[1] := V1.f32[1] - V2.f32[1];
    Result.f32[2] := V1.f32[2] - V2.f32[2];
    Result.f32[3] := V1.f32[3] - V2.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vsubq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_sub_ps( V1, V2 );
           MOVUPS  XMM0,[v1]
           SUBPS   XMM0,[v2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSubtractAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    Zero, Mask, Offset: TXMVECTOR;
begin
    Zero := XMVectorZero();

    // Subtract the given angles.  If the range of V1 is such
    // that -Pi <= V1 < Pi and the range of V2 is such that
    // -2Pi <= V2 <= 2Pi, then the range of the resulting angle
    // will be -Pi <= Result < Pi.
    Result := XMVectorSubtract(V1, V2);

    Mask := XMVectorLess(Result, g_XMNegativePi.v);
    Offset := XMVectorSelect(Zero, g_XMTwoPi.v, Mask);

    Mask := XMVectorGreaterOrEqual(Result, g_XMPi.v);
    Offset := XMVectorSelect(Offset, g_XMNegativeTwoPi.v, Mask);

    Result := XMVectorAdd(Result, Offset);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSubtractAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Adjust the angles
    XMVECTOR vResult = vsubq_f32(V1,V2);
    // Less than Pi?
    uint32x4_t vOffset = vcltq_f32(vResult,g_XMNegativePi);
    vOffset = vandq_u32(vOffset,g_XMTwoPi);
    // Add 2Pi to all entries less than -Pi
    vResult = vaddq_f32(vResult,vOffset);
    // Greater than or equal to Pi?
    vOffset = vcgeq_f32(vResult,g_XMPi);
    vOffset = vandq_u32(vOffset,g_XMTwoPi);
    // Sub 2Pi to all entries greater than Pi
    vResult = vsubq_f32(vResult,vOffset);
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSubtractAngles(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Adjust the angles
           // vResult = _mm_sub_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           SUBPS   XMM0, [V2]
           // Less than Pi?
           // vOffset = _mm_cmplt_ps(vResult,g_XMNegativePi);
           MOVUPS  XMM1,XMM0
           CMPPS   XMM1, [g_XMNegativePi], 1
           //  vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
           ANDPS   XMM1, [g_XMTwoPi]
           // Add 2Pi to all entries less than -Pi
           // vResult = _mm_add_ps(vResult,vOffset);
           ADDPS   XMM0,XMM1
           // Greater than or equal to Pi?
           // vOffset = _mm_cmpge_ps(vResult,g_XMPi);
           MOVUPS  XMM1, XMM0
           CMPPS   XMM1, [g_XMPi], 2
           // vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
           ANDPS   XMM1, [g_XMTwoPi]
           // Sub 2Pi to all entries greater than Pi
           // vResult = _mm_sub_ps(vResult,vOffset);
           SUBPS   XMM0, XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorMultiply(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := V1.f32[0] * V2.f32[0];
    Result.f32[0] := V1.f32[1] * V2.f32[1];
    Result.f32[0] := V1.f32[2] * V2.f32[2];
    Result.f32[0] := V1.f32[3] * V2.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorMultiply(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vmulq_f32( V1, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorMultiply(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_mul_ps( V1, V2 );
           MOVUPS  XMM0,[V1]
           MULPS   XMM0, [V2]
           MOVUPS  [result],XMM0

end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorMultiplyAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := V1.f32[0] * V2.f32[0] + V3.f32[0];
    Result.f32[1] := V1.f32[1] * V2.f32[1] + V3.f32[1];
    Result.f32[2] := V1.f32[2] * V2.f32[2] + V3.f32[2];
    Result.f32[3] := V1.f32[3] * V2.f32[3] + V3.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorMultiplyAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vmlaq_f32( V3, V1, V2 );
*)
end;

{$ELSEIF DEFINED(_XM_FMA3_INTRINSICS_)}
function XMVectorMultiplyAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR; assembler;
asm
           // _mm_fmadd_ps( V1, V2, V3 );
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM2, [V2]
           MOVUPS  XMM1,[V3]
           VFMADD132PS XMM0, XMM1, XMM2        // Multiply packed single-precision floating-point values from xmm0 and xmm2/mem, add to xmm1 and put result in xmm0.
           MOVUPS  [result],XMM0

end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorMultiplyAdd(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vResult = _mm_mul_ps( V1, V2 );
           MOVUPS  XMM0,[v1]
           MULPS   XMM0, [v2]
           // return _mm_add_ps(vResult, V3 );
           ADDPS   XMM0,[v3]
           MOVUPS  [result],XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorDivide(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := V1.f32[0] / V2.f32[0];
    Result.f32[1] := V1.f32[1] / V2.f32[1];
    Result.f32[2] := V1.f32[2] / V2.f32[2];
    Result.f32[3] := V1.f32[3] / V2.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorDivide(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    return vdivq_f32( V1, V2 );
#else
    // 2 iterations of Newton-Raphson refinement of reciprocal
    float32x4_t Reciprocal = vrecpeq_f32(V2);
    float32x4_t S = vrecpsq_f32( Reciprocal, V2 );
    Reciprocal = vmulq_f32( S, Reciprocal );
    S = vrecpsq_f32( Reciprocal, V2 );
    Reciprocal = vmulq_f32( S, Reciprocal );
    return vmulq_f32( V1, Reciprocal );
#endif
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorDivide(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_div_ps( V1, V2 );
           MOVUPS  XMM0, [V1]
           DIVPS   XMM0, [V2]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Computes the difference of a third vector and the product of the first two vectors.
// Result := V3-V1*V2;
function XMVectorNegativeMultiplySubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := V3.f32[0] - (V1.f32[0] * V2.f32[0]);
    Result.f32[1] := V3.f32[1] - (V1.f32[1] * V2.f32[1]);
    Result.f32[2] := V3.f32[2] - (V1.f32[2] * V2.f32[2]);
    Result.f32[3] := V3.f32[3] - (V1.f32[3] * V2.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorNegativeMultiplySubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vmlsq_f32( V3, V1, V2 );
*)
end;

{$ELSEIF DEFINED(_XM_FMA3_INTRINSICS_)}
function XMVectorNegativeMultiplySubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return  _mm_fnmadd_ps(V1, V2, V3);
           MOVUPS  XMM0,[V1]
           MOVUPS  XMM2,[V2]
           MOVUPS  XMM1, [V3]
           VFNMADD132PS XMM0, XMM1, XMM2 // Multiply packed single-precision floating-point val-ues from xmm0 and xmm2/mem, negate the multi-plication result and add to xmm1 and put result in xmm0.
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorNegativeMultiplySubtract(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR; assembler;
asm
           // R = _mm_mul_ps( V1, V2 );
           MOVUPS  XMM1,[V1]
           MULPS   XMM1, [V2]
           // return _mm_sub_ps( V3, R );
           MOVUPS  XMM0,[V3]
           SUBPS   XMM0,XMM1
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorScale(constref V: TXMVECTOR; constref ScaleFactor: single): TXMVECTOR;
begin
    Result.f32[0] := V.f32[0] * ScaleFactor;
    Result.f32[1] := V.f32[1] * ScaleFactor;
    Result.f32[2] := V.f32[2] * ScaleFactor;
    Result.f32[3] := V.f32[3] * ScaleFactor;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorScale(constref V: TXMVECTOR; constref ScaleFactor: single): TXMVECTOR;
begin
    (* ToDo
    return vmulq_n_f32( V, ScaleFactor );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorScale(constref V: TXMVECTOR; constref ScaleFactor: single): TXMVECTOR; assembler;
asm
           // vResult = _mm_set_ps1(ScaleFactor);
           MOVSS   XMM1, [ScaleFactor]
           SHUFPS  XMM1, XMM1, 0
           // return _mm_mul_ps(vResult,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM1
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReciprocalEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := 1.0 / V.f32[0];
    Result.f32[1] := 1.0 / V.f32[1];
    Result.f32[2] := 1.0 / V.f32[2];
    Result.f32[3] := 1.0 / V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReciprocalEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     return vrecpeq_f32(V);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReciprocalEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_rcp_ps(V);
           RCPPS   XMM0, [V]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReciprocal(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := 1.0 / V.f32[0];
    Result.f32[1] := 1.0 / V.f32[1];
    Result.f32[2] := 1.0 / V.f32[2];
    Result.f32[3] := 1.0 / V.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReciprocal(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    float32x4_t one = vdupq_n_f32(1.0f);
    return vdivq_f32(one,V);
#else
    // 2 iterations of Newton-Raphson refinement
    float32x4_t Reciprocal = vrecpeq_f32(V);
    float32x4_t S = vrecpsq_f32( Reciprocal, V );
    Reciprocal = vmulq_f32( S, Reciprocal );
    S = vrecpsq_f32( Reciprocal, V );
    return vmulq_f32( S, Reciprocal );
#endif
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReciprocal(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_div_ps(g_XMOne,V);
           MOVUPS  XMM0,[g_XMOne]
           DIVPS   XMM0, [V]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Return an estimated square root
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSqrtEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := sqrt(V.f32[0]);
    Result.f32[1] := sqrt(V.f32[1]);
    Result.f32[2] := sqrt(V.f32[2]);
    Result.f32[3] := sqrt(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSqrtEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // 1 iteration of Newton-Raphson refinment of sqrt
    float32x4_t S0 = vrsqrteq_f32(V);
    float32x4_t P0 = vmulq_f32( V, S0 );
    float32x4_t R0 = vrsqrtsq_f32( P0, S0 );
    float32x4_t S1 = vmulq_f32( S0, R0 );

    XMVECTOR VEqualsInfinity = XMVectorEqualInt(V, g_XMInfinity.v);
    XMVECTOR VEqualsZero = XMVectorEqual(V, vdupq_n_f32(0) );
    XMVECTOR Result = vmulq_f32( V, S1 );
    XMVECTOR Select = XMVectorEqualInt(VEqualsInfinity, VEqualsZero);
    return XMVectorSelect(V, Result, Select);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSqrtEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_sqrt_ps(V);
           MOVUPS  XMM0,[V]
           SQRTPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSqrt(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := sqrt(V.f32[0]);
    Result.f32[1] := sqrt(V.f32[1]);
    Result.f32[2] := sqrt(V.f32[2]);
    Result.f32[3] := sqrt(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSqrt(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // 3 iterations of Newton-Raphson refinment of sqrt
    float32x4_t S0 = vrsqrteq_f32(V);
    float32x4_t P0 = vmulq_f32( V, S0 );
    float32x4_t R0 = vrsqrtsq_f32( P0, S0 );
    float32x4_t S1 = vmulq_f32( S0, R0 );
    float32x4_t P1 = vmulq_f32( V, S1 );
    float32x4_t R1 = vrsqrtsq_f32( P1, S1 );
    float32x4_t S2 = vmulq_f32( S1, R1 );
    float32x4_t P2 = vmulq_f32( V, S2 );
    float32x4_t R2 = vrsqrtsq_f32( P2, S2 );
    float32x4_t S3 = vmulq_f32( S2, R2 );

    XMVECTOR VEqualsInfinity = XMVectorEqualInt(V, g_XMInfinity.v);
    XMVECTOR VEqualsZero = XMVectorEqual(V, vdupq_n_f32(0) );
    XMVECTOR Result = vmulq_f32( V, S3 );
    XMVECTOR Select = XMVectorEqualInt(VEqualsInfinity, VEqualsZero);
    return XMVectorSelect(V, Result, Select);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSqrt(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_sqrt_ps(V);
           MOVUPS  XMM0,[V]
           SQRTPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReciprocalSqrtEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := 1.0 / sqrt(V.f32[0]);
    Result.f32[1] := 1.0 / sqrt(V.f32[1]);
    Result.f32[2] := 1.0 / sqrt(V.f32[2]);
    Result.f32[3] := 1.0 / sqrt(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReciprocalSqrtEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    return vrsqrteq_f32(V);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReciprocalSqrtEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_rsqrt_ps(V);
           MOVUPS  XMM0,[V]
           RSQRTPS XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorReciprocalSqrt(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := 1.0 / sqrt(V.f32[0]);
    Result.f32[1] := 1.0 / sqrt(V.f32[1]);
    Result.f32[2] := 1.0 / sqrt(V.f32[2]);
    Result.f32[3] := 1.0 / sqrt(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorReciprocalSqrt(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // 2 iterations of Newton-Raphson refinement of reciprocal
    float32x4_t S0 = vrsqrteq_f32(V);

    float32x4_t P0 = vmulq_f32( V, S0 );
    float32x4_t R0 = vrsqrtsq_f32( P0, S0 );

    float32x4_t S1 = vmulq_f32( S0, R0 );
    float32x4_t P1 = vmulq_f32( V, S1 );
    float32x4_t R1 = vrsqrtsq_f32( P1, S1 );

    return vmulq_f32( S1, R1 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorReciprocalSqrt(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vResult = _mm_sqrt_ps(V);
           MOVUPS  XMM1,[V]
           SQRTPS  XMM1, XMM1
           // vResult = _mm_div_ps(g_XMOne,vResult);
           MOVUPS  XMM0,[g_XMOne]
           DIVPS   XMM0, XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorExp2(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := power(2.0, V.f32[0]);
    Result.f32[1] := power(2.0, V.f32[1]);
    Result.f32[2] := power(2.0, V.f32[2]);
    Result.f32[3] := power(2.0, V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorExp2(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    int32x4_t itrunc = vcvtq_s32_f32(V);
    float32x4_t ftrunc = vcvtq_f32_s32(itrunc);
    float32x4_t y = vsubq_f32(V, ftrunc);

    float32x4_t poly = vmlaq_f32( g_XMExpEst6, g_XMExpEst7, y );
    poly = vmlaq_f32( g_XMExpEst5, poly, y );
    poly = vmlaq_f32( g_XMExpEst4, poly, y );
    poly = vmlaq_f32( g_XMExpEst3, poly, y );
    poly = vmlaq_f32( g_XMExpEst2, poly, y );
    poly = vmlaq_f32( g_XMExpEst1, poly, y );
    poly = vmlaq_f32( g_XMOne, poly, y );

    int32x4_t biased = vaddq_s32(itrunc, g_XMExponentBias);
    biased = vshlq_n_s32(biased, 23);
    float32x4_t result0 = XMVectorDivide(biased, poly);

    biased = vaddq_s32(itrunc, g_XM253);
    biased = vshlq_n_s32(biased, 23);
    float32x4_t result1 = XMVectorDivide(biased, poly);
    result1 = vmulq_f32(g_XMMinNormal.v, result1);

    // Use selection to handle the cases
    //  if (V is NaN) -> QNaN;
    //  else if (V sign bit set)
    //      if (V > -150)
    //         if (V.exponent < -126) -> result1
    //         else -> result0
    //      else -> +0
    //  else
    //      if (V < 128) -> result0
    //      else -> +inf

    int32x4_t comp = vcltq_s32( V, g_XMBin128);
    float32x4_t result2 = vbslq_f32( comp, result0, g_XMInfinity );

    comp = vcltq_s32(itrunc, g_XMSubnormalExponent);
    float32x4_t result3 = vbslq_f32( comp, result1, result0 );

    comp = vcltq_s32(V, g_XMBinNeg150);
    float32x4_t result4 = vbslq_f32( comp, result3, g_XMZero );

    int32x4_t sign = vandq_s32(V, g_XMNegativeZero);
    comp = vceqq_s32(sign, g_XMNegativeZero);
    float32x4_t result5 = vbslq_f32( comp, result4, result2 );

    int32x4_t t0 = vandq_s32(V, g_XMQNaNTest);
    int32x4_t t1 = vandq_s32(V, g_XMInfinity);
    t0 = vceqq_s32(t0, g_XMZero);
    t1 = vceqq_s32(t1, g_XMInfinity);
    int32x4_t isNaN = vbicq_s32( t1,t0);

    float32x4_t vResult = vbslq_f32( isNaN, g_XMQNaN, result5 );
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorExp2(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i itrunc = _mm_cvttps_epi32(V);
           MOVUPS  XMM4,[V]
           CVTTPS2DQ XMM4, XMM4 // itrunc = xmm4
           // __m128 ftrunc = _mm_cvtepi32_ps(itrunc);
           CVTDQ2PS XMM0, XMM4
           // __m128 y = _mm_sub_ps(V, ftrunc);
           MOVUPS  XMM1,[V]
           SUBPS   XMM1,XMM0  // y = XMM1
           // __m128 poly = _mm_mul_ps(g_XMExpEst7, y);
           MOVUPS  XMM0, [g_XMExpEst7]
           MULPS   XMM0, XMM1 // poly = XMM0
           // poly = _mm_add_ps(g_XMExpEst6, poly);
           ADDPS   XMM0, [g_XMExpEst6]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM0, XMM1
           // poly = _mm_add_ps(g_XMExpEst5, poly);
           ADDPS   XMM0,[g_XMExpEst5]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM0, XMM1
           // poly = _mm_add_ps(g_XMExpEst4, poly);
           ADDPS   XMM0, [g_XMExpEst4]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM0, XMM1
           // poly = _mm_add_ps(g_XMExpEst3, poly);
           ADDPS   XMM0, [g_XMExpEst3]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM0, XMM1
           // poly = _mm_add_ps(g_XMExpEst2, poly);
           ADDPS   XMM0,[g_XMExpEst2]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM0, XMM1
           // poly = _mm_add_ps(g_XMExpEst1, poly);
           ADDPS   XMM0, [g_XMExpEst1]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM0, XMM1
           // poly = _mm_add_ps(g_XMOne, poly);
           ADDPS   XMM0,[g_XMOne]  // XMM0 = poly, XMM4 = itrunc

           //__m128i biased = _mm_add_epi32(itrunc, g_XMExponentBias);
           MOVUPS  XMM1, XMM4
           PADDD   XMM1, [g_XMExponentBias]
           // biased = _mm_slli_epi32(biased, 23);
           PSLLD   XMM1, 23
           //__m128 result0 = _mm_div_ps(_mm_castsi128_ps(biased), poly);
           DIVPS   XMM1, XMM0 // XMM0 = poly, XMM1 = result0,  XMM4 = itrunc

           // biased = _mm_add_epi32(itrunc, g_XM253);
           MOVUPS  XMM2, XMM4
           PADDD   XMM2, [g_XM253]
           // biased = _mm_slli_epi32(biased, 23);
           PSLLD   XMM2, 23
           // __m128 result1 = _mm_div_ps(_mm_castsi128_ps(biased), poly);
           DIVPS   XMM2, XMM0
           // result1 = _mm_mul_ps(g_XMMinNormal.v, result1);
           MULPS   XMM2, [g_XMMinNormal] // XMM1 = result0, XMM2 = result1, XMM4 = itrunc

           // Use selection to handle the cases
           //  if (V is NaN) -> QNaN;
           //  else if (V sign bit set)
           //      if (V > -150)
           //         if (V.exponent < -126) -> result1
           //         else -> result0
           //      else -> +0
           //  else
           //      if (V < 128) -> result0
           //      else -> +inf

           // __m128i comp = _mm_cmplt_epi32( _mm_castps_si128(V), g_XMBin128);
           MOVUPS  XMM0,[V]
           PCMPGTD XMM0, [g_XMBin128]  // XMM0 = comp
           // __m128i select0 = _mm_and_si128(comp, _mm_castps_si128(result0));
           MOVUPS  XMM3, XMM0
           PAND    XMM3, XMM1   // select0 = XMM3
           // __m128i select1 = _mm_andnot_si128(comp, g_XMInfinity);
           MOVUPS  XMM5, XMM0
           PANDN   XMM5, [g_XMInfinity]
           // __m128i result2 = _mm_or_si128(select0, select1);
           POR     XMM3, XMM5      // result2 = XMM3

           // comp = _mm_cmplt_epi32(itrunc, g_XMSubnormalExponent);
           PCMPGTD XMM4, [g_XMSubnormalExponent] // comp = XMM4
           // select1 = _mm_and_si128(comp, _mm_castps_si128(result1));
           MOVUPS  XMM5, XMM4
           PAND    XMM5, XMM2
           // select0 = _mm_andnot_si128(comp, _mm_castps_si128(result0));
           PANDN   XMM6, XMM4
           // __m128i result3 = _mm_or_si128(select0, select1);
           POR     XMM6, XMM5 // XMM6 = result3

           // comp = _mm_cmplt_epi32(_mm_castps_si128(V), g_XMBinNeg150);
           MOVUPS  XMM0, [V]
           PCMPGTD XMM0, [g_XMBinNeg150]
           // select0 = _mm_and_si128(comp, result3);
           MOVUPS  XMM1, XMM0
           PAND    XMM1, XMM6
           // select1 = _mm_andnot_si128(comp, g_XMZero);
           PANDN   XMM0, [g_XMZero]
           // __m128i result4 = _mm_or_si128(select0, select1);
           POR     XMM0, XMM1 // XMM0 = result4, result2 = XMM3

           // __m128i sign = _mm_and_si128(_mm_castps_si128(V), g_XMNegativeZero);
           MOVUPS  XMM1, [V]
           PAND    XMM1, [g_XMNegativeZero]
           // comp = _mm_cmpeq_epi32(sign, g_XMNegativeZero);
           PCMPEQD XMM1, [g_XMNegativeZero]
           // select0 = _mm_and_si128(comp, result4);
           PAND    XMM0, XMM1
           // select1 = _mm_andnot_si128(comp, result2);
           PANDN   XMM3, XMM1
           // __m128i result5 = _mm_or_si128(select0, select1);
           POR     XMM0, XMM3 // XMM0 = result5

           // __m128i t0 = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
           MOVUPS  XMM1,[V]
           MOVUPS  XMM2,XMM1
           PAND    XMM1, [g_XMQNaNTest]
           // __m128i t1 = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
           PAND    XMM2, [g_XMInfinity]
           // t0 = _mm_cmpeq_epi32(t0, g_XMZero);
           PCMPEQD XMM1, [g_XMZero]
           // t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
           PCMPEQD XMM2, [g_XMInfinity]
           // __m128i isNaN = _mm_andnot_si128(t0, t1);
           PANDN   XMM1, XMM2 // XMM1 = isNaN

           // select0 = _mm_and_si128(isNaN, g_XMQNaN);
           MOVUPS  XMM2,XMM1
           PAND    XMM1, [g_XMQNaN]
           // select1 = _mm_andnot_si128(isNaN, result5);
           PANDN   XMM2, XMM0
           // __m128i vResult = _mm_or_si128(select0, select1);
           POR     XMM1, XMM2

           // return _mm_castsi128_ps(vResult);
           MOVUPS  [result],XMM1
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorExpE(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := exp(V.f32[0]);
    Result.f32[1] := exp(V.f32[1]);
    Result.f32[2] := exp(V.f32[2]);
    Result.f32[3] := exp(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorExpE(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   // expE(V) = exp2(vin*log2(e))
    float32x4_t Ve = vmulq_f32(g_XMLgE, V);

    int32x4_t itrunc = vcvtq_s32_f32(Ve);
    float32x4_t ftrunc = vcvtq_f32_s32(itrunc);
    float32x4_t y = vsubq_f32(Ve, ftrunc);


    float32x4_t poly = vmlaq_f32( g_XMExpEst6, g_XMExpEst7, y );
    poly = vmlaq_f32( g_XMExpEst5, poly, y );
    poly = vmlaq_f32( g_XMExpEst4, poly, y );
    poly = vmlaq_f32( g_XMExpEst3, poly, y );
    poly = vmlaq_f32( g_XMExpEst2, poly, y );
    poly = vmlaq_f32( g_XMExpEst1, poly, y );
    poly = vmlaq_f32( g_XMOne, poly, y );

    int32x4_t biased = vaddq_s32(itrunc, g_XMExponentBias);
    biased = vshlq_n_s32(biased, 23);
    float32x4_t result0 = XMVectorDivide(biased, poly);

    biased = vaddq_s32(itrunc, g_XM253);
    biased = vshlq_n_s32(biased, 23);
    float32x4_t result1 = XMVectorDivide(biased, poly);
    result1 = vmulq_f32(g_XMMinNormal.v, result1);

    // Use selection to handle the cases
    //  if (V is NaN) -> QNaN;
    //  else if (V sign bit set)
    //      if (V > -150)
    //         if (V.exponent < -126) -> result1
    //         else -> result0
    //      else -> +0
    //  else
    //      if (V < 128) -> result0
    //      else -> +inf

    int32x4_t comp = vcltq_s32( Ve, g_XMBin128);
    float32x4_t result2 = vbslq_f32( comp, result0, g_XMInfinity );

    comp = vcltq_s32(itrunc, g_XMSubnormalExponent);
    float32x4_t result3 = vbslq_f32( comp, result1, result0 );

    comp = vcltq_s32(Ve, g_XMBinNeg150);
    float32x4_t result4 = vbslq_f32( comp, result3, g_XMZero );

    int32x4_t sign = vandq_s32(Ve, g_XMNegativeZero);
    comp = vceqq_s32(sign, g_XMNegativeZero);
    float32x4_t result5 = vbslq_f32( comp, result4, result2 );

    int32x4_t t0 = vandq_s32(Ve, g_XMQNaNTest);
    int32x4_t t1 = vandq_s32(Ve, g_XMInfinity);
    t0 = vceqq_s32(t0, g_XMZero);
    t1 = vceqq_s32(t1, g_XMInfinity);
    int32x4_t isNaN = vbicq_s32( t1,t0);

    float32x4_t vResult = vbslq_f32( isNaN, g_XMQNaN, result5 );
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorExpE(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // expE(V) = exp2(vin*log2(e))
           //__m128 Ve = _mm_mul_ps(g_XMLgE, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, [g_XMLgE]

           //__m128i itrunc = _mm_cvttps_epi32(Ve);
           CVTTPS2DQ XMM1, XMM0  // XMM0 = Ve, XMM1 = itrunc
           // __m128 ftrunc = _mm_cvtepi32_ps(itrunc);
           CVTDQ2PS XMM2, XMM1
           // __m128 y = _mm_sub_ps(Ve, ftrunc);
           MOVUPS  XMM3,XMM0
           SUBPS   XMM3, XMM2  // XMM0 = Ve, XMM1 = itrunc, XMM3 = y
           // __m128 poly = _mm_mul_ps(g_XMExpEst7, y);
           MOVUPS  XMM2, [g_XMExpEst7]
           MULPS   XMM2, XMM3
           // poly = _mm_add_ps(g_XMExpEst6, poly);
           ADDPS   XMM2, [g_XMExpEst6]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM2, XMM3
           // poly = _mm_add_ps(g_XMExpEst5, poly);
           ADDPS   XMM2, [g_XMExpEst5]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM2, XMM3
           // poly = _mm_add_ps(g_XMExpEst4, poly);
           ADDPS   XMM2, [g_XMExpEst4]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM2, XMM3
           // poly = _mm_add_ps(g_XMExpEst3, poly);
           ADDPS   XMM2, [g_XMExpEst3]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM2, XMM3
           // poly = _mm_add_ps(g_XMExpEst2, poly);
           ADDPS   XMM2, [g_XMExpEst2]
           //poly = _mm_mul_ps(poly, y);
           MULPS   XMM2, XMM3
           // poly = _mm_add_ps(g_XMExpEst1, poly);
           ADDPS   XMM2, [g_XMExpEst1]
           // poly = _mm_mul_ps(poly, y);
           MULPS   XMM2, XMM3
           // poly = _mm_add_ps(g_XMOne, poly);
           ADDPS   XMM2, [g_XMOne] // XMM0 = Ve, XMM1 = itrunc, XMM2 = poly

           // __m128i biased = _mm_add_epi32(itrunc, g_XMExponentBias);
           MOVUPS  XMM3, [g_XMExponentBias]
           PADDD   XMM3, XMM1
           // biased = _mm_slli_epi32(biased, 23);
           PSLLD   XMM3, 23
           // __m128 result0 = _mm_div_ps(_mm_castsi128_ps(biased), poly);
           DIVPS   XMM3, XMM2

           // biased = _mm_add_epi32(itrunc, g_XM253);
           MOVUPS  XMM4, [g_XM253]
           PADDD   XMM4, XMM1
           // biased = _mm_slli_epi32(biased, 23);
           PSLLD   XMM4, 23
           //__m128 result1 = _mm_div_ps(_mm_castsi128_ps(biased), poly);
           DIVPS   XMM4, XMM2
           // result1 = _mm_mul_ps(g_XMMinNormal.v, result1);
           MULPS   XMM4, [g_XMMinNormal] // XMM0 = Ve, XMM1 = itrunc,  xmm3 = result0, xmm4 = result1

           // Use selection to handle the cases
           //  if (V is NaN) -> QNaN;
           //  else if (V sign bit set)
           //      if (V > -150)
           //         if (V.exponent < -126) -> result1
           //         else -> result0
           //      else -> +0
           //  else
           //      if (V < 128) -> result0
           //      else -> +inf

           //__m128i comp = _mm_cmplt_epi32( _mm_castps_si128(Ve), g_XMBin128);
           MOVUPS  XMM2, XMM0
           PCMPGTD XMM2, [g_XMBin128]
           // __m128i select0 = _mm_and_si128(comp, _mm_castps_si128(result0));
           MOVUPS  XMM5, XMM2
           PAND    XMM5, XMM3
           // __m128i select1 = _mm_andnot_si128(comp, g_XMInfinity);
           PANDN   XMM2, [g_XMInfinity]
           //__m128i result2 = _mm_or_si128(select0, select1);
           POR     XMM5, XMM2  // XMM0 = Ve, XMM1 = itrunc, xmm3 = result0, xmm4 = result1, XMM5 = result2

           // comp = _mm_cmplt_epi32(itrunc, g_XMSubnormalExponent);
           PCMPGTD XMM1, [g_XMSubnormalExponent]
           MOVUPS  XMM2, XMM1
           // select1 = _mm_and_si128(comp, _mm_castps_si128(result1));
           PAND    XMM1, XMM4
           // select0 = _mm_andnot_si128(comp, _mm_castps_si128(result0));
           PANDN   XMM2, XMM3
           // __m128i result3 = _mm_or_si128(select0, select1);
           POR     XMM2, XMM1 // XMM0 = Ve, xmm2 = result3, XMM5 = result2

           // comp = _mm_cmplt_epi32(_mm_castps_si128(Ve), g_XMBinNeg150);
           MOVUPS  XMM1, XMM0
           PCMPGTD XMM1, [g_XMBinNeg150]
           // select0 = _mm_and_si128(comp, result3);
           PAND    XMM2, XMM1
           // select1 = _mm_andnot_si128(comp, g_XMZero);
           MOVUPS  XMM3, [g_XMZero]
           PANDN   XMM3, XMM1
           // __m128i result4 = _mm_or_si128(select0, select1);
           POR     XMM2, XMM3    // XMM0 = Ve, xmm2 = result4, XMM5 = result2

           // __m128i sign = _mm_and_si128(_mm_castps_si128(Ve), g_XMNegativeZero);
           MOVUPS  XMM1, XMM0
           PAND    XMM1, [g_XMNegativeZero]
           // comp = _mm_cmpeq_epi32(sign, g_XMNegativeZero);
           PCMPEQD XMM1, [g_XMNegativeZero]
           MOVUPS  XMM3, XMM1
           // select0 = _mm_and_si128(comp, result4);
           PAND    XMM1, XMM2
           // select1 = _mm_andnot_si128(comp, result2);
           PANDN   XMM3, XMM5
           // __m128i result5 = _mm_or_si128(select0, select1);
           POR     XMM1, XMM3 // XMM0 = Ve, XMM1 = result5

           // __m128i t0 = _mm_and_si128(_mm_castps_si128(Ve), g_XMQNaNTest);
           MOVUPS  XMM2, XMM0
           PAND    XMM2, [g_XMQNaNTest]
           // __m128i t1 = _mm_and_si128(_mm_castps_si128(Ve), g_XMInfinity);
           MOVUPS  XMM3, XMM0
           PAND    XMM3, [g_XMInfinity]
           // t0 = _mm_cmpeq_epi32(t0, g_XMZero);
           PCMPEQD XMM2, [g_XMZero]
           // t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
           PCMPEQD XMM3, [g_XMInfinity]
           // __m128i isNaN = _mm_andnot_si128(t0, t1);
           PANDN   XMM2, XMM3  // XMM1 = result5 ,xmm2 = isNaN

           // select0 = _mm_and_si128(isNaN, g_XMQNaN);
           MOVUPS  XMM0, XMM2
           PAND    XMM0, [g_XMQNaN]
           // select1 = _mm_andnot_si128(isNaN, result5);
           PANDN   XMM2, XMM1
           // __m128i vResult = _mm_or_si128(select0, select1);
           POR     XMM0, XMM2

           // return _mm_castsi128_ps(vResult);
           MOVUPS  [result],XMM0
end;
{$ENDIF}

function XMVectorExp(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVectorExp2(V);
end;


{$if defined(_XM_SSE_INTRINSICS_) and not DEFINED(_XM_NO_INTRINSICS_)}


function multi_sll_epi32(Value, Count: TXMVECTOR {__m128i}): TXMVECTOR {__m128i}; assembler;
asm

           // __m128i v = _mm_shuffle_epi32(value, _MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0, [value]
           PSHUFD  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // __m128i c = _mm_shuffle_epi32(count, _MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM1, [count]
           PSHUFD  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM1, [g_XMMaskX]
           // __m128i r0 = _mm_sll_epi32(v, c);
           PSLLD   XMM0, XMM1 // XMM0 = r0

           // v = _mm_shuffle_epi32(value, _MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1, [value]
           PSHUFD  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // c = _mm_shuffle_epi32(count, _MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM2, [count]
           PSHUFD  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM2, [g_XMMaskX]
           // __m128i r1 = _mm_sll_epi32(v, c);
           PSLLD   XMM1, XMM2 // XMM1 = r1

           // v = _mm_shuffle_epi32(value, _MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM2, [value]
           PSHUFD  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // c = _mm_shuffle_epi32(count, _MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM3, [count]
           PSHUFD  XMM3, XMM3, _MM_SHUFFLE_2_2_2_2
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM3, [g_XMMaskX]
           // __m128i r2 = _mm_sll_epi32(v, c);
           PSLLD   XMM2, XMM3 // XMM2 = r2

           // v = _mm_shuffle_epi32(value, _MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM3, [value]
           PSHUFD  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // c = _mm_shuffle_epi32(count, _MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM4, [count]
           PSHUFD  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM4, [g_XMMaskX]
           // __m128i r3 = _mm_sll_epi32(v, c);
           PSLLD   XMM3, XMM4 // XMM3 = r3

           // (r0,r0,r1,r1)
           // __m128 r01 = _mm_shuffle_ps(_mm_castsi128_ps(r0), _mm_castsi128_ps(r1), _MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_0_0_0_0
           // (r2,r2,r3,r3)
           // __m128 r23 = _mm_shuffle_ps(_mm_castsi128_ps(r2), _mm_castsi128_ps(r3), _MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM2, XMM3, _MM_SHUFFLE_0_0_0_0
           // (r0,r1,r2,r3)
           // __m128 result = _mm_shuffle_ps(r01, r23, _MM_SHUFFLE(2,0,2,0));
           SHUFPS  XMM0, XMM2, _MM_SHUFFLE_2_0_2_0
           // return _mm_castps_si128(result);
           MOVUPS  [result],XMM0
end;

function multi_srl_epi32(Value, Count: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128i v = _mm_shuffle_epi32(value, _MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0, [value]
           PSHUFD  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // __m128i c = _mm_shuffle_epi32(count, _MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM1, [count]
           PSHUFD  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM1, [g_XMMaskX]
           // __m128i r0 = _mm_srl_epi32(v, c);
           PSRLD   XMM0, XMM1

           // v = _mm_shuffle_epi32(value, _MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1, [value]
           PSHUFD  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // c = _mm_shuffle_epi32(count, _MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM2, [count]
           PSHUFD  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM2, [g_XMMaskX]
           // __m128i r1 = _mm_srl_epi32(v, c);
           PSRLD   XMM1, XMM2

           // v = _mm_shuffle_epi32(value, _MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM2, [value]
           PSHUFD  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // c = _mm_shuffle_epi32(count, _MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM3, [count]
           PSHUFD  XMM3, XMM3, _MM_SHUFFLE_2_2_2_2
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM3, [g_XMMaskX]
           // __m128i r2 = _mm_srl_epi32(v, c);
           PSRLD   XMM2, XMM3

           // v = _mm_shuffle_epi32(value, _MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM3, [value]
           PSHUFD  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // c = _mm_shuffle_epi32(count, _MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM4, [count]
           PSHUFD  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3
           // c = _mm_and_si128(c, g_XMMaskX);
           PAND    XMM4, [g_XMMaskX]
           // __m128i r3 = _mm_srl_epi32(v, c);
           PSRLD   XMM3, XMM4

           // (r0,r0,r1,r1)
           // __m128 r01 = _mm_shuffle_ps(_mm_castsi128_ps(r0), _mm_castsi128_ps(r1), _MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_0_0_0_0
           // (r2,r2,r3,r3)
           // __m128 r23 = _mm_shuffle_ps(_mm_castsi128_ps(r2), _mm_castsi128_ps(r3), _MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM2, XMM3, _MM_SHUFFLE_0_0_0_0
           // (r0,r1,r2,r3)
           // __m128 result = _mm_shuffle_ps(r01, r23, _MM_SHUFFLE(2,0,2,0));
           SHUFPS  XMM0, XMM2, _MM_SHUFFLE_2_0_2_0
           // return _mm_castps_si128(result);
           MOVUPS  [result],XMM0
end;

function GetLeadingBit(const Value: TXMVECTOR): TXMVECTOR; inline;
const
    g_XM0000FFFF: TXMVECTORI32 = (i: ($0000FFFF, $0000FFFF, $0000FFFF, $0000FFFF));
    g_XM000000FF: TXMVECTORI32 = (i: ($000000FF, $000000FF, $000000FF, $000000FF));
    g_XM0000000F: TXMVECTORI32 = (i: ($0000000F, $0000000F, $0000000F, $0000000F));
    g_XM00000003: TXMVECTORI32 = (i: ($00000003, $00000003, $00000003, $00000003));
var
    v, r, s: TXMVECTOR;
begin
    asm
               //__m128i v = value, r, c, b, s;

               // c = _mm_cmpgt_epi32(v, g_XM0000FFFF);   // c = (v > 0xFFFF)
               MOVUPS  XMM0, [value]
               MOVUPS  [v], XMM0
               PCMPGTD XMM0, [g_XM0000FFFF]
               // b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
               PSRLD   XMM0, 31
               // r = _mm_slli_epi32(b, 4);               // r = (b << 4)
               PSLLD   XMM0, 4
               MOVUPS  [r], XMM0
    end;
    // v = multi_srl_epi32(v, r);              // v = (v >> r)
    v := multi_srl_epi32(v, r);

    asm
               // c = _mm_cmpgt_epi32(v, g_XM000000FF);   // c = (v > 0xFF)
               MOVUPS  XMM0,[v]
               PCMPGTD XMM0, [g_XM000000FF]
               // b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
               PSRLD   XMM0, 31
               // s = _mm_slli_epi32(b, 3);               // s = (b << 3)
               PSLLD   XMM0, 3
               MOVUPS  [s],XMM0
    end;
    // v = multi_srl_epi32(v, s);              // v = (v >> s)
    v := multi_srl_epi32(v, s);
    asm
               // r = _mm_or_si128(r, s);                 // r = (r | s)
               MOVUPS  XMM0,[r]
               POR     XMM0, [s]
               MOVUPS  [r],XMM0

               // c = _mm_cmpgt_epi32(v, g_XM0000000F);   // c = (v > 0xF)
               MOVUPS  XMM2, [v]
               PCMPGTD XMM2, [g_XM0000000F]
               // b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
               PSRLD   XMM2, 31
               // s = _mm_slli_epi32(b, 2);               // s = (b << 2)
               PSLLD   XMM2, 2
               MOVUPS  [s],XMM2
    end;
    // v = multi_srl_epi32(v, s);              // v = (v >> s)
    v := multi_srl_epi32(v, s);
    asm
               // r = _mm_or_si128(r, s);                 // r = (r | s)
               MOVUPS  XMM0,[r]
               POR     XMM0, [s]
               MOVUPS  [r],XMM0

               // c = _mm_cmpgt_epi32(v, g_XM00000003);   // c = (v > 0x3)
               MOVUPS  XMM2,[v]
               PCMPGTD XMM2, [g_XM00000003]
               // b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
               PSRLD   XMM2, 31
               // s = _mm_slli_epi32(b, 1);               // s = (b << 1)
               PSLLD   XMM2, 1
               MOVUPS  [s],XMM2
    end;
    // v = multi_srl_epi32(v, s);              // v = (v >> s)
    v := multi_srl_epi32(v, s);
    asm
               // r = _mm_or_si128(r, s);                 // r = (r | s)
               MOVUPS  XMM0,[r]
               POR     XMM0, [s]

               // s = _mm_srli_epi32(v, 1);
               MOVUPS  XMM1,[v]
               PSRLD   XMM1, 1
               // r = _mm_or_si128(r, s);
               POR     XMM0, XMM1
               // return r;
               MOVUPS  [result],XMM0

    end;
end;


{$endif}// _XM_SSE_INTRINSICS_

{$if defined(_XM_ARM_NEON_INTRINSICS_)}
     (* ToDo
     function GetLeadingBit(const int32x4_t value):int32x4_t ;inline;
    begin
        static const XMVECTORI32 g_XM0000FFFF = { { { 0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF } } };
        static const XMVECTORI32 g_XM000000FF = { { { 0x000000FF, 0x000000FF, 0x000000FF, 0x000000FF } } };
        static const XMVECTORI32 g_XM0000000F = { { { 0x0000000F, 0x0000000F, 0x0000000F, 0x0000000F } } };
        static const XMVECTORI32 g_XM00000003 = { { { 0x00000003, 0x00000003, 0x00000003, 0x00000003 } } };

        int32x4_t v = value, r, c, b, s;

        c = vcgtq_s32(v, g_XM0000FFFF);     // c = (v > 0xFFFF)
        b = vshrq_n_u32(c, 31);             // b = (c ? 1 : 0)
        r = vshlq_n_s32(b, 4);              // r = (b << 4)
        r = vnegq_s32( r );
        v = vshlq_u32( v, r );              // v = (v >> r)

        c = vcgtq_s32(v, g_XM000000FF);     // c = (v > 0xFF)
        b = vshrq_n_u32(c, 31);             // b = (c ? 1 : 0)
        s = vshlq_n_s32(b, 3);              // s = (b << 3)
        s = vnegq_s32( s );
        v = vshlq_u32(v, s);                // v = (v >> s)
        r = vorrq_s32(r, s);                // r = (r | s)

        c = vcgtq_s32(v, g_XM0000000F);     // c = (v > 0xF)
        b = vshrq_n_u32(c, 31);             // b = (c ? 1 : 0)
        s = vshlq_n_s32(b, 2);              // s = (b << 2)
        s = vnegq_s32( s );
        v = vshlq_u32(v, s);                // v = (v >> s)
        r = vorrq_s32(r, s);                // r = (r | s)

        c = vcgtq_s32(v, g_XM00000003);     // c = (v > 0x3)
        b = vshrq_n_u32(c, 31);             // b = (c ? 1 : 0)
        s = vshlq_n_s32(b, 1);              // s = (b << 1)
        s = vnegq_s32( s );
        v = vshlq_u32(v, s);                // v = (v >> s)
        r = vorrq_s32(r, s);                // r = (r | s)

        s = vshrq_n_u32(v, 1);
        r = vorrq_s32(r, s);
        return r;
    end;
    *)

{$endif}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Computes the base two logarithm of each component of a vector.
function XMVectorLog2(constref V: TXMVECTOR): TXMVECTOR;
const
    fScale: single = 1.4426950; // (1.0  / logf(2.0 ));
begin
    Result.f32[0] := ln(V.f32[0]) * fScale;
    Result.f32[0] := ln(V.f32[1]) * fScale;
    Result.f32[0] := ln(V.f32[2]) * fScale;
    Result.f32[0] := ln(V.f32[3]) * fScale;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorLog2(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    int32x4_t rawBiased = vandq_s32(V, g_XMInfinity);
    int32x4_t trailing = vandq_s32(V, g_XMQNaNTest);
    int32x4_t isExponentZero = vceqq_s32(g_XMZero, rawBiased);

    // Compute exponent and significand for normals.
    int32x4_t biased = vshrq_n_u32(rawBiased, 23);
    int32x4_t exponentNor = vsubq_s32(biased, g_XMExponentBias);
    int32x4_t trailingNor = trailing;

    // Compute exponent and significand for subnormals.
    int32x4_t leading = Internal::GetLeadingBit(trailing);
    int32x4_t shift = vsubq_s32(g_XMNumTrailing, leading);
    int32x4_t exponentSub = vsubq_s32(g_XMSubnormalExponent, shift);
    int32x4_t trailingSub = vshlq_u32(trailing, shift);
    trailingSub = vandq_s32(trailingSub, g_XMQNaNTest);
    int32x4_t e = vbslq_f32( isExponentZero, exponentSub, exponentNor );
    int32x4_t t = vbslq_f32( isExponentZero, trailingSub, trailingNor );

    // Compute the approximation.
    int32x4_t tmp = vorrq_s32(g_XMOne, t);
    float32x4_t y = vsubq_f32(tmp, g_XMOne);

    float32x4_t log2 = vmlaq_f32( g_XMLogEst6, g_XMLogEst7, y );
    log2 = vmlaq_f32( g_XMLogEst5, log2, y );
    log2 = vmlaq_f32( g_XMLogEst4, log2, y );
    log2 = vmlaq_f32( g_XMLogEst3, log2, y );
    log2 = vmlaq_f32( g_XMLogEst2, log2, y );
    log2 = vmlaq_f32( g_XMLogEst1, log2, y );
    log2 = vmlaq_f32( g_XMLogEst0, log2, y );
    log2 = vmlaq_f32( vcvtq_f32_s32(e), log2, y );

    //  if (x is NaN) -> QNaN
    //  else if (V is positive)
    //      if (V is infinite) -> +inf
    //      else -> log2(V)
    //  else
    //      if (V is zero) -> -inf
    //      else -> -QNaN

    int32x4_t isInfinite = vandq_s32((V), g_XMAbsMask);
    isInfinite = vceqq_s32(isInfinite, g_XMInfinity);

    int32x4_t isGreaterZero = vcgtq_s32((V), g_XMZero);
    int32x4_t isNotFinite = vcgtq_s32((V), g_XMInfinity);
    int32x4_t isPositive = vbicq_s32( isGreaterZero,isNotFinite);

    int32x4_t isZero = vandq_s32((V), g_XMAbsMask);
    isZero = vceqq_s32(isZero, g_XMZero);

    int32x4_t t0 = vandq_s32((V), g_XMQNaNTest);
    int32x4_t t1 = vandq_s32((V), g_XMInfinity);
    t0 = vceqq_s32(t0, g_XMZero);
    t1 = vceqq_s32(t1, g_XMInfinity);
    int32x4_t isNaN = vbicq_s32( t1,t0);

    float32x4_t result = vbslq_f32( isInfinite, g_XMInfinity, log2 );
    tmp = vbslq_f32( isZero, g_XMNegInfinity, g_XMNegQNaN );
    result = vbslq_f32(isPositive, result, tmp);
    result = vbslq_f32(isNaN, g_XMQNaN, result );
    return result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorLog2(constref V: TXMVECTOR): TXMVECTOR; inline;
var
    trailing, trailingNor, isExponentZero, exponentNor, leading: TXMVECTOR;
    shift, trailingSub, exponentSub: TXMVECTOR;
begin
    asm
               // __m128i rawBiased = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
               MOVUPS  XMM0,[V]
               MOVUPS  XMM1,XMM0
               PAND    XMM0, [g_XMInfinity] // XMM0 = rawBiased
               // __m128i trailing = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
               PAND    XMM1, [g_XMQNaNTest] // XMM1 = trailing
               MOVUPS  [trailing],XMM1
               MOVUPS  [trailingNor],XMM1
               //__m128i isExponentZero = _mm_cmpeq_epi32(g_XMZero, rawBiased);
               MOVUPS  XMM2, [g_XMZero]
               PCMPEQD XMM2, XMM0  // XMM2 = isExponentZero
               MOVUPS  [isExponentZero],XMM2
               // Compute exponent and significand for normals.
               //__m128i biased = _mm_srli_epi32(rawBiased, 23);
               MOVUPS  XMM3,XMM0
               PSRLD   XMM3, 23
               //__m128i exponentNor = _mm_sub_epi32(biased, g_XMExponentBias);
               PSUBD   XMM3, [g_XMExponentBias]
               MOVUPS  [exponentNor],XMM3
               //__m128i trailingNor = trailing;
    end;
    // Compute exponent and significand for subnormals.
    leading := GetLeadingBit(trailing);
    asm
               //__m128i shift = _mm_sub_epi32(g_XMNumTrailing, leading);
               MOVUPS  XMM0, [g_XMNumTrailing]
               PSUBD   XMM0, [leading]
               // __m128i exponentSub = _mm_sub_epi32(g_XMSubnormalExponent, shift);
               MOVUPS  XMM1, [g_XMSubnormalExponent]
               PSUBD   XMM1, XMM0
               MOVUPS  [shift],XMM0
               MOVUPS  [exponentSub],XMM1
    end;
    trailingSub := multi_sll_epi32(trailing, shift);
    asm

               // trailingSub = _mm_and_si128(trailingSub, g_XMQNaNTest);
               MOVUPS  XMM0, [trailingSub]
               PAND    XMM0, [g_XMQNaNTest] // trailingSub = XMM0

               // __m128i select0 = _mm_and_si128(isExponentZero, exponentSub);
               MOVUPS  XMM1, [isExponentZero]
               PAND    XMM1, [exponentSub]
               // __m128i select1 = _mm_andnot_si128(isExponentZero, exponentNor);
               MOVUPS  XMM2, [isExponentZero]
               PANDN   XMM2, [exponentNor]
               //__m128i e = _mm_or_si128(select0, select1);
               POR     XMM1, XMM2 // XMM1 = e

               // select0 = _mm_and_si128(isExponentZero, trailingSub);
               MOVUPS  XMM2,[isExponentZero]
               PAND    XMM2, [trailingSub]
               // select1 = _mm_andnot_si128(isExponentZero, trailingNor);
               MOVUPS  XMM3,[isExponentZero]
               PANDN   XMM3, [trailingNor]
               // __m128i t = _mm_or_si128(select0, select1);
               POR     XMM2, XMM3 // XMM2 = t

               // Compute the approximation.
               //__m128i tmp = _mm_or_si128(g_XMOne, t);

               POR     XMM2, [g_XMOne] // XMM2 = tmp
               // __m128 y = _mm_sub_ps(_mm_castsi128_ps(tmp), g_XMOne);
               SUBPS   XMM2, [g_XMOne] // XMM2 = y
               // __m128 log2 = _mm_mul_ps(g_XMLogEst7, y);
               MOVUPS  XMM3, [g_XMLogEst7]
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(g_XMLogEst6, log2);
               ADDPS   XMM3, [g_XMLogEst6]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(g_XMLogEst5, log2);
               ADDPS   XMM3, [g_XMLogEst5]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(g_XMLogEst4, log2);
               ADDPS   XMM3, [g_XMLogEst4]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(g_XMLogEst3, log2);
               ADDPS   XMM3, [g_XMLogEst3]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(g_XMLogEst2, log2);
               ADDPS   XMM3, [g_XMLogEst2]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(g_XMLogEst1, log2);
               ADDPS   XMM3, [g_XMLogEst1]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(g_XMLogEst0, log2);
               ADDPS   XMM3, [g_XMLogEst0]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM2
               // log2 = _mm_add_ps(log2, _mm_cvtepi32_ps(e));
               CVTDQ2PS XMM1, XMM1
               ADDPS   XMM3, XMM1 // XMM3 = log2

               //  if (x is NaN) -> QNaN
               //  else if (V is positive)
               //      if (V is infinite) -> +inf
               //      else -> log2(V)
               //  else
               //      if (V is zero) -> -inf
               //      else -> -QNaN

               // __m128i isInfinite = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
               MOVUPS  XMM0, [V]
               PAND    XMM0, [g_XMAbsMask]
               // isInfinite = _mm_cmpeq_epi32(isInfinite, g_XMInfinity);
               PCMPEQD XMM0, [g_XMInfinity] // XMM0 = isInfinite

               // __m128i isGreaterZero = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMZero);
               MOVUPS  XMM1,[V]
               PCMPGTD XMM1, [g_XMZero]
               // __m128i isNotFinite = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMInfinity);
               MOVUPS  XMM2,[V]
               PCMPGTD XMM2, [g_XMInfinity]
               // __m128i isPositive = _mm_andnot_si128(isNotFinite, isGreaterZero);
               PANDN   XMM2, XMM1 // XMM2 = isPositive

               // __m128i isZero = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
               MOVUPS  XMM1, [V]
               PAND    XMM1, [g_XMAbsMask]
               // isZero = _mm_cmpeq_epi32(isZero, g_XMZero);
               PCMPEQD XMM1, [g_XMZero] // XMM1 = isZero

               // __m128i t0 = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
               MOVUPS  XMM4, [V]
               PAND    XMM4, [g_XMQNaNTest]
               // __m128i t1 = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
               MOVUPS  XMM5, [V]
               PAND    XMM5, [g_XMInfinity]
               // t0 = _mm_cmpeq_epi32(t0, g_XMZero);
               PCMPEQD XMM4, [g_XMZero]
               // t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
               PCMPEQD XMM5, [g_XMInfinity]
               // __m128i isNaN = _mm_andnot_si128(t0, t1);
               PANDN   XMM4, XMM5 // XMM4 = isNaN

               // select0 = _mm_and_si128(isInfinite, g_XMInfinity);
               MOVUPS  XMM5, XMM0
               PAND    XMM5, [g_XMInfinity]
               // select1 = _mm_andnot_si128(isInfinite, _mm_castps_si128(log2));
               PANDN   XMM0, XMM3
               // __m128i result = _mm_or_si128(select0, select1);
               POR     XMM0, XMM5 // XMM0 = result

               // select0 = _mm_and_si128(isZero, g_XMNegInfinity);
               MOVUPS  XMM5,XMM1
               PAND    XMM5, [g_XMNegInfinity]
               // select1 = _mm_andnot_si128(isZero, g_XMNegQNaN);
               PANDN   XMM1, [g_XMNegQNaN]
               // tmp = _mm_or_si128(select0, select1);
               POR     XMM1, XMM5 // XMM1 = tmp

               // select0 = _mm_and_si128(isPositive, result);
               PAND    XMM0, XMM2
               // select1 = _mm_andnot_si128(isPositive, tmp);
               PANDN   XMM2, XMM1
               // result = _mm_or_si128(select0, select1);
               POR     XMM0, XMM2

               // select0 = _mm_and_si128(isNaN, g_XMQNaN);
               MOVUPS  XMM2, XMM4
               PAND    XMM2, [g_XMQNaN]
               // select1 = _mm_andnot_si128(isNaN, result);
               PANDN   XMM0, XMM4
               // result = _mm_or_si128(select0, select1);
               POR     XMM0, XMM2

               // return _mm_castsi128_ps(result);
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorLogE(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := ln(V.f32[0]);
    Result.f32[1] := ln(V.f32[1]);
    Result.f32[2] := ln(V.f32[2]);
    Result.f32[3] := ln(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorLogE(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    int32x4_t rawBiased = vandq_s32(V, g_XMInfinity);
    int32x4_t trailing = vandq_s32(V, g_XMQNaNTest);
    int32x4_t isExponentZero = vceqq_s32(g_XMZero, rawBiased);

    // Compute exponent and significand for normals.
    int32x4_t biased = vshrq_n_u32(rawBiased, 23);
    int32x4_t exponentNor = vsubq_s32(biased, g_XMExponentBias);
    int32x4_t trailingNor = trailing;

    // Compute exponent and significand for subnormals.
    int32x4_t leading = Internal::GetLeadingBit(trailing);
    int32x4_t shift = vsubq_s32(g_XMNumTrailing, leading);
    int32x4_t exponentSub = vsubq_s32(g_XMSubnormalExponent, shift);
    int32x4_t trailingSub = vshlq_u32(trailing, shift);
    trailingSub = vandq_s32(trailingSub, g_XMQNaNTest);
    int32x4_t e = vbslq_f32( isExponentZero, exponentSub, exponentNor );
    int32x4_t t = vbslq_f32( isExponentZero, trailingSub, trailingNor );

    // Compute the approximation.
    int32x4_t tmp = vorrq_s32(g_XMOne, t);
    float32x4_t y = vsubq_f32(tmp, g_XMOne);

    float32x4_t log2 = vmlaq_f32( g_XMLogEst6, g_XMLogEst7, y );
    log2 = vmlaq_f32( g_XMLogEst5, log2, y );
    log2 = vmlaq_f32( g_XMLogEst4, log2, y );
    log2 = vmlaq_f32( g_XMLogEst3, log2, y );
    log2 = vmlaq_f32( g_XMLogEst2, log2, y );
    log2 = vmlaq_f32( g_XMLogEst1, log2, y );
    log2 = vmlaq_f32( g_XMLogEst0, log2, y );
    log2 = vmlaq_f32( vcvtq_f32_s32(e), log2, y );

    log2 = vmulq_f32(g_XMInvLgE, log2);

    //  if (x is NaN) -> QNaN
    //  else if (V is positive)
    //      if (V is infinite) -> +inf
    //      else -> log2(V)
    //  else
    //      if (V is zero) -> -inf
    //      else -> -QNaN

    int32x4_t isInfinite = vandq_s32((V), g_XMAbsMask);
    isInfinite = vceqq_s32(isInfinite, g_XMInfinity);

    int32x4_t isGreaterZero = vcgtq_s32((V), g_XMZero);
    int32x4_t isNotFinite = vcgtq_s32((V), g_XMInfinity);
    int32x4_t isPositive = vbicq_s32( isGreaterZero,isNotFinite);

    int32x4_t isZero = vandq_s32((V), g_XMAbsMask);
    isZero = vceqq_s32(isZero, g_XMZero);

    int32x4_t t0 = vandq_s32((V), g_XMQNaNTest);
    int32x4_t t1 = vandq_s32((V), g_XMInfinity);
    t0 = vceqq_s32(t0, g_XMZero);
    t1 = vceqq_s32(t1, g_XMInfinity);
    int32x4_t isNaN = vbicq_s32( t1,t0);

    float32x4_t result = vbslq_f32( isInfinite, g_XMInfinity, log2 );
    tmp = vbslq_f32( isZero, g_XMNegInfinity, g_XMNegQNaN );
    result = vbslq_f32(isPositive, result, tmp);
    result = vbslq_f32(isNaN, g_XMQNaN, result );
    return result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorLogE(constref V: TXMVECTOR): TXMVECTOR; inline;
var
    leading, trailing, isExponentZero, shift, exponentSub, trailingSub, trailingNor, exponentNor: TXMVECTOR;
begin
    asm
               // rawBiased = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
               MOVUPS  XMM0,[V]
               MOVUPS  XMM1,XMM0
               PAND    XMM0, [g_XMInfinity] // XMM0 = rawBiased
               // trailing = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
               PAND    XMM1, [g_XMQNaNTest] // XMM1  = trailing
               MOVUPS  [trailing],XMM1

               // isExponentZero = _mm_cmpeq_epi32(g_XMZero, rawBiased);
               MOVUPS  XMM2, [g_XMZero]
               PCMPEQD XMM2, XMM0 // XMM2 = isExponentZero
               MOVUPS  [isExponentZero],XMM2

               // Compute exponent and significand for normals.
               // biased = _mm_srli_epi32(rawBiased, 23);
               PSRLD   XMM0, 23
               // exponentNor = _mm_sub_epi32(biased, g_XMExponentBias);
               PSUBD   XMM0, [g_XMExponentBias]
               MOVUPS  [exponentNor],XMM0
               // trailingNor = trailing;
               MOVUPS  [trailingNor],XMM1
    end;
    // Compute exponent and significand for subnormals.
    leading := GetLeadingBit(trailing);
    asm
               // shift = _mm_sub_epi32(g_XMNumTrailing, leading);
               MOVUPS  XMM0, [g_XMNumTrailing]
               PSUBD   XMM0, [leading]
               MOVUPS  [shift],XMM0
               // exponentSub = _mm_sub_epi32(g_XMSubnormalExponent, shift);
               MOVUPS  XMM1,[g_XMSubnormalExponent]
               PSUBD   XMM1, XMM0
               MOVUPS  [exponentSub],XMM1
    end;
    trailingSub := multi_sll_epi32(trailing, shift);
    asm

               //    trailingSub = _mm_and_si128(trailingSub, g_XMQNaNTest);
               MOVUPS  XMM0, [trailingSub]
               PAND    XMM0, [g_XMQNaNTest]

               // select0 = _mm_and_si128(isExponentZero, exponentSub);
               MOVUPS  XMM1, [isExponentZero]
               PAND    XMM1, [exponentSub]
               // select1 = _mm_andnot_si128(isExponentZero, exponentNor);
               MOVUPS  XMM2, [isExponentZero]
               PANDN   XMM2, [exponentNor]
               // e = _mm_or_si128(select0, select1);
               POR     XMM1, XMM2 // XMM1 = e

               // select0 = _mm_and_si128(isExponentZero, trailingSub);
               MOVUPS  XMM2, [isExponentZero]
               PAND    XMM2, XMM0
               // select1 = _mm_andnot_si128(isExponentZero, trailingNor);
               MOVUPS  XMM3, [isExponentZero]
               PANDN   XMM3, [trailingNor]
               // t = _mm_or_si128(select0, select1);
               POR     XMM2, XMM3  // XMM2 = t
               // Compute the approximation.
               // tmp = _mm_or_si128(g_XMOne, t);
               MOVUPS  XMM0, [g_XMOne]
               POR     XMM0, XMM2 // XMM0 = tmp
               // y = _mm_sub_ps(_mm_castsi128_ps(tmp), g_XMOne);
               SUBPS   XMM0, [g_XMOne] // XMM0 = y

               // log2 = _mm_mul_ps(g_XMLogEst7, y);
               MOVUPS  XMM3, [g_XMLogEst7]
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(g_XMLogEst6, log2);
               ADDPS   XMM3, [g_XMLogEst6]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(g_XMLogEst5, log2);
               ADDPS   XMM3, [g_XMLogEst5]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(g_XMLogEst4, log2);
               ADDPS   XMM3, [g_XMLogEst4]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(g_XMLogEst3, log2);
               ADDPS   XMM3, [g_XMLogEst3]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(g_XMLogEst2, log2);
               ADDPS   XMM3, [g_XMLogEst2]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(g_XMLogEst1, log2);
               ADDPS   XMM3, [g_XMLogEst1]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(g_XMLogEst0, log2);
               ADDPS   XMM3, [g_XMLogEst0]
               // log2 = _mm_mul_ps(log2, y);
               MULPS   XMM3, XMM0
               // log2 = _mm_add_ps(log2, _mm_cvtepi32_ps(e));
               CVTDQ2PS XMM1, XMM1
               ADDPS   XMM3, XMM1

               // log2 = _mm_mul_ps(g_XMInvLgE, log2);
               MULPS   XMM3, [g_XMInvLgE]  // XMM3 = log2

               //  if (x is NaN) -> QNaN
               //  else if (V is positive)
               //      if (V is infinite) -> +inf
               //      else -> log2(V)
               //  else
               //      if (V is zero) -> -inf
               //      else -> -QNaN

               // isInfinite = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
               MOVUPS  XMM0,[V]
               PAND    XMM0, [g_XMAbsMask]
               // isInfinite = _mm_cmpeq_epi32(isInfinite, g_XMInfinity);
               PCMPEQD XMM0, [g_XMInfinity]  // XMM0 = isInfinite

               //  isGreaterZero = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMZero);
               MOVUPS  XMM1,[V]
               PCMPGTD XMM1, [g_XMZero]
               //  isNotFinite = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMInfinity);
               MOVUPS  XMM2,[V]
               PCMPGTD XMM2, [g_XMInfinity]
               //  isPositive = _mm_andnot_si128(isNotFinite, isGreaterZero);
               PANDN   XMM2, XMM1   // XMM2 = isPositive

               //  isZero = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
               MOVUPS  XMM1, [V]
               PAND    XMM1, [g_XMAbsMask]
               // isZero = _mm_cmpeq_epi32(isZero, g_XMZero);
               PCMPEQD XMM1, [g_XMZero] // XMM1 = isZero

               //  t0 = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
               MOVUPS  XMM4, [V]
               PAND    XMM4, [g_XMQNaNTest]
               //  t1 = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
               MOVUPS  XMM5, [V]
               PAND    XMM5, [g_XMInfinity]
               // t0 = _mm_cmpeq_epi32(t0, g_XMZero);
               PCMPEQD XMM4, [g_XMZero]
               // t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
               PCMPEQD XMM5, [g_XMInfinity]
               //  isNaN = _mm_andnot_si128(t0, t1);
               PANDN   XMM4, XMM5 // XMM4 = isNaN

               // select0 = _mm_and_si128(isInfinite, g_XMInfinity);
               MOVUPS  XMM5, [g_XMInfinity]
               PAND    XMM5, XMM0
               // select1 = _mm_andnot_si128(isInfinite, _mm_castps_si128(log2));
               PANDN   XMM3, XMM0
               //  result = _mm_or_si128(select0, select1);
               POR     XMM5, XMM3 // XMM5 = result

               // select0 = _mm_and_si128(isZero, g_XMNegInfinity);
               MOVUPS  XMM6, [g_XMNegInfinity]
               PAND    XMM6, XMM1
               // select1 = _mm_andnot_si128(isZero, g_XMNegQNaN);
               PANDN   XMM1, [g_XMNegQNaN]
               // tmp = _mm_or_si128(select0, select1);
               POR     XMM6, XMM1 // XMM6 = tmp

               // select0 = _mm_and_si128(isPositive, result);
               PAND    XMM5, XMM2
               // select1 = _mm_andnot_si128(isPositive, tmp);
               PANDN   XMM2, XMM6
               // result = _mm_or_si128(select0, select1);
               POR     XMM5, XMM2

               // select0 = _mm_and_si128(isNaN, g_XMQNaN);
               MOVUPS  XMM6, [g_XMQNaN]
               PAND    XMM6, XMM4
               // select1 = _mm_andnot_si128(isNaN, result);
               PANDN   XMM4, XMM5
               // result = _mm_or_si128(select0, select1);
               POR     XMM6, XMM4

               // return _mm_castsi128_ps(result);
               MOVUPS  [result],XMM6
    end;
end;

{$ENDIF}


function XMVectorLog(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVectorLog2(V);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorPow(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := power(V1.f32[0], V2.f32[0]);
    Result.f32[1] := power(V1.f32[1], V2.f32[1]);
    Result.f32[2] := power(V1.f32[2], V2.f32[2]);
    Result.f32[3] := power(V1.f32[3], V2.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorPow(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    XMVECTORF32 vResult = { { {
            powf(vgetq_lane_f32(V1, 0), vgetq_lane_f32(V2, 0)),
            powf(vgetq_lane_f32(V1, 1), vgetq_lane_f32(V2, 1)),
            powf(vgetq_lane_f32(V1, 2), vgetq_lane_f32(V2, 2)),
            powf(vgetq_lane_f32(V1, 3), vgetq_lane_f32(V2, 3))
        } } };
    return vResult.v;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorPow(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; inline;
begin
    Result.f32[0] := power(V1.f32[0], V2.f32[0]);
    Result.f32[1] := power(V1.f32[1], V2.f32[1]);
    Result.f32[2] := power(V1.f32[2], V2.f32[2]);
    Result.f32[3] := power(V1.f32[3], V2.f32[3]);
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorAbs(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := abs(V.f32[0]);
    Result.f32[1] := abs(V.f32[1]);
    Result.f32[2] := abs(V.f32[2]);
    Result.f32[3] := abs(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorAbs(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   return vabsq_f32( V );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorAbs(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vResult = _mm_setzero_ps();
           XORPS   XMM0,XMM0
           // vResult = _mm_sub_ps(vResult,V);
           SUBPS   XMM0,[V]
           // vResult = _mm_max_ps(vResult,V);
           MAXPS   XMM0, [V]
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// V1 % V2  :=  V1 - V2 * truncate(V1 / V2)
function XMVectorMod(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    Quotient: TXMVECTOR;
begin
    Quotient := XMVectorDivide(V1, V2);
    Quotient := XMVectorTruncate(Quotient);
    Result := XMVectorNegativeMultiplySubtract(V2, Quotient, V1);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorMod(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     XMVECTOR vResult = XMVectorDivide(V1, V2);
    vResult = XMVectorTruncate(vResult);
    return vmlsq_f32( V1, vResult, V2 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorMod(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vResult = _mm_div_ps(V1, V2);
           MOVUPS  XMM0,[V1]
           DIVPS   XMM0, [V2] // vResult = XMM0
           // vResult := XMVectorTruncate(vResult);
           // ToDo: check this code
           MOVUPS  XMM4, XMM0
           PAND    XMM4, [g_XMAbsMask]
           PCMPGTD XMM4, [g_XMNoFraction]
           CVTTPS2DQ XMM5, XMM0
           CVTDQ2PS XMM5, XMM5
           ANDPS   XMM5,XMM4
           PANDN   XMM4, XMM0
           ORPS    XMM5, XMM4 // vResult = XMM5

           // vResult = _mm_mul_ps(vResult,V2);
           MOVUPS  XMM1,XMM5
           MULPS   XMM1,[V2]
           // vResult = _mm_sub_ps(V1,vResult);
           MOVUPS  XMM0,[V1]
           SUBPS   XMM0, XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorModAngles(constref Angles: TXMVECTOR): TXMVECTOR;
var
    V: TXMVECTOR;
begin
    // Modulo the range of the given angles such that -XM_PI <= Angles < XM_PI
    V := XMVectorMultiply(Angles, g_XMReciprocalTwoPi.v);
    V := XMVectorRound(V);
    Result := XMVectorNegativeMultiplySubtract(g_XMTwoPi.v, V, Angles);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorModAngles(constref Angles: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Modulo the range of the given angles such that -XM_PI <= Angles < XM_PI
    XMVECTOR vResult = vmulq_f32(Angles,g_XMReciprocalTwoPi);
    // Use the inline function due to complexity for rounding
    vResult = XMVectorRound(vResult);
    return vmlsq_f32( Angles, vResult, g_XMTwoPi );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorModAngles(constref Angles: TXMVECTOR): TXMVECTOR; inline;
var
    vResult: TXMVECTOR;
begin
    asm
               // Modulo the range of the given angles such that -XM_PI <= Angles < XM_PI
               // vResult = _mm_mul_ps(Angles,g_XMReciprocalTwoPi);
               MOVUPS  XMM0,[Angles]
               MULPS   XMM0, g_XMReciprocalTwoPi
               MOVUPS  [vResult],XMM0
    end;

    // Use the inline function due to complexity for rounding
    vResult := XMVectorRound(vResult);
    asm
               // vResult = _mm_mul_ps(vResult,g_XMTwoPi);
               MOVUPS  XMM1,[vResult]
               MULPS   XMM1, [g_XMTwoPi]
               //vResult = _mm_sub_ps(Angles,vResult);
               MOVUPS  XMM0, [Angles]
               SUBPS   XMM0,XMM1
               // return vResult;
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

// 11-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSin(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := sin(V.f32[0]);
    Result.f32[1] := sin(V.f32[1]);
    Result.f32[2] := sin(V.f32[2]);
    Result.f32[3] := sin(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSin(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with sin(y) = sin(x).
    uint32x4_t sign = vandq_u32(x, g_XMNegativeZero);
    uint32x4_t c = vorrq_u32(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    float32x4_t absx = vabsq_f32( x );
    float32x4_t rflx = vsubq_f32(c, x);
    uint32x4_t comp = vcleq_f32(absx, g_XMHalfPi);
    x = vbslq_f32( comp, x, rflx );

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation
    const XMVECTOR SC1 = g_XMSinCoefficients1;
    const XMVECTOR SC0 = g_XMSinCoefficients0;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(SC0), 1);
    XMVECTOR Result = vmlaq_lane_f32(vConstants, x2, vget_low_f32(SC1), 0);

    vConstants = vdupq_lane_f32(vget_high_f32(SC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(SC0), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(SC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    Result = vmulq_f32(Result, x);
    return Result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSin(constref V: TXMVECTOR): TXMVECTOR; inline;
var
    x: TXMVECTOR;
begin
    // Force the value within the bounds of pi
    x := XMVectorModAngles(V);
    asm
               // Map in [-pi/2,pi/2] with sin(y) = sin(x).
               // sign = _mm_and_ps(x, g_XMNegativeZero);
               MOVUPS  XMM0,[x]
               ANDPS   XMM0, [g_XMNegativeZero]
               // c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
               MOVUPS  XMM1,[g_XMPi]
               ORPS    XMM1, XMM0
               //  absx = _mm_andnot_ps(sign, x);  // |x|
               ANDNPS  XMM0, [x]
               // rflx = _mm_sub_ps(c, x);
               SUBPS   XMM1, [x]
               // comp = _mm_cmple_ps(absx, g_XMHalfPi);
               CMPPS   XMM0, [g_XMHalfPi], 2
               // __m128 select1 = _mm_andnot_ps(comp, rflx);
               ANDNPS  XMM1, XMM0
               // __m128 select0 = _mm_and_ps(comp, x);
               ANDPS   XMM0, [x]
               // x = _mm_or_ps(select0, select1);
               ORPS    XMM0, XMM1
               MOVUPS  [x],XMM0
               //  x2 = _mm_mul_ps(x, x);
               MULPS   XMM0, XMM0

               // Compute polynomial approximation
               // SC1 = g_XMSinCoefficients1;
               MOVUPS  XMM7, [g_XMSinCoefficients1]
               // vConstants = XM_PERMUTE_PS( SC1, _MM_SHUFFLE(0, 0, 0, 0) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_0_0_0_0
               // Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0

               // SC0 = g_XMSinCoefficients0;
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               // vConstants = XM_PERMUTE_PS( SC0, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_3_3_3_3
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7,XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SC0, _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6

               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SC0,  _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6

               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SC0, _MM_SHUFFLE(0, 0, 0, 0) );
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_0_0_0_0
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0
               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM7, [g_XMOne]
               //  Result = _mm_mul_ps(Result, x);
               MULPS   XMM7, [x]
               // return Result;
               MOVUPS  [result],XMM7
    end;
end;

{$ENDIF}



// 7-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSinEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := sin(V.f32[0]);
    Result.f32[1] := sin(V.f32[1]);
    Result.f32[2] := sin(V.f32[2]);
    Result.f32[3] := sin(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSinEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with sin(y) = sin(x).
    uint32x4_t sign = vandq_u32(x, g_XMNegativeZero);
    uint32x4_t c = vorrq_u32(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    float32x4_t absx = vabsq_f32( x );
    float32x4_t rflx = vsubq_f32(c, x);
    uint32x4_t comp = vcleq_f32(absx, g_XMHalfPi);
    x = vbslq_f32( comp, x, rflx );

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation
    const XMVECTOR SEC = g_XMSinCoefficients1;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(SEC), 0);
    XMVECTOR Result = vmlaq_lane_f32(vConstants, x2, vget_high_f32(SEC), 1);

    vConstants = vdupq_lane_f32(vget_low_f32(SEC), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    Result = vmulq_f32(Result, x);
    return Result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSinEst(constref V: TXMVECTOR): TXMVECTOR; inline;
var
    x: TXMVECTOR;
begin
    // Force the value within the bounds of pi
    x := XMVectorModAngles(V);
    asm
               // Map in [-pi/2,pi/2] with sin(y) = sin(x).
               // sign = _mm_and_ps(x, g_XMNegativeZero);
               MOVUPS  XMM0,[x]
               ANDPS   XMM0, [g_XMNegativeZero]
               //  c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
               MOVUPS  XMM1,XMM0
               ORPS    XMM1, [g_XMPi]
               //  absx = _mm_andnot_ps(sign, x);  // |x|
               ANDNPS  XMM0, [x]
               //  rflx = _mm_sub_ps(c, x);
               SUBPS   XMM1, [x]
               //  comp = _mm_cmple_ps(absx, g_XMHalfPi);
               CMPPS   XMM0, [g_XMHalfPi], 2
               //  select1 = _mm_andnot_ps(comp, rflx);
               ANDNPS  XMM1, XMM0
               //  select0 = _mm_and_ps(comp, x);
               ANDPS   XMM0, [x]

               // x = _mm_or_ps(select0, select1);
               ORPS    XMM0, XMM1
               MOVUPS  [x],XMM0
               //  x2 = _mm_mul_ps(x, x);
               MULPS   XMM0, XMM0

               // Compute polynomial approximation
               //  SEC = g_XMSinCoefficients1;
               MOVUPS  XMM7,[g_XMSinCoefficients1]
               //  vConstants = XM_PERMUTE_PS( SEC, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_3_3_3_3
               //  Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SEC, _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6,[g_XMSinCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SEC, _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6,[g_XMSinCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM7, [g_XMOne]
               // Result = _mm_mul_ps(Result, x);
               MULPS   XMM7, [x]
               // return Result;
               MOVUPS  [result],XMM7
    end;
end;

{$ENDIF}

// 10-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorCos(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := cos(V.f32[0]);
    Result.f32[1] := cos(V.f32[1]);
    Result.f32[2] := cos(V.f32[2]);
    Result.f32[3] := cos(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorCos(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Map V to x in [-pi,pi].
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
    uint32x4_t sign = vandq_u32(x, g_XMNegativeZero);
    uint32x4_t c = vorrq_u32(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    float32x4_t absx = vabsq_f32( x );
    float32x4_t rflx = vsubq_f32(c, x);
    uint32x4_t comp = vcleq_f32(absx, g_XMHalfPi);
    x = vbslq_f32( comp, x, rflx );
    sign = vbslq_f32( comp, g_XMOne, g_XMNegativeOne );

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation
    const XMVECTOR CC1 = g_XMCosCoefficients1;
    const XMVECTOR CC0 = g_XMCosCoefficients0;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(CC0), 1);
    XMVECTOR Result = vmlaq_lane_f32(vConstants, x2, vget_low_f32(CC1), 0 );

    vConstants = vdupq_lane_f32(vget_high_f32(CC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(CC0), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(CC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    Result = vmulq_f32(Result, sign);
    return Result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorCos(constref V: TXMVECTOR): TXMVECTOR; inline;
var
    x: TXMVECTOR;
begin
    // Map V to x in [-pi,pi].
    x := XMVectorModAngles(V);
    asm
               // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
               // sign = _mm_and_ps(x, g_XMNegativeZero);
               MOVUPS  XMM0,[x]
               ANDPS   XMM0, [g_XMNegativeZero]
               //  c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
               MOVUPS  XMM1,XMM0
               ORPS    XMM1, [g_XMPi]
               //  absx = _mm_andnot_ps(sign, x);  // |x|
               ANDNPS  XMM0, [x]
               //  rflx = _mm_sub_ps(c, x);
               SUBPS   XMM1, [x]
               //  comp = _mm_cmple_ps(absx, g_XMHalfPi);
               CMPPS   XMM0, [g_XMHalfPi], 2
               MOVUPS  XMM4,XMM0
               //  select1 = _mm_andnot_ps(comp, rflx);
               ANDNPS  XMM1, XMM0
               //  select0 = _mm_and_ps(comp, x);
               ANDPS   XMM0, [x]

               // x = _mm_or_ps(select0, select1);
               ORPS    XMM0, XMM1
               // select0 = _mm_and_ps(comp, g_XMOne);
               MOVUPS  XMM2, [g_XMOne]
               ANDPS   XMM2, XMM4
               // select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
               ANDNPS  XMM4, [g_XMNegativeOne]
               // sign = _mm_or_ps(select0, select1);
               ORPS    XMM2, XMM4  // XMM2 = sign

               //  x2 = _mm_mul_ps(x, x);
               MULPS   XMM0, XMM0 // XMM0 = x2

               // Compute polynomial approximation
               //  CC1 = g_XMCosCoefficients1;
               MOVUPS  XMM7, [g_XMCosCoefficients1]
               // vConstants = XM_PERMUTE_PS( CC1, _MM_SHUFFLE(0, 0, 0, 0) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_0_0_0_0
               // Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0

               // CC0 = g_XMCosCoefficients0;
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               // vConstants = XM_PERMUTE_PS( CC0, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_3_3_3_3
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CC0, _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CC0, _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CC0, _MM_SHUFFLE(0, 0, 0, 0) );
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_0_0_0_0
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0
               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM7, [g_XMOne]
               // Result = _mm_mul_ps(Result, sign);
               MULPS   XMM7, XMM2
               // return Result;
               MOVUPS  [result],XMM7
    end;
end;

{$ENDIF}

// 6-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorCosEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := cos(V.f32[0]);
    Result.f32[1] := cos(V.f32[1]);
    Result.f32[2] := cos(V.f32[2]);
    Result.f32[3] := cos(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorCosEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Map V to x in [-pi,pi].
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
    uint32x4_t sign = vandq_u32(x, g_XMNegativeZero);
    uint32x4_t c = vorrq_u32(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    float32x4_t absx = vabsq_f32( x );
    float32x4_t rflx = vsubq_f32(c, x);
    uint32x4_t comp = vcleq_f32(absx, g_XMHalfPi);
    x = vbslq_f32( comp, x, rflx );
    sign = vbslq_f32( comp, g_XMOne, g_XMNegativeOne );

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation
    const XMVECTOR CEC = g_XMCosCoefficients1;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(CEC), 0);
    XMVECTOR Result = vmlaq_lane_f32(vConstants, x2, vget_high_f32(CEC), 1);

    vConstants = vdupq_lane_f32(vget_low_f32(CEC), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    Result = vmulq_f32(Result, sign);
    return Result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorCosEst(constref V: TXMVECTOR): TXMVECTOR; inline;
var
    x: TXMVECTOR;
begin
    // Map V to x in [-pi,pi].
    x := XMVectorModAngles(V);
    asm
               // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
               //  sign = _mm_and_ps(x, g_XMNegativeZero);
               MOVUPS  XMM0,[x]
               ANDPS   XMM0, [g_XMNegativeZero]
               //  c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
               MOVUPS  XMM1, [g_XMPi]
               ORPS    XMM1, XMM0
               //  absx = _mm_andnot_ps(sign, x);  // |x|
               ANDNPS  XMM0, [x]
               //  rflx = _mm_sub_ps(c, x);
               SUBPS   XMM1, [x]
               //  comp = _mm_cmple_ps(absx, g_XMHalfPi);
               CMPPS   XMM0, [g_XMHalfPi], 2
               MOVUPS  XMM4,XMM0
               //  select1 = _mm_andnot_ps(comp, rflx);
               ANDNPS  XMM1, XMM0
               //  select0 = _mm_and_ps(comp, x);
               ANDPS   XMM0, [x]

               // x = _mm_or_ps(select0, select1);
               ORPS    XMM0, XMM1
               // select0 = _mm_and_ps(comp, g_XMOne);
               MOVUPS  XMM2,[g_XMOne]
               ANDPS   XMM2, XMM4
               // select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
               ANDNPS  XMM4, [g_XMNegativeOne]
               // sign = _mm_or_ps(select0, select1);
               ORPS    XMM2, XMM4  // XMM2 = sign

               //  x2 = _mm_mul_ps(x, x);
               MULPS   XMM0, XMM0 // XMM0 = x2

               // Compute polynomial approximation
               // CEC = g_XMCosCoefficients1;
               MOVUPS  XMM7, [g_XMCosCoefficients1]
               //  vConstants = XM_PERMUTE_PS( CEC, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_3_3_3_3
               //  Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CEC, _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6, [g_XMCosCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CEC, _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6, [g_XMCosCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM0, XMM7

               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM0, [g_XMOne]
               // Result = _mm_mul_ps(Result, sign);
               MULPS   XMM0, XMM2

               // return Result;
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}




// 11/10-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorSinCos(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR);
begin
    pSin.f32[0] := sin(V.f32[0]);
    pSin.f32[1] := sin(V.f32[1]);
    pSin.f32[2] := sin(V.f32[2]);
    pSin.f32[3] := sin(V.f32[3]);

    pCos.f32[0] := cos(V.f32[0]);
    pCos.f32[0] := cos(V.f32[1]);
    pCos.f32[0] := cos(V.f32[2]);
    pCos.f32[0] := cos(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorSinCos(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR);
begin
    (* ToDo
     // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
    uint32x4_t sign = vandq_u32(x, g_XMNegativeZero);
    uint32x4_t c = vorrq_u32(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    float32x4_t absx = vabsq_f32( x );
    float32x4_t  rflx = vsubq_f32(c, x);
    uint32x4_t comp = vcleq_f32(absx, g_XMHalfPi);
    x = vbslq_f32( comp, x, rflx );
    sign = vbslq_f32( comp, g_XMOne, g_XMNegativeOne );

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation for sine
    const XMVECTOR SC1 = g_XMSinCoefficients1;
    const XMVECTOR SC0 = g_XMSinCoefficients0;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(SC0), 1);
    XMVECTOR Result = vmlaq_lane_f32(vConstants, x2, vget_low_f32(SC1), 0);

    vConstants = vdupq_lane_f32(vget_high_f32(SC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(SC0), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(SC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    *pSin = vmulq_f32(Result, x);

    // Compute polynomial approximation for cosine
    const XMVECTOR CC1 = g_XMCosCoefficients1;
    const XMVECTOR CC0 = g_XMCosCoefficients0;
    vConstants = vdupq_lane_f32(vget_high_f32(CC0), 1);
    Result = vmlaq_lane_f32(vConstants, x2, vget_low_f32(CC1), 0);

    vConstants = vdupq_lane_f32(vget_high_f32(CC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(CC0), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    vConstants = vdupq_lane_f32(vget_low_f32(CC0), 0);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    *pCos = vmulq_f32(Result, sign);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorSinCos(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR); inline;
var
    x: TXMVECTOR;
begin
    // Force the value within the bounds of pi
    x := XMVectorModAngles(V);
    asm
               // Map in [-pi/2,pi/2] with sin(y) = sin(x), cos(y) = sign*cos(x).
               // sign = _mm_and_ps(x, g_XMNegativeZero);
               MOVUPS  XMM0, [x]
               ANDPS   XMM0, [g_XMNegativeZero]
               //  c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
               MOVUPS  XMM1, [g_XMPi]
               ORPS    XMM1, XMM0
               //  absx = _mm_andnot_ps(sign, x);  // |x|
               ANDNPS  XMM0, [x]
               //  rflx = _mm_sub_ps(c, x);
               SUBPS   XMM1, [x]
               //  comp = _mm_cmple_ps(absx, g_XMHalfPi);
               CMPPS   XMM0, [g_XMHalfPi], 2
               MOVUPS  XMM4,XMM0
               //  select1 = _mm_andnot_ps(comp, rflx);
               ANDNPS  XMM1, XMM0
               //  select0 = _mm_and_ps(comp, x);
               ANDPS   XMM0, [x]

               // x = _mm_or_ps(select0, select1);
               ORPS    XMM0, XMM1
               // select0 = _mm_and_ps(comp, g_XMOne);
               MOVUPS  XMM2, [g_XMOne]
               ANDPS   XMM2, XMM4
               // select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
               ANDNPS  XMM4, [g_XMNegativeOne]
               // sign = _mm_or_ps(select0, select1);
               ORPS    XMM2, XMM4 // XMM2 = sign

               //  x2 = _mm_mul_ps(x, x);
               MOVUPS  XMM1, XMM0
               MULPS   XMM0, XMM0  // XMM0 = x2, XMM1 = x

               // Compute polynomial approximation of sine
               //  SC1 = g_XMSinCoefficients1;
               MOVUPS  XMM7, [g_XMSinCoefficients1]
               //  vConstants = XM_PERMUTE_PS( SC1, _MM_SHUFFLE(0, 0, 0, 0) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_0_0_0_0
               //  Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0

               //  SC0 = g_XMSinCoefficients0;
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               // vConstants = XM_PERMUTE_PS( SC0, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_3_3_3_3
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SC0, _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SC0, _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SC0, _MM_SHUFFLE(0, 0, 0, 0) );
               MOVUPS  XMM6, [g_XMSinCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_0_0_0_0
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0
               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM7,[g_XMOne]
               // Result = _mm_mul_ps(Result, x);
               MULPS   XMM7, XMM1
               //*pSin = Result;
               MOVUPS  [pSin],XMM7

               // Compute polynomial approximation of cosine
               //  CC1 = g_XMCosCoefficients1;
               MOVUPS  XMM7, [g_XMCosCoefficients1]
               // vConstants = XM_PERMUTE_PS( CC1, _MM_SHUFFLE(0, 0, 0, 0) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_0_0_0_0
               // Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0

               // CC0 = g_XMCosCoefficients0;
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               // vConstants = XM_PERMUTE_PS( CC0, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_3_3_3_3
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CC0,  _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CC0,  _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CC0, _MM_SHUFFLE(0, 0, 0, 0) );
               MOVUPS  XMM6, [g_XMCosCoefficients0]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_0_0_0_0
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7,XMM0
               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM7, [g_XMOne]
               // Result = _mm_mul_ps(Result, sign);
               MULPS   XMM7,XMM2
               // *pCos = Result;
               MOVUPS  [pCos],XMM7
    end;
end;

{$ENDIF}



// 7/6-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
procedure XMVectorSinCosEst(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR);
begin
    pSin.f32[0] := sin(V.f32[0]);
    pSin.f32[1] := sin(V.f32[1]);
    pSin.f32[2] := sin(V.f32[2]);
    pSin.f32[3] := sin(V.f32[3]);

    pCos.f32[0] := cos(V.f32[0]);
    pCos.f32[0] := cos(V.f32[1]);
    pCos.f32[0] := cos(V.f32[2]);
    pCos.f32[0] := cos(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
procedure XMVectorSinCosEst(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR);
begin
    (* ToDo
     // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
    uint32x4_t sign = vandq_u32(x, g_XMNegativeZero);
    uint32x4_t c = vorrq_u32(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    float32x4_t absx = vabsq_f32( x );
    float32x4_t rflx = vsubq_f32(c, x);
    uint32x4_t comp = vcleq_f32(absx, g_XMHalfPi);
    x = vbslq_f32( comp, x, rflx );
    sign = vbslq_f32( comp, g_XMOne, g_XMNegativeOne );

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation for sine
    const XMVECTOR SEC = g_XMSinCoefficients1;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(SEC), 0);
    XMVECTOR Result = vmlaq_lane_f32(vConstants, x2, vget_high_f32(SEC), 1);

    vConstants = vdupq_lane_f32(vget_low_f32(SEC), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    *pSin = vmulq_f32(Result, x);

    // Compute polynomial approximation
    const XMVECTOR CEC = g_XMCosCoefficients1;
    vConstants = vdupq_lane_f32(vget_high_f32(CEC), 0);
    Result = vmlaq_lane_f32(vConstants, x2, vget_high_f32(CEC), 1);

    vConstants = vdupq_lane_f32(vget_low_f32(CEC), 1);
    Result = vmlaq_f32(vConstants, Result, x2);

    Result = vmlaq_f32(g_XMOne, Result, x2);
    *pCos = vmulq_f32(Result, sign);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
procedure XMVectorSinCosEst(out pSin: TXMVECTOR; out pCos: TXMVECTOR; constref V: TXMVECTOR); inline;
var
    x: TXMVECTOR;
begin
    // Force the value within the bounds of pi
    x := XMVectorModAngles(V);
    asm
               // Map in [-pi/2,pi/2] with sin(y) = sin(x), cos(y) = sign*cos(x).
               //  sign = _mm_and_ps(x, g_XMNegativeZero);
               MOVUPS  XMM0,[x]
               ANDPS   XMM0, [g_XMNegativeZero]
               //  c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
               MOVUPS  XMM1,[g_XMPi]
               ORPS    XMM1, XMM0
               //  absx = _mm_andnot_ps(sign, x);  // |x|
               ANDNPS  XMM0, [x]
               //  rflx = _mm_sub_ps(c, x);
               SUBPS   XMM1,[x]
               //  comp = _mm_cmple_ps(absx, g_XMHalfPi);
               CMPPS   XMM0, [g_XMHalfPi], 2
               MOVUPS  XMM4,XMM0
               //  select1 = _mm_andnot_ps(comp, rflx);
               ANDNPS  XMM1, XMM0
               //  select0 = _mm_and_ps(comp, x);
               ANDPS   XMM0, [x]

               // x = _mm_or_ps(select0, select1);
               ORPS    XMM0,XMM1
               // select0 = _mm_and_ps(comp, g_XMOne);
               MOVUPS  XMM2, [g_XMOne]
               ANDPS   XMM2, XMM4
               // select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
               ANDNPS  XMM4, [g_XMNegativeOne]
               // sign = _mm_or_ps(select0, select1);
               ORPS    XMM2, XMM4
               //  x2 = _mm_mul_ps(x, x);
               MOVUPS  XMM1,XMM0
               MULPS   XMM0,XMM0

               // Compute polynomial approximation for sine
               //  SEC = g_XMSinCoefficients1;
               MOVUPS  XMM7,[g_XMSinCoefficients1]
               // vConstants = XM_PERMUTE_PS( SEC, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_3_3_3_3
               //  Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SEC, _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6,[g_XMSinCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( SEC, _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6,[g_XMSinCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM7, [g_XMOne]
               // Result = _mm_mul_ps(Result, x);
               MULPS   XMM7, XMM1

               // *pSin = Result;
               MOVUPS  [pSin],XMM7

               // Compute polynomial approximation for cosine
               //  CEC = g_XMCosCoefficients1;
               MOVUPS  XMM7,[g_XMCosCoefficients1]
               // vConstants = XM_PERMUTE_PS( CEC, _MM_SHUFFLE(3, 3, 3, 3) );
               SHUFPS  XMM7, XMM7, _MM_SHUFFLE_3_3_3_3
               // Result = _mm_mul_ps(vConstants, x2);
               MULPS   XMM7, XMM0
               // vConstants = XM_PERMUTE_PS( CEC, _MM_SHUFFLE(2, 2, 2, 2) );
               MOVUPS  XMM6,[g_XMCosCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2

               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // vConstants = XM_PERMUTE_PS( CEC, _MM_SHUFFLE(1, 1, 1, 1) );
               MOVUPS  XMM6,[g_XMCosCoefficients1]
               SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
               // Result = _mm_add_ps(Result, vConstants);
               ADDPS   XMM7, XMM6
               // Result = _mm_mul_ps(Result, x2);
               MULPS   XMM7, XMM0

               // Result = _mm_add_ps(Result, g_XMOne);
               ADDPS   XMM7, [g_XMOne]
               // Result = _mm_mul_ps(Result, sign);
               MULPS   XMM7, XMM2
               // *pCos = Result;
               MOVUPS  [pCos],XMM7
    end;
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Cody and Waite algorithm to compute tangent.
function XMVectorTan(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := tan(V.f32[0]);
    Result.f32[1] := tan(V.f32[1]);
    Result.f32[2] := tan(V.f32[2]);
    Result.f32[3] := tan(V.f32[3]);
end;

{$ELSE}// _XM_SSE_INTRINSICS_ or  _XM_ARM_NEON_INTRINSICS_
function XMVectorTan(constref V: TXMVECTOR): TXMVECTOR; inline;
const
    TanCoefficients0: TXMVECTORF32 = (f: (1.0, -4.667168334e-1, 2.566383229e-2, -3.118153191e-4));
    TanCoefficients1: TXMVECTORF32 = (f: (4.981943399e-7, -1.333835001e-1, 3.424887824e-3, -1.786170734e-5));
    TanConstants: TXMVECTORF32 = (f: (1.570796371, 6.077100628e-11, 0.000244140625, 0.63661977228));
    Mask: TXMVECTORU32 = (u: ($1, $1, $1, $1));
var
    TwoDivPi, Zero, C0, C1, Epsilon, VA: TXMVECTOR;
    VC, VB, VC2, T0, T1, T2, T3, T4, T5, T6, T7: TXMVECTOR;
    VBIsEven, N, D, VCNearZero, R0, R1: TXMVECTOR;
    VIsZero: TXMVECTOR;
begin
    TwoDivPi := XMVectorSplatW(TanConstants.v);

    Zero := XMVectorZero();

    C0 := XMVectorSplatX(TanConstants.v);
    C1 := XMVectorSplatY(TanConstants.v);
    Epsilon := XMVectorSplatZ(TanConstants.v);

    VA := XMVectorMultiply(V, TwoDivPi);

    VA := XMVectorRound(VA);

    VC := XMVectorNegativeMultiplySubtract(VA, C0, V);

    VB := XMVectorAbs(VA);

    VC := XMVectorNegativeMultiplySubtract(VA, C1, VC);

{$if defined(_XM_ARM_NEON_INTRINSICS_) }
    (* ToDo  VB = vcvtq_u32_f32( VB ); *)
{$ELSEif defined(_XM_SSE_INTRINSICS_) }
    asm
               // reinterpret_cast<__m128i *>(&VB)[0] = _mm_cvttps_epi32(VB);
               MOVUPS  XMM0, [VB]
               CVTTPS2DQ XMM0, XMM0
               MOVUPS  [VB],XMM0
    end;
{$else}
    for i := 0 to 3 do
    begin
        VB.u32[i] := uint32(VB.f32[i]);
    end;
{$ENDIF}

    VC2 := XMVectorMultiply(VC, VC);

    T7 := XMVectorSplatW(TanCoefficients1.v);
    T6 := XMVectorSplatZ(TanCoefficients1.v);
    T4 := XMVectorSplatX(TanCoefficients1.v);
    T3 := XMVectorSplatW(TanCoefficients0.v);
    T5 := XMVectorSplatY(TanCoefficients1.v);
    T2 := XMVectorSplatZ(TanCoefficients0.v);
    T1 := XMVectorSplatY(TanCoefficients0.v);
    T0 := XMVectorSplatX(TanCoefficients0.v);


    VBIsEven := XMVectorAndInt(VB, Mask.v);
    VBIsEven := XMVectorEqualInt(VBIsEven, Zero);

    N := XMVectorMultiplyAdd(VC2, T7, T6);
    D := XMVectorMultiplyAdd(VC2, T4, T3);
    N := XMVectorMultiplyAdd(VC2, N, T5);
    D := XMVectorMultiplyAdd(VC2, D, T2);
    N := XMVectorMultiply(VC2, N);
    D := XMVectorMultiplyAdd(VC2, D, T1);
    N := XMVectorMultiplyAdd(VC, N, VC);


    VCNearZero := XMVectorInBounds(VC, Epsilon);
    D := XMVectorMultiplyAdd(VC2, D, T0);

    N := XMVectorSelect(N, VC, VCNearZero);
    D := XMVectorSelect(D, g_XMOne.v, VCNearZero);

    R0 := XMVectorNegate(N);
    R1 := XMVectorDivide(N, D);
    R0 := XMVectorDivide(D, R0);

    VIsZero := XMVectorEqual(V, Zero);

    Result := XMVectorSelect(R0, R1, VBIsEven);
    Result := XMVectorSelect(Result, Zero, VIsZero);

end;

{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorTanEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := tan(V.f32[0]);
    Result.f32[1] := tan(V.f32[1]);
    Result.f32[2] := tan(V.f32[2]);
    Result.f32[3] := tan(V.f32[3]);
end;

{$ELSE}
function XMVectorTanEst(constref V: TXMVECTOR): TXMVECTOR;
var
    OneOverPi, V1, T0, T1, T2, V2T2: TXMVECTOR;
    V2, V1T0, V1T1, D, N: TXMVECTOR;
begin
    OneOverPi := XMVectorSplatW(g_XMTanEstCoefficients.v);

    V1 := XMVectorMultiply(V, OneOverPi);
    V1 := XMVectorRound(V1);

    V1 := XMVectorNegativeMultiplySubtract(g_XMPi.v, V1, V);

    T0 := XMVectorSplatX(g_XMTanEstCoefficients.v);
    T1 := XMVectorSplatY(g_XMTanEstCoefficients.v);
    T2 := XMVectorSplatZ(g_XMTanEstCoefficients.v);

    V2T2 := XMVectorNegativeMultiplySubtract(V1, V1, T2);
    V2 := XMVectorMultiply(V1, V1);
    V1T0 := XMVectorMultiply(V1, T0);
    V1T1 := XMVectorMultiply(V1, T1);

    D := XMVectorReciprocalEst(V2T2);
    N := XMVectorMultiplyAdd(V2, V1T1, V1T0);

    Result := XMVectorMultiply(N, D);
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorSinH(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := sinh(V.f32[0]);
    Result.f32[1] := sinh(V.f32[1]);
    Result.f32[2] := sinh(V.f32[2]);
    Result.f32[3] := sinh(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorSinH(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    static const XMVECTORF32 Scale = { { { 1.442695040888963f, 1.442695040888963f, 1.442695040888963f, 1.442695040888963f } } }; // 1.0f / ln(2.0f)

    XMVECTOR V1 = vmlaq_f32( g_XMNegativeOne.v, V, Scale.v );
    XMVECTOR V2 = vmlsq_f32( g_XMNegativeOne.v, V, Scale.v );
    XMVECTOR E1 = XMVectorExp(V1);
    XMVECTOR E2 = XMVectorExp(V2);

    return vsubq_f32(E1, E2);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorSinH(constref V: TXMVECTOR): TXMVECTOR; {assembler;}
const
    Scale: TXMVECTORF32 = (f: (1.442695040888963, 1.442695040888963, 1.442695040888963, 1.442695040888963)); // 1.0 / ln(2.0)
var
    v1, v2, E1, E2: TXMVECTOR;
begin
    asm
               MOVUPS  XMM0,[V]
               MOVUPS  XMM1,XMM0
               MULPS   XMM1, [Scale]//  V1 = _mm_mul_ps(V, Scale);
               ADDPS   XMM1, [g_XMNegativeOne] // V1 = _mm_add_ps(V1,g_XMNegativeOne);
               MOVUPS  [v1], XMM1
               MOVUPS  XMM2,XMM0
               MULPS   XMM2, [Scale] // V2 = _mm_mul_ps(V, Scale);
               MOVUPS  XMM3,[g_XMNegativeOne]
               SUBPS   XMM3, XMM2 // V2 = _mm_sub_ps(g_XMNegativeOne,V2);
               MOVUPS  [v2], XMM3
    end;
    E1 := XMVectorExp(V1);
    E2 := XMVectorExp(V2);
    asm
               MOVUPS  XMM0,[E1]
               SUBPS   XMM0, [E2] // return _mm_sub_ps(E1, E2);
               MOVUPS  [result], XMM0
    end;
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorCosH(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := cosh(V.f32[0]);
    Result.f32[1] := cosh(V.f32[1]);
    Result.f32[2] := cosh(V.f32[2]);
    Result.f32[3] := cosh(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorCosH(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    static const XMVECTORF32 Scale = { { { 1.442695040888963f, 1.442695040888963f, 1.442695040888963f, 1.442695040888963f } } }; // 1.0f / ln(2.0f)

    XMVECTOR V1 = vmlaq_f32(g_XMNegativeOne.v, V, Scale.v);
    XMVECTOR V2 = vmlsq_f32(g_XMNegativeOne.v, V, Scale.v);
    XMVECTOR E1 = XMVectorExp(V1);
    XMVECTOR E2 = XMVectorExp(V2);
    return vaddq_f32(E1, E2);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorCosH(constref V: TXMVECTOR): TXMVECTOR;
const
    Scale: TXMVECTORF32 = (f: (1.442695040888963, 1.442695040888963, 1.442695040888963, 1.442695040888963)); // 1.0f / ln(2.0f)
var
    V1, V2, E1, E2: TXMVECTOR;
begin
    asm
               MOVUPS  XMM0, [V]
               MOVUPS  XMM1, XMM0
               MULPS   XMM1,[Scale] // V1 = _mm_mul_ps(V,Scale.v);
               ADDPS   XMM1,[g_XMNegativeOne] // V1 = _mm_add_ps(V1,g_XMNegativeOne.v);
               MOVUPS  [V1], XMM1
               MOVUPS  XMM1, XMM0
               MULPS   XMM1,[Scale] // V2 = _mm_mul_ps(V, Scale.v);
               MOVUPS  XMM2, [g_XMNegativeOne]
               SUBPS   XMM2, XMM1 // V2 = _mm_sub_ps(g_XMNegativeOne.v,V2);
               MOVUPS  [V2], XMM2
    end;
    E1 := XMVectorExp(V1);
    E2 := XMVectorExp(V2);
    asm
               MOVUPS  XMM0, [E1]
               ADDPS   XMM0, [E2]
               MOVUPS  [Result], XMM0 // return _mm_add_ps(E1, E2);
    end;
end;

{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorTanH(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := tanh(V.f32[0]);
    Result.f32[1] := tanh(V.f32[1]);
    Result.f32[2] := tanh(V.f32[2]);
    Result.f32[3] := tanh(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorTanH(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    static const XMVECTORF32 Scale = { { { 2.8853900817779268f, 2.8853900817779268f, 2.8853900817779268f, 2.8853900817779268f } } }; // 2.0f / ln(2.0f)

    XMVECTOR E = vmulq_f32(V, Scale.v);
    E = XMVectorExp(E);
    E = vmlaq_f32( g_XMOneHalf.v, E, g_XMOneHalf.v );
    E = XMVectorReciprocal(E);
    return vsubq_f32(g_XMOne.v, E);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorTanH(constref V: TXMVECTOR): TXMVECTOR;
const
    Scale: TXMVECTORF32 = (f: (2.8853900817779268, 2.8853900817779268, 2.8853900817779268, 2.8853900817779268)); // 2.0f / ln(2.0f)
var
    E: TXMVECTOR;
begin
    asm
               MOVUPS  XMM0,[V];
               MULPS   XMM0, [Scale];
               MOVUPS  [E],XMM0 // E = _mm_mul_ps(V, Scale.v);
    end;
    E := XMVectorExp(E);
    asm
               MOVUPS  XMM0,[E]
               MULPS   XMM0, [g_XMOneHalf] //  E = _mm_mul_ps(E,g_XMOneHalf.v);
               ADDPS   XMM0, [g_XMOneHalf] // E = _mm_add_ps(E,g_XMOneHalf.v);
               MOVUPS  XMM1,[g_XMOne]
               DIVPS   XMM1, XMM0 //E = _mm_div_ps(g_XMOne.v,E);
               MOVUPS  XMM0,[g_XMOne]
               SUBPS   XMM0, XMM1 // return _mm_sub_ps(g_XMOne.v,E);
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

// Computes the arcsine of each component of an XMVECTOR.
// 7-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorASin(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arcsin(V.f32[0]);
    Result.f32[1] := arcsin(V.f32[1]);
    Result.f32[2] := arcsin(V.f32[2]);
    Result.f32[3] := arcsin(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorASin(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t nonnegative = vcgeq_f32(V, g_XMZero);
    float32x4_t x = vabsq_f32(V);

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    float32x4_t oneMValue = vsubq_f32(g_XMOne, x);
    float32x4_t clampOneMValue = vmaxq_f32(g_XMZero, oneMValue);
    float32x4_t root = XMVectorSqrt(clampOneMValue);

    // Compute polynomial approximation
    const XMVECTOR AC1 = g_XMArcCoefficients1;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(AC1), 0);
    XMVECTOR t0 = vmlaq_lane_f32( vConstants, x, vget_high_f32(AC1), 1 );

    vConstants = vdupq_lane_f32(vget_low_f32(AC1), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AC1), 0);
    t0 = vmlaq_f32( vConstants, t0, x );

    const XMVECTOR AC0 = g_XMArcCoefficients0;
    vConstants = vdupq_lane_f32(vget_high_f32(AC0), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_high_f32(AC0), 0);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AC0), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AC0), 0);
    t0 = vmlaq_f32( vConstants, t0, x );
    t0 = vmulq_f32(t0, root);

    float32x4_t t1 = vsubq_f32(g_XMPi, t0);
    t0 = vbslq_f32( nonnegative, t0, t1 );
    t0 = vsubq_f32(g_XMHalfPi, t0);
    return t0;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorASin(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // nonnegative = _mm_cmpge_ps(V, g_XMZero);
           MOVUPS  XMM0, [V]
           CMPPS   XMM0, [g_XMZero], 2
           //  mvalue = _mm_sub_ps(g_XMZero, V);
           MOVUPS  XMM1, [g_XMZero]
           SUBPS   XMM1 ,[V]
           //  x = _mm_max_ps(V, mvalue);  // |V|
           MAXPS   XMM1, [V] // XMM1 = x

           // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
           //  oneMValue = _mm_sub_ps(g_XMOne, x);
           MOVUPS  XMM2, [g_XMOne]
           SUBPS   XMM2, XMM1
           //  clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
           MAXPS   XMM2, [g_XMZero]
           //  root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)
           SQRTPS  XMM2, XMM2  // XMM2 = root

           // Compute polynomial approximation
           //  AC1 = g_XMArcCoefficients1;
           MOVUPS  XMM7,[g_XMArcCoefficients1]
           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM7, XMM7, _MM_SHUFFLE_3_3_3_3
           //  t0 = _mm_mul_ps(vConstants, x);
           MULPS   XMM7, XMM1

           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM6,[g_XMArcCoefficients1]
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM7, XMM6
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM7, XMM1

           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM6,[g_XMArcCoefficients1]
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM7, XMM6
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM7, XMM1

           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM6,[g_XMArcCoefficients1]
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_0_0_0_0
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM7, XMM6
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM7, XMM1

           //  AC0 = g_XMArcCoefficients0;
           MOVUPS  XMM6,[g_XMArcCoefficients0]
           // vConstants = XM_PERMUTE_PS( AC0, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_3_3_3_3
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM7, XMM6
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM7, XMM1

           // vConstants = XM_PERMUTE_PS( AC0,_MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM6,[g_XMArcCoefficients0]
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM7, XMM6
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM7, XMM1

           // vConstants = XM_PERMUTE_PS( AC0, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM6,[g_XMArcCoefficients0]
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM7, XMM6
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM7, XMM1

           // vConstants = XM_PERMUTE_PS( AC0, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM6,[g_XMArcCoefficients0]
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_0_0_0_0
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM7, XMM6
           // t0 = _mm_mul_ps(t0, root);
           MULPS   XMM7, XMM1

           //  t1 = _mm_sub_ps(g_XMPi, t0);
           MOVUPS  XMM6, [g_XMPi]
           SUBPS   XMM6, XMM7
           // t0 = _mm_and_ps(nonnegative, t0);
           ANDPS   XMM7, XMM0
           // t1 = _mm_andnot_ps(nonnegative, t1);
           ANDNPS  XMM6, XMM0
           // t0 = _mm_or_ps(t0, t1);
           ORPS    XMM7,XMM6
           // t0 = _mm_sub_ps(g_XMHalfPi, t0);
           MOVUPS  XMM0,[g_XMHalfPi]
           SUBPS   XMM0, XMM7
           // return t0;
           MOVUPS  [result], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
// 3-degree minimax approximation
function XMVectorASinEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arcsin(V.f32[0]);
    Result.f32[1] := arcsin(V.f32[1]);
    Result.f32[2] := arcsin(V.f32[2]);
    Result.f32[3] := arcsin(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorASinEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo

    uint32x4_t nonnegative = vcgeq_f32(V, g_XMZero);
    float32x4_t x = vabsq_f32(V);

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    float32x4_t oneMValue = vsubq_f32(g_XMOne, x);
    float32x4_t clampOneMValue = vmaxq_f32(g_XMZero, oneMValue);
    float32x4_t root = XMVectorSqrt(clampOneMValue);

    // Compute polynomial approximation
    const XMVECTOR AEC = g_XMArcEstCoefficients;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(AEC), 0);
    XMVECTOR t0 = vmlaq_lane_f32( vConstants, x, vget_high_f32(AEC), 1 );

    vConstants = vdupq_lane_f32(vget_low_f32(AEC), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AEC), 0);
    t0 = vmlaq_f32( vConstants, t0, x );
    t0 = vmulq_f32(t0, root);

    float32x4_t t1 = vsubq_f32(g_XMPi, t0);
    t0 = vbslq_f32( nonnegative, t0, t1 );
    t0 = vsubq_f32(g_XMHalfPi, t0);
    return t0;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorASinEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // nonnegative = _mm_cmpge_ps(V, g_XMZero);
           MOVUPS  XMM0, [g_XMZero]
           MOVUPS  XMM1,XMM0
           CMPPS   XMM0, [V], 2
           //  mvalue = _mm_sub_ps(g_XMZero, V);
           SUBPS   XMM1, [V]
           //  x = _mm_max_ps(V, mvalue);  // |V|
           MAXPS   XMM1, [V]

           // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
           //  oneMValue = _mm_sub_ps(g_XMOne, x);
           MOVUPS  XMM2, [g_XMOne]
           SUBPS   XMM2, XMM1
           //  clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
           MAXPS   XMM2, g_XMZero
           //  root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)
           SQRTPS  XMM2, XMM2
           // Compute polynomial approximation
           //  AEC = g_XMArcEstCoefficients;
           MOVUPS  XMM3,[g_XMArcEstCoefficients]
           //  vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           //  t0 = _mm_mul_ps(vConstants, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM4,[g_XMArcEstCoefficients]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_2_2_2_2
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM4,[g_XMArcEstCoefficients]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_1_1_1
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM4,[g_XMArcEstCoefficients]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_0_0_0_0
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, root);
           MULPS   XMM3, XMM2

           //  t1 = _mm_sub_ps(g_XMPi, t0);
           MOVUPS  XMM5, [g_XMPi]
           SUBPS   XMM5, XMM3
           // t0 = _mm_and_ps(nonnegative, t0);
           ANDPS   XMM3, XMM0
           // t1 = _mm_andnot_ps(nonnegative, t1);
           ANDNPS  XMM5, XMM0
           // t0 = _mm_or_ps(t0, t1);
           ORPS    XMM3,XMM5
           // t0 = _mm_sub_ps(g_XMHalfPi, t0);
           MOVUPS  XMM6, [g_XMHalfPi]
           SUBPS   XMM6, XMM3
           // return t0;
           MOVUPS  [result], XMM6
end;
{$ENDIF}


// 7-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorACos(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arccos(V.f32[0]);
    Result.f32[1] := arccos(V.f32[1]);
    Result.f32[2] := arccos(V.f32[2]);
    Result.f32[3] := arccos(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorACos(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t nonnegative = vcgeq_f32(V, g_XMZero);
    float32x4_t x = vabsq_f32(V);

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    float32x4_t oneMValue = vsubq_f32(g_XMOne, x);
    float32x4_t clampOneMValue = vmaxq_f32(g_XMZero, oneMValue);
    float32x4_t root = XMVectorSqrt(clampOneMValue);

    // Compute polynomial approximation
    const XMVECTOR AC1 = g_XMArcCoefficients1;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(AC1), 0);
    XMVECTOR t0 = vmlaq_lane_f32( vConstants, x, vget_high_f32(AC1), 1 );

    vConstants = vdupq_lane_f32(vget_low_f32(AC1), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AC1), 0);
    t0 = vmlaq_f32( vConstants, t0, x );

    const XMVECTOR AC0 = g_XMArcCoefficients0;
    vConstants = vdupq_lane_f32(vget_high_f32(AC0), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_high_f32(AC0), 0);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AC0), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AC0), 0);
    t0 = vmlaq_f32( vConstants, t0, x );
    t0 = vmulq_f32(t0, root);

    float32x4_t t1 = vsubq_f32(g_XMPi, t0);
    t0 = vbslq_f32( nonnegative, t0, t1 );
    return t0;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorACos(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // __m128 nonnegative = _mm_cmpge_ps(V, g_XMZero);
           MOVUPS  XMM0,[g_XMZero]
           MOVUPS  XMM1, XMM0
           CMPPS   XMM0, [V], 2 // nonnegative = XMM0
           // __m128 mvalue = _mm_sub_ps(g_XMZero, V);
           SUBPS   XMM1, [V]  // mvalue = XMM1
           //    __m128 x = _mm_max_ps(V, mvalue);  // |V|
           MAXPS   XMM1, [V] // x = XMM1

           // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
           // __m128 oneMValue = _mm_sub_ps(g_XMOne, x);
           MOVUPS  XMM2, [g_XMOne]
           SUBPS   XMM2, XMM1
           //  __m128 clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
           MAXPS   XMM2, [g_XMZero]
           // __m128 root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)
           SQRTPS  XMM2, XMM2    // root = XMM2

           // Compute polynomial approximation
           // AC1 = g_XMArcCoefficients1;
           MOVUPS  XMM3, [g_XMArcCoefficients1]
           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // C t0 = _mm_mul_ps(vConstants, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM4, [g_XMArcCoefficients1]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_2_2_2_2
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM4, [g_XMArcCoefficients1]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_1_1_1
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AC1, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM4, [g_XMArcCoefficients1]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_0_0_0_0
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           //  AC0 = g_XMArcCoefficients0;
           MOVUPS  XMM4, [g_XMArcCoefficients0]
           // vConstants = XM_PERMUTE_PS( AC0, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3

           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AC0, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM4, [g_XMArcCoefficients0]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_2_2_2_2
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AC0, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM4, [g_XMArcCoefficients0]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_1_1_1
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AC0, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM4, [g_XMArcCoefficients0]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_0_0_0_0
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, root);
           MULPS   XMM3, XMM2

           //  t1 = _mm_sub_ps(g_XMPi, t0);
           MOVUPS  XMM5, [g_XMPi]
           SUBPS   XMM5, XMM3
           // t0 = _mm_and_ps(nonnegative, t0);
           ANDPS   XMM3, XMM0
           // t1 = _mm_andnot_ps(nonnegative, t1);
           ANDNPS  XMM5, XMM0
           // t0 = _mm_or_ps(t0, t1);
           ORPS    XMM3,XMM5
           // return t0;
           MOVUPS  [result],XMM3
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
// 3-degree minimax approximation
function XMVectorACosEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arccos(V.f32[0]);
    Result.f32[1] := arccos(V.f32[1]);
    Result.f32[2] := arccos(V.f32[2]);
    Result.f32[3] := arccos(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorACosEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    uint32x4_t nonnegative = vcgeq_f32(V, g_XMZero);
    float32x4_t x = vabsq_f32(V);

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    float32x4_t oneMValue = vsubq_f32(g_XMOne, x);
    float32x4_t clampOneMValue = vmaxq_f32(g_XMZero, oneMValue);
    float32x4_t root = XMVectorSqrt(clampOneMValue);

    // Compute polynomial approximation
    const XMVECTOR AEC = g_XMArcEstCoefficients;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(AEC), 0);
    XMVECTOR t0 = vmlaq_lane_f32( vConstants, x, vget_high_f32(AEC), 1 );

    vConstants = vdupq_lane_f32(vget_low_f32(AEC), 1);
    t0 = vmlaq_f32( vConstants, t0, x );

    vConstants = vdupq_lane_f32(vget_low_f32(AEC), 0);
    t0 = vmlaq_f32( vConstants, t0, x );
    t0 = vmulq_f32(t0, root);

    float32x4_t t1 = vsubq_f32(g_XMPi, t0);
    t0 = vbslq_f32( nonnegative, t0, t1 );
    return t0;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorACosEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  nonnegative = _mm_cmpge_ps(V, g_XMZero);
           MOVUPS  XMM0,[g_XMZero]
           MOVUPS  XMM1, XMM0
           CMPPS   XMM0, [V], 2
           //  mvalue = _mm_sub_ps(g_XMZero, V);

           SUBPS   XMM1, [V]
           //  x = _mm_max_ps(V, mvalue);  // |V|
           MAXPS   XMM1, [V]

           // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
           //  oneMValue = _mm_sub_ps(g_XMOne, x);
           MOVUPS  XMM2,[g_XMOne]
           SUBPS   XMM2, XMM1
           //  clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
           MAXPS   XMM2, [g_XMZero]
           //  root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)
           SQRTPS  XMM2, XMM2

           // Compute polynomial approximation
           //  AEC = g_XMArcEstCoefficients;
           MOVUPS  XMM3, [g_XMArcEstCoefficients]
           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           //  t0 = _mm_mul_ps(vConstants, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM4, [g_XMArcEstCoefficients]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_2_2_2_2
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM4, [g_XMArcEstCoefficients]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_1_1_1
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, x);
           MULPS   XMM3, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM4, [g_XMArcEstCoefficients]
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_0_0_0_0
           // t0 = _mm_add_ps(t0, vConstants);
           ADDPS   XMM3, XMM4
           // t0 = _mm_mul_ps(t0, root);
           MULPS   XMM3, XMM2

           //  t1 = _mm_sub_ps(g_XMPi, t0);
           MOVUPS  XMM5, [g_XMPi]
           SUBPS   XMM5, XMM3
           // t0 = _mm_and_ps(nonnegative, t0);
           ANDPS   XMM3, XMM0
           // t1 = _mm_andnot_ps(nonnegative, t1);
           ANDNPS  XMM5, XMM0
           // t0 = _mm_or_ps(t0, t1);
           ORPS    XMM3,XMM5
           // return t0;
           MOVUPS  [result], XMM3
end;
{$ENDIF}

// 17-degree minimax approximation
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorATan(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arctan(V.f32[0]);
    Result.f32[1] := arctan(V.f32[1]);
    Result.f32[2] := arctan(V.f32[2]);
    Result.f32[3] := arctan(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorATan(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    float32x4_t absV = vabsq_f32(V);
    float32x4_t invV = XMVectorReciprocal(V);
    uint32x4_t comp = vcgtq_f32(V, g_XMOne);
    uint32x4_t sign = vbslq_f32(comp, g_XMOne, g_XMNegativeOne);
    comp = vcleq_f32(absV, g_XMOne);
    sign = vbslq_f32(comp, g_XMZero, sign);
    uint32x4_t x = vbslq_f32(comp, V, invV);

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation
    const XMVECTOR TC1 = g_XMATanCoefficients1;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(TC1), 0);
    XMVECTOR Result = vmlaq_lane_f32( vConstants, x2, vget_high_f32(TC1), 1 );

    vConstants = vdupq_lane_f32(vget_low_f32(TC1), 1);
    Result = vmlaq_f32( vConstants, Result, x2 );

    vConstants = vdupq_lane_f32(vget_low_f32(TC1), 0);
    Result = vmlaq_f32( vConstants, Result, x2 );

    const XMVECTOR TC0 = g_XMATanCoefficients0;
    vConstants = vdupq_lane_f32(vget_high_f32(TC0), 1);
    Result = vmlaq_f32( vConstants, Result, x2 );

    vConstants = vdupq_lane_f32(vget_high_f32(TC0), 0);
    Result = vmlaq_f32( vConstants, Result, x2 );

    vConstants = vdupq_lane_f32(vget_low_f32(TC0), 1);
    Result = vmlaq_f32( vConstants, Result, x2 );

    vConstants = vdupq_lane_f32(vget_low_f32(TC0), 0);
    Result = vmlaq_f32( vConstants, Result, x2 );

    Result = vmlaq_f32( g_XMOne, Result, x2 );
    Result = vmulq_f32( Result, x );

    float32x4_t result1 = vmulq_f32(sign, g_XMHalfPi);
    result1 = vsubq_f32(result1, Result);

    comp = vceqq_f32(sign, g_XMZero);
    Result = vbslq_f32( comp, Result, result1 );
    return Result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorATan(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // XMVectorAbs(V);
           XORPS   XMM0,XMM0
           SUBPS   XMM0,[V]
           MAXPS   XMM0,[V] // XMM0 = absV

           // invV = _mm_div_ps(g_XMOne, V);
           MOVUPS  XMM1, [g_XMOne]
           DIVPS   XMM1, [V]
           //  comp = _mm_cmpgt_ps(V, g_XMOne);
           MOVUPS  XMM2,[g_XMOne]
           CMPPS   XMM2, [V], 1
           //  select0 = _mm_and_ps(comp, g_XMOne);
           MOVUPS  XMM3, [g_XMOne]
           ANDPS   XMM3, XMM2
           //  select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
           ANDNPS  XMM2, [g_XMNegativeOne]
           //  sign = _mm_or_ps(select0, select1);
           ORPS    XMM3,XMM2
           // comp = _mm_cmple_ps(absV, g_XMOne);
           CMPPS   XMM0, [g_XMOne], 2
           // select0 = _mm_and_ps(comp, g_XMZero);
           MOVUPS  XMM7, [g_XMZero]
           ANDPS   XMM7, XMM0
           // select1 = _mm_andnot_ps(comp, sign);
           ANDNPS  XMM3, XMM0
           // sign = _mm_or_ps(select0, select1);
           ORPS    XMM7, XMM3 // XMM7 = sign
           // select0 = _mm_and_ps(comp, V);
           MOVUPS  XMM3, XMM0
           ANDPS   XMM3, [V]
           // select1 = _mm_andnot_ps(comp, invV);
           ANDNPS  XMM0, XMM1
           //  x = _mm_or_ps(select0, select1);
           ORPS    XMM0,XMM3
           //  x2 = _mm_mul_ps(x, x);
           MOVUPS  XMM1,XMM0
           MULPS   XMM1, XMM1

           // Compute polynomial approximation
           //  TC1 = g_XMATanCoefficients1;
           MOVUPS  XMM4, [g_XMATanCoefficients1]
           //  vConstants = XM_PERMUTE_PS( TC1, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3
           //  Result = _mm_mul_ps(vConstants, x2);
           MULPS   XMM4, XMM1

           //  vConstants = XM_PERMUTE_PS( TC1, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM5, [g_XMATanCoefficients1]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_2_2_2_2
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( TC1, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM5, [g_XMATanCoefficients1]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_1_1_1_1
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( TC1, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM5, [g_XMATanCoefficients1]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_0_0_0_0
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           //  TC0 = g_XMATanCoefficients0;
           MOVUPS  XMM5, [g_XMATanCoefficients0]
           // vConstants = XM_PERMUTE_PS( TC0, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_3_3_3_3
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( TC0, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM5, [g_XMATanCoefficients0]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_2_2_2_2
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( TC0, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM5, [g_XMATanCoefficients0]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_1_1_1_1
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( TC0, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM5, [g_XMATanCoefficients0]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_0_0_0_0
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1
           // Result = _mm_add_ps(Result, g_XMOne);
           ADDPS   XMM4, [g_XMOne]
           // Result = _mm_mul_ps(Result, x);
           MULPS   XMM4, XMM0
           //  result1 = _mm_mul_ps(sign, g_XMHalfPi);
           MOVUPS  XMM5,[g_XMHalfPi]
           MULPS   XMM5, XMM7
           // result1 = _mm_sub_ps(result1, Result);
           SUBPS   XMM5, XMM4

           // comp = _mm_cmpeq_ps(sign, g_XMZero);
           CMPPS   XMM7, [g_XMZero], 0
           // select0 = _mm_and_ps(comp, Result);
           ANDPS   XMM4, XMM7
           // select1 = _mm_andnot_ps(comp, result1);
           ANDNPS  XMM7, XMM5
           // Result = _mm_or_ps(select0, select1);
           ORPS    XMM4, XMM7
           // return Result;
           MOVUPS  [result], XMM4
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// 9-degree minimax approximation
function XMVectorATanEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arctan(V.f32[0]);
    Result.f32[1] := arctan(V.f32[1]);
    Result.f32[2] := arctan(V.f32[2]);
    Result.f32[3] := arctan(V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorATanEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    float32x4_t absV = vabsq_f32(V);
    float32x4_t invV = XMVectorReciprocalEst(V);
    uint32x4_t comp = vcgtq_f32(V, g_XMOne);
    uint32x4_t sign = vbslq_f32(comp, g_XMOne, g_XMNegativeOne );
    comp = vcleq_f32(absV, g_XMOne);
    sign = vbslq_f32(comp, g_XMZero, sign );
    uint32x4_t x = vbslq_f32(comp, V, invV );

    float32x4_t x2 = vmulq_f32(x, x);

    // Compute polynomial approximation
    const XMVECTOR AEC = g_XMATanEstCoefficients1;
    XMVECTOR vConstants = vdupq_lane_f32(vget_high_f32(AEC), 0);
    XMVECTOR Result = vmlaq_lane_f32( vConstants, x2, vget_high_f32(AEC), 1 );

    vConstants = vdupq_lane_f32(vget_low_f32(AEC), 1);
    Result = vmlaq_f32( vConstants, Result, x2 );

    vConstants = vdupq_lane_f32(vget_low_f32( AEC), 0);
    Result = vmlaq_f32( vConstants, Result, x2 );

    // ATanEstCoefficients0 is already splatted
    Result = vmlaq_f32( g_XMATanEstCoefficients0, Result, x2 );
    Result = vmulq_f32( Result, x );

    float32x4_t result1 = vmulq_f32(sign, g_XMHalfPi);
    result1 = vsubq_f32(result1, Result);

    comp = vceqq_f32(sign, g_XMZero);
    Result = vbslq_f32( comp, Result, result1 );
    return Result;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorATanEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // XMVectorAbs(V);
           XORPS   XMM0,XMM0
           SUBPS   XMM0,[V]
           MAXPS   XMM0,[V] // XMM0 = absV

           //  invV = _mm_div_ps(g_XMOne, V);
           MOVUPS  XMM1,[g_XMOne]
           MOVUPS  XMM3,XMM1
           DIVPS   XMM1, [V]  // XMM1 = invV
           //  comp = _mm_cmpgt_ps(V, g_XMOne);
           MOVUPS  XMM2, [g_XMOne]
           CMPPS   XMM2, [V], 1
           //  select0 = _mm_and_ps(comp, g_XMOne);
           ANDPS   XMM3, XMM2
           //  select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
           ANDNPS  XMM2, [g_XMNegativeOne]
           //  sign = _mm_or_ps(select0, select1);
           ORPS    XMM2, XMM3 //  XMM2 = sign

           // comp = _mm_cmple_ps(absV, g_XMOne);
           CMPPS   XMM0, [g_XMOne], 2
           // select0 = _mm_and_ps(comp, g_XMZero);
           MOVUPS  XMM3, [g_XMZero]
           ANDPS   XMM3, XMM0
           // select1 = _mm_andnot_ps(comp, sign);
           ANDNPS  XMM2, XMM0
           // sign = _mm_or_ps(select0, select1);
           ORPS    XMM2, XMM3  //  XMM2 = sign
           // select0 = _mm_and_ps(comp, V);
           MOVUPS  XMM3,[V]
           ANDPS   XMM3, XMM0
           // select1 = _mm_andnot_ps(comp, invV);
           ANDNPS  XMM0, XMM1
           //  x = _mm_or_ps(select0, select1);
           ORPS    XMM0, XMM3 // XMM0 = x

           //  x2 = _mm_mul_ps(x, x);
           MOVUPS  XMM1,XMM0
           MULPS   XMM1, XMM1 // XMM1 = x2

           // Compute polynomial approximation
           //  AEC = g_XMATanEstCoefficients1;
           MOVUPS  XMM4, [g_XMATanEstCoefficients1]
           //  vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(3, 3, 3, 3) );
           SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3
           //  Result = _mm_mul_ps(vConstants, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(2, 2, 2, 2) );
           MOVUPS  XMM5, [g_XMATanEstCoefficients1]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_2_2_2_2
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(1, 1, 1, 1) );
           MOVUPS  XMM5, [g_XMATanEstCoefficients1]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_1_1_1_1
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // vConstants = XM_PERMUTE_PS( AEC, _MM_SHUFFLE(0, 0, 0, 0) );
           MOVUPS  XMM5, [g_XMATanEstCoefficients1]
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_0_0_0_0
           // Result = _mm_add_ps(Result, vConstants);
           ADDPS   XMM4, XMM5
           // Result = _mm_mul_ps(Result, x2);
           MULPS   XMM4, XMM1

           // ATanEstCoefficients0 is already splatted
           // Result = _mm_add_ps(Result, g_XMATanEstCoefficients0);
           ADDPS   XMM4, [g_XMATanEstCoefficients0]
           // Result = _mm_mul_ps(Result, x);
           MULPS   XMM4, XMM0
           //  result1 = _mm_mul_ps(sign, g_XMHalfPi);
           MOVUPS  XMM5, [g_XMHalfPi]
           MULPS   XMM5, XMM2
           // result1 = _mm_sub_ps(result1, Result);
           SUBPS   XMM5, XMM4

           // comp = _mm_cmpeq_ps(sign, g_XMZero);
           CMPPS   XMM2, [g_XMZero], 0
           // select0 = _mm_and_ps(comp, Result);
           ANDPS   XMM4, XMM2
           // select1 = _mm_andnot_ps(comp, result1);
           ANDNPS  XMM5, XMM2
           // Result = _mm_or_ps(select0, select1);
           ORPS    XMM4, XMM5
           // return Result;
           MOVUPS  [result], XMM4
end;
{$ENDIF}

// Computes the arctangent of Y/X.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorATan2(constref Y: TXMVECTOR; constref X: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arctan2(Y.f32[0], X.f32[0]);
    Result.f32[1] := arctan2(Y.f32[1], X.f32[1]);
    Result.f32[2] := arctan2(Y.f32[2], X.f32[2]);
    Result.f32[3] := arctan2(Y.f32[3], X.f32[3]);
end;

{$ELSE}
function XMVectorATan2(constref Y: TXMVECTOR; constref X: TXMVECTOR): TXMVECTOR;
const
    ATan2Constants: TXMVECTORF32 = (f: (3.141592654, 1.570796327, 0.785398163, 3.141592654 * 3.0 / 4.0));
var
    Zero, ATanResultValid, Pi, PiOverTwo, PiOverFour, ThreePiOverFour: TXMVECTOR;
    YEqualsZero, XEqualsZero, XIsPositive: TXMVECTOR;
    YEqualsInfinity, XEqualsInfinity, YSign: TXMVECTOR;
    R1, R2, R3, R4, R5: TXMVECTOR;
    V, R0: TXMVECTOR;
begin
    // Return the inverse tangent of Y / X in the range of -Pi to Pi with the following exceptions:

    //     Y == 0 and X is Negative         -> Pi with the sign of Y
    //     y == 0 and x is positive         -> 0 with the sign of y
    //     Y != 0 and X == 0                -> Pi / 2 with the sign of Y
    //     Y != 0 and X is Negative         -> atan(y/x) + (PI with the sign of Y)
    //     X == -Infinity and Finite Y      -> Pi with the sign of Y
    //     X == +Infinity and Finite Y      -> 0 with the sign of Y
    //     Y == Infinity and X is Finite    -> Pi / 2 with the sign of Y
    //     Y == Infinity and X == -Infinity -> 3Pi / 4 with the sign of Y
    //     Y == Infinity and X == +Infinity -> Pi / 4 with the sign of Y



    Zero := XMVectorZero();
    ATanResultValid := XMVectorTrueInt();

    Pi := XMVectorSplatX(ATan2Constants);
    PiOverTwo := XMVectorSplatY(ATan2Constants);
    PiOverFour := XMVectorSplatZ(ATan2Constants);
    ThreePiOverFour := XMVectorSplatW(ATan2Constants);

    YEqualsZero := XMVectorEqual(Y, Zero);
    XEqualsZero := XMVectorEqual(X, Zero);
    XIsPositive := XMVectorAndInt(X, g_XMNegativeZero.v);
    XIsPositive := XMVectorEqualInt(XIsPositive, Zero);
    YEqualsInfinity := XMVectorIsInfinite(Y);
    XEqualsInfinity := XMVectorIsInfinite(X);

    YSign := XMVectorAndInt(Y, g_XMNegativeZero.v);
    Pi := XMVectorOrInt(Pi, YSign);
    PiOverTwo := XMVectorOrInt(PiOverTwo, YSign);
    PiOverFour := XMVectorOrInt(PiOverFour, YSign);
    ThreePiOverFour := XMVectorOrInt(ThreePiOverFour, YSign);

    R1 := XMVectorSelect(Pi, YSign, XIsPositive);
    R2 := XMVectorSelect(ATanResultValid, PiOverTwo, XEqualsZero);
    R3 := XMVectorSelect(R2, R1, YEqualsZero);
    R4 := XMVectorSelect(ThreePiOverFour, PiOverFour, XIsPositive);
    R5 := XMVectorSelect(PiOverTwo, R4, XEqualsInfinity);
    Result := XMVectorSelect(R3, R5, YEqualsInfinity);
    ATanResultValid := XMVectorEqualInt(Result, ATanResultValid);

    V := XMVectorDivide(Y, X);

    R0 := XMVectorATan(V);

    R1 := XMVectorSelect(Pi, g_XMNegativeZero, XIsPositive);
    R2 := XMVectorAdd(R0, R1);

    Result := XMVectorSelect(Result, R2, ATanResultValid);

end;

{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorATan2Est(constref Y: TXMVECTOR; constref X: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := arctan2(Y.f32[0], X.f32[0]);
    Result.f32[1] := arctan2(Y.f32[1], X.f32[1]);
    Result.f32[2] := arctan2(Y.f32[2], X.f32[2]);
    Result.f32[3] := arctan2(Y.f32[3], X.f32[3]);
end;

{$ELSE}
function XMVectorATan2Est(constref Y: TXMVECTOR; constref X: TXMVECTOR): TXMVECTOR; inline;
const
    ATan2Constants: TXMVECTORF32 = (f: (3.141592654, 1.570796327, 0.785398163, 2.3561944905));
var
    Zero, ATanResultValid, Pi, PiOverTwo, PiOverFour, ThreePiOverFour, YEqualsZero, XEqualsZero, XIsPositive: TXMVECTOR;
    YEqualsInfinity, XEqualsInfinity, YSign: TXMVECTOR;
    R1, R2, R3, R4, R5: TXMVECTOR;
    Reciprocal, V, R0: TXMVECTOR;
begin
    Zero := XMVectorZero();
    ATanResultValid := XMVectorTrueInt();

    Pi := XMVectorSplatX(ATan2Constants);
    PiOverTwo := XMVectorSplatY(ATan2Constants);
    PiOverFour := XMVectorSplatZ(ATan2Constants);
    ThreePiOverFour := XMVectorSplatW(ATan2Constants);

    YEqualsZero := XMVectorEqual(Y, Zero);
    XEqualsZero := XMVectorEqual(X, Zero);
    XIsPositive := XMVectorAndInt(X, g_XMNegativeZero.v);
    XIsPositive := XMVectorEqualInt(XIsPositive, Zero);
    YEqualsInfinity := XMVectorIsInfinite(Y);
    XEqualsInfinity := XMVectorIsInfinite(X);

    YSign := XMVectorAndInt(Y, g_XMNegativeZero.v);
    Pi := XMVectorOrInt(Pi, YSign);
    PiOverTwo := XMVectorOrInt(PiOverTwo, YSign);
    PiOverFour := XMVectorOrInt(PiOverFour, YSign);
    ThreePiOverFour := XMVectorOrInt(ThreePiOverFour, YSign);

    R1 := XMVectorSelect(Pi, YSign, XIsPositive);
    R2 := XMVectorSelect(ATanResultValid, PiOverTwo, XEqualsZero);
    R3 := XMVectorSelect(R2, R1, YEqualsZero);
    R4 := XMVectorSelect(ThreePiOverFour, PiOverFour, XIsPositive);
    R5 := XMVectorSelect(PiOverTwo, R4, XEqualsInfinity);
    Result := XMVectorSelect(R3, R5, YEqualsInfinity);
    ATanResultValid := XMVectorEqualInt(Result, ATanResultValid);

    Reciprocal := XMVectorReciprocalEst(X);
    V := XMVectorMultiply(Y, Reciprocal);
    R0 := XMVectorATanEst(V);

    R1 := XMVectorSelect(Pi, g_XMNegativeZero, XIsPositive);
    R2 := XMVectorAdd(R0, R1);

    Result := XMVectorSelect(Result, R2, ATanResultValid);
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// V0 + t * (V1 - V0)
function XMVectorLerp(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref t: single): TXMVECTOR;
var
    Scale, Length: TXMVECTOR;
begin
    Scale := XMVectorReplicate(t);
    Length := XMVectorSubtract(V1, V0);
    Result := XMVectorMultiplyAdd(Length, Scale, V0);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorLerp(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref t: single): TXMVECTOR;
begin
    (* ToDo
    XMVECTOR L = vsubq_f32( V1, V0 );
    return vmlaq_n_f32( V0, L, t );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorLerp(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref t: single): TXMVECTOR; assembler;
asm

           //  L = _mm_sub_ps( V1, V0 );
           MOVUPS  XMM0,[V1]
           SUBPS   XMM0, [V0]
           //  S = _mm_set_ps1( t );
           MOVSS   XMM1, [t]
           SHUFPS  XMM1, XMM1, 0
           //  Result = _mm_mul_ps( L, S );
           MULPS   XMM0, XMM1
           //  return _mm_add_ps( Result, V0 );
           ADDPS   XMM0, [V0]
           MOVUPS  [result], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// V0 + T * (V1 - V0)
function XMVectorLerpV(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR;
var
    Length: TXMVECTOR;
begin
    Length := XMVectorSubtract(V1, V0);
    Result := XMVectorMultiplyAdd(Length, T, V0);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorLerpV(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   XMVECTOR L = vsubq_f32( V1, V0 );
    return vmlaq_f32( V0, L, T );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorLerpV(constref V0: TXMVECTOR; constref V1: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Length = _mm_sub_ps( V1, V0 );
           MOVUPS  XMM0,[V1]
           SUBPS   XMM0, [V0]
           //  Result = _mm_mul_ps( Length, T );
           MULPS   XMM0, [T]
           // return _mm_add_ps( Result, V0 );
           ADDPS   XMM0, [V0]
           MOVUPS  [result], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Result  :=  (2 * t^3 - 3 * t^2 + 1) * Position0 +
//          (t^3 - 2 * t^2 + t) * Tangent0 +
//          (-2 * t^3 + 3 * t^2) * Position1 +
//          (t^3 - t^2) * Tangent1
function XMVectorHermite(constref Position0: TXMVECTOR; constref Tangent0: TXMVECTOR; constref Position1: TXMVECTOR; constref Tangent1: TXMVECTOR; constref t: single): TXMVECTOR;
var
    t2, t3: single;
    P0, T0, P1, T1: TXMVECTOR;
begin
    t2 := t * t;
    t3 := t * t2;

    P0 := XMVectorReplicate(2.0 * t3 - 3.0 * t2 + 1.0);
    T0 := XMVectorReplicate(t3 - 2.0 * t2 + t);
    P1 := XMVectorReplicate(-2.0 * t3 + 3.0 * t2);
    T1 := XMVectorReplicate(t3 - t2);

    Result := XMVectorMultiply(P0, Position0);
    Result := XMVectorMultiplyAdd(T0, Tangent0, Result);
    Result := XMVectorMultiplyAdd(P1, Position1, Result);
    Result := XMVectorMultiplyAdd(T1, Tangent1, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorHermite(Position0: TXMVECTOR; Tangent0: TXMVECTOR; Position1: TXMVECTOR; Tangent1: TXMVECTOR; t: single): TXMVECTOR;
begin
    (* ToDo
    float t2 = t * t;
    float t3 = t * t2;

    float p0 = 2.0f * t3 - 3.0f * t2 + 1.0;
    float t0 = t3 - 2.0f * t2 + t;
    float p1 = -2.0f * t3 + 3.0f * t2;
    float t1 = t3 - t2;

    XMVECTOR vResult = vmulq_n_f32(Position0, p0 );
    vResult = vmlaq_n_f32( vResult, Tangent0, t0 );
    vResult = vmlaq_n_f32( vResult, Position1, p1 );
    vResult = vmlaq_n_f32( vResult, Tangent1, t1 );
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorHermite(constref Position0: TXMVECTOR; constref Tangent0: TXMVECTOR; constref Position1: TXMVECTOR; constref Tangent1: TXMVECTOR; constref t: single): TXMVECTOR;
var
    t2, t3: single;
    f1, f2, f3, f4: single;
begin
    t2 := t * t;
    t3 := t * t2;
    f1 := 2.0 * t3 - 3.0 * t2 + 1.0;
    f2 := t3 - 2.0 * t2 + t;
    f3 := -2.0 * t3 + 3.0 * t2;
    f4 := t3 - t2;

    asm
               //  P0 = _mm_set_ps1(2.0 * t3 - 3.0 * t2 + 1.0);
               MOVSS   XMM0, [f1]
               SHUFPS  XMM0, XMM0, 0
               //  T0 = _mm_set_ps1(t3 - 2.0 * t2 + t);
               MOVSS   XMM1, [f2]
               SHUFPS  XMM1, XMM1, 0
               //  P1 = _mm_set_ps1(-2.0 * t3 + 3.0 * t2);
               MOVSS   XMM2, [f3]
               SHUFPS  XMM2, XMM2, 0
               //  T1 = _mm_set_ps1(t3 - t2);
               MOVSS   XMM3, [f4]
               SHUFPS  XMM3, XMM3, 0

               //  vResult = _mm_mul_ps(P0, Position0);
               MULPS   XMM0, [Position0]
               //  vTemp = _mm_mul_ps(T0, Tangent0);
               MULPS   XMM1, [Tangent0]
               // vResult = _mm_add_ps(vResult,vTemp);
               ADDPS   XMM0, XMM1
               // vTemp = _mm_mul_ps(P1, Position1);
               MULPS   XMM2, [Position1]
               // vResult = _mm_add_ps(vResult,vTemp);
               ADDPS   XMM0, XMM2
               // vTemp = _mm_mul_ps(T1, Tangent1);
               MULPS   XMM3, [Tangent1]
               // vResult = _mm_add_ps(vResult,vTemp);
               ADDPS   XMM0, XMM3
               // return vResult;
               MOVUPS  [result], XMM0
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Result  :=  (2 * t^3 - 3 * t^2 + 1) * Position0 +
//          (t^3 - 2 * t^2 + t) * Tangent0 +
//          (-2 * t^3 + 3 * t^2) * Position1 +
//          (t^3 - t^2) * Tangent1
function XMVectorHermiteV(constref Position0: TXMVECTOR; constref Tangent0: TXMVECTOR; constref Position1: TXMVECTOR; constref Tangent1: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR;
var
    T2, T3, P0, T0, P1, T1: TXMVECTOR;
begin
    T2 := XMVectorMultiply(T, T);
    T3 := XMVectorMultiply(T, T2);

    P0 := XMVectorReplicate(2.0 * T3.f32[0] - 3.0 * T2.f32[0] + 1.0);
    T0 := XMVectorReplicate(T3.f32[1] - 2.0 * T2.f32[1] + T.f32[1]);
    P1 := XMVectorReplicate(-2.0 * T3.f32[2] + 3.0 * T2.f32[2]);
    T1 := XMVectorReplicate(T3.f32[3] - T2.f32[3]);

    Result := XMVectorMultiply(P0, Position0);
    Result := XMVectorMultiplyAdd(T0, Tangent0, Result);
    Result := XMVectorMultiplyAdd(P1, Position1, Result);
    Result := XMVectorMultiplyAdd(T1, Tangent1, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorHermiteV(Position0: TXMVECTOR; Tangent0: TXMVECTOR; Position1: TXMVECTOR; Tangent1: TXMVECTOR; T: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     static const XMVECTORF32 CatMulT2 = { { { -3.0f, -2.0f, 3.0f, -1.0f } } };
    static const XMVECTORF32 CatMulT3 = { { { 2.0f, 1.0f, -2.0f, 1.0f } } };

    XMVECTOR T2 = vmulq_f32(T,T);
    XMVECTOR T3 = vmulq_f32(T,T2);
    // Mul by the constants against t^2
    T2 = vmulq_f32(T2,CatMulT2);
    // Mul by the constants against t^3
    T3 = vmlaq_f32(T2, T3, CatMulT3 );
    // T3 now has the pre-result.
    // I need to add t.y only
    T2 = vandq_u32(T,g_XMMaskY);
    T3 = vaddq_f32(T3,T2);
    // Add 1.0f to x
    T3 = vaddq_f32(T3,g_XMIdentityR0);
    // Now, I have the constants created
    // Mul the x constant to Position0
    XMVECTOR vResult = vmulq_lane_f32( Position0, vget_low_f32( T3 ), 0 ); // T3[0]
    // Mul the y constant to Tangent0
    vResult = vmlaq_lane_f32(vResult, Tangent0, vget_low_f32( T3 ), 1 ); // T3[1]
    // Mul the z constant to Position1
    vResult = vmlaq_lane_f32(vResult, Position1, vget_high_f32( T3 ), 0  ); // T3[2]
    // Mul the w constant to Tangent1
    vResult = vmlaq_lane_f32(vResult, Tangent1, vget_high_f32( T3 ), 1 ); // T3[3]
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorHermiteV(constref Position0: TXMVECTOR; constref Tangent0: TXMVECTOR; constref Position1: TXMVECTOR; constref Tangent1: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR; assembler;
const
    CatMulT2: TXMVECTORF32 = (f: (-3.0, -2.0, 3.0, -1.0));
    CatMulT3: TXMVECTORF32 = (f: (2.0, 1.0, -2.0, 1.0));
asm
           //  T2 = _mm_mul_ps(T,T);
           MOVUPS  XMM0, [T]
           MULPS   XMM0, XMM0
           //  T3 = _mm_mul_ps(T,T2);
           MULPS   XMM1, [T]
           // Mul by the constants against t^2
           // T2 = _mm_mul_ps(T2,CatMulT2);
           MULPS   XMM0, [CatMulT2]
           // Mul by the constants against t^3
           // T3 = _mm_mul_ps(T3,CatMulT3);
           MULPS   XMM1, [CatMulT3]
           // T3 now has the pre-result.
           // T3 = _mm_add_ps(T3,T2);
           ADDPS   XMM1, XMM0
           // I need to add t.y only
           // T2 = _mm_and_ps(T,g_XMMaskY);
           MOVUPS  XMM0,[T]
           ANDPS   XMM0, [g_XMMaskY]
           // T3 = _mm_add_ps(T3,T2);
           ADDPS   XMM1, XMM0
           // Add 1.0f to x
           // T3 = _mm_add_ps(T3,g_XMIdentityR0);
           ADDPS   XMM1, [g_XMIdentityR0]
           // Now, I have the constants created
           // Mul the x constant to Position0
           //  vResult = XM_PERMUTE_PS(T3,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM5,XMM1
           SHUFPS  XMM5, XMM5, _MM_SHUFFLE_0_0_0_0
           // vResult = _mm_mul_ps(vResult,Position0);
           MULPS   XMM5, [Position0]
           // Mul the y constant to Tangent0
           // T2 = XM_PERMUTE_PS(T3,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM6,XMM1
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_1_1_1_1
           // T2 = _mm_mul_ps(T2,Tangent0);
           MULPS   XMM6, [Tangent0]
           // vResult = _mm_add_ps(vResult,T2);
           ADDPS   XMM5, XMM6
           // Mul the z constant to Position1
           // T2 = XM_PERMUTE_PS(T3,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM6,XMM1
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_2_2_2_2
           // T2 = _mm_mul_ps(T2,Position1);
           MULPS   XMM6, [Position1]
           // vResult = _mm_add_ps(vResult,T2);
           ADDPS   XMM5, XMM6
           // Mul the w constant to Tangent1
           // T3 = XM_PERMUTE_PS(T3,_MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM6,XMM1
           SHUFPS  XMM6, XMM6, _MM_SHUFFLE_3_3_3_3
           // T3 = _mm_mul_ps(T3,Tangent1);
           MULPS   XMM6, [Tangent1]
           // vResult = _mm_add_ps(vResult,T3);
           ADDPS   XMM5, XMM6
           // return vResult;
           MOVUPS  [result], XMM5
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Result  :=  ((-t^3 + 2 * t^2 - t) * Position0 +
//           (3 * t^3 - 5 * t^2 + 2) * Position1 +
//           (-3 * t^3 + 4 * t^2 + t) * Position2 +
//           (t^3 - t^2) * Position3) * 0.5
function XMVectorCatmullRom(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref Position3: TXMVECTOR; constref t: single): TXMVECTOR;
var
    t2, t3: single;
    P0, P1, P2, P3: TXMVECTOR;
begin
    t2 := t * t;
    t3 := t * t2;

    P0 := XMVectorReplicate((-t3 + 2.0 * t2 - t) * 0.5);
    P1 := XMVectorReplicate((3.0 * t3 - 5.0 * t2 + 2.0) * 0.5);
    P2 := XMVectorReplicate((-3.0 * t3 + 4.0 * t2 + t) * 0.5);
    P3 := XMVectorReplicate((t3 - t2) * 0.5);

    Result := XMVectorMultiply(P0, Position0);
    Result := XMVectorMultiplyAdd(P1, Position1, Result);
    Result := XMVectorMultiplyAdd(P2, Position2, Result);
    Result := XMVectorMultiplyAdd(P3, Position3, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorCatmullRom(Position0: TXMVECTOR; Position1: TXMVECTOR; Position2: TXMVECTOR; Position3: TXMVECTOR; t: single): TXMVECTOR;
begin
    (* ToDo
    float t2 = t * t;
    float t3 = t * t2;

    float p0 = (-t3 + 2.0f * t2 - t) * 0.5f;
    float p1 = (3.0f * t3 - 5.0f * t2 + 2.0f) * 0.5f;
    float p2 = (-3.0f * t3 + 4.0f * t2 + t) * 0.5f;
    float p3 = (t3 - t2) * 0.5f;

    XMVECTOR P1 = vmulq_n_f32(Position1, p1);
    XMVECTOR P0 = vmlaq_n_f32(P1, Position0, p0);
    XMVECTOR P3 = vmulq_n_f32(Position3, p3);
    XMVECTOR P2 = vmlaq_n_f32(P3, Position2, p2);
    P0 = vaddq_f32(P0,P2);
    return P0;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorCatmullRom(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref Position3: TXMVECTOR; constref t: single): TXMVECTOR; inline;
var
    t2, t3, f1, f2, f3, f4: single;
begin
    t2 := t * t;
    t3 := t * t2;
    f1 := (-t3 + 2.0 * t2 - t) * 0.5;
    f2 := (3.0 * t3 - 5.0 * t2 + 2.0) * 0.5;
    f3 := (-3.0 * t3 + 4.0 * t2 + t) * 0.5;
    f4 := (t3 - t2) * 0.5;
    asm
               //  P0 = _mm_set_ps1((-t3 + 2.0 * t2 - t) * 0.5);
               MOVSS   XMM0, [f1]
               SHUFPS  XMM0, XMM0, 0
               //  P1 = _mm_set_ps1((3.0f * t3 - 5.0f * t2 + 2.0f) * 0.5f);
               MOVSS   XMM1, [f2]
               SHUFPS  XMM1, XMM1, 0
               //  P2 = _mm_set_ps1((-3.0f * t3 + 4.0f * t2 + t) * 0.5f);
               MOVSS   XMM2, [f3]
               SHUFPS  XMM2, XMM2, 0
               //  P3 = _mm_set_ps1((t3 - t2) * 0.5f);
               MOVSS   XMM3, [f4]
               SHUFPS  XMM3, XMM3, 0

               // P0 = _mm_mul_ps(P0, Position0);
               MULPS   XMM0, [Position0]
               // P1 = _mm_mul_ps(P1, Position1);
               MULPS   XMM1, [Position1]
               // P2 = _mm_mul_ps(P2, Position2);
               MULPS   XMM2, [Position2]
               // P3 = _mm_mul_ps(P3, Position3);
               MULPS   XMM3, [Position3]
               // P0 = _mm_add_ps(P0,P1);
               ADDPS   XMM0, XMM1
               // P2 = _mm_add_ps(P2,P3);
               ADDPS   XMM2, XMM3
               // P0 = _mm_add_ps(P0,P2);
               ADDPS   XMM0, XMM2
               // return P0;
               MOVUPS  [result], XMM0

    end;
end;

{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVectorCatmullRomV(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref Position3: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR;
var
    fx, fy, fz, fw: single;
begin
    fx := T.f32[0];
    fy := T.f32[1];
    fz := T.f32[2];
    fw := T.f32[3];

    Result.f32[0] := 0.5 * ((-fx * fx * fx + 2 * fx * fx - fx) * Position0.f32[0] + (3 * fx * fx * fx - 5 * fx * fx + 2) * Position1.f32[0] + (-3 * fx * fx * fx + 4 * fx * fx + fx) *
        Position2.f32[0] + (fx * fx * fx - fx * fx) * Position3.f32[0]);

    Result.f32[1] := 0.5 * ((-fy * fy * fy + 2 * fy * fy - fy) * Position0.f32[1] + (3 * fy * fy * fy - 5 * fy * fy + 2) * Position1.f32[1] + (-3 * fy * fy * fy + 4 * fy * fy + fy) *
        Position2.f32[1] + (fy * fy * fy - fy * fy) * Position3.f32[1]);

    Result.f32[2] := 0.5 * ((-fz * fz * fz + 2 * fz * fz - fz) * Position0.f32[2] + (3 * fz * fz * fz - 5 * fz * fz + 2) * Position1.f32[2] + (-3 * fz * fz * fz + 4 * fz * fz + fz) *
        Position2.f32[2] + (fz * fz * fz - fz * fz) * Position3.f32[2]);

    Result.f32[3] := 0.5 * ((-fw * fw * fw + 2 * fw * fw - fw) * Position0.f32[3] + (3 * fw * fw * fw - 5 * fw * fw + 2) * Position1.f32[3] + (-3 * fw * fw * fw + 4 * fw * fw + fw) *
        Position2.f32[3] + (fw * fw * fw - fw * fw) * Position3.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorCatmullRomV(Position0: TXMVECTOR; Position1: TXMVECTOR; Position2: TXMVECTOR; Position3: TXMVECTOR; T: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   static const XMVECTORF32 Catmul2 = { { { 2.0f, 2.0f, 2.0f, 2.0f } } };
    static const XMVECTORF32 Catmul3 = { { { 3.0f, 3.0f, 3.0f, 3.0f } } };
    static const XMVECTORF32 Catmul4 = { { { 4.0f, 4.0f, 4.0f, 4.0f } } };
    static const XMVECTORF32 Catmul5 = { { { 5.0f, 5.0f, 5.0f, 5.0f } } };
    // Cache T^2 and T^3
    XMVECTOR T2 = vmulq_f32(T,T);
    XMVECTOR T3 = vmulq_f32(T,T2);
    // Perform the Position0 term
    XMVECTOR vResult = vaddq_f32(T2,T2);
    vResult = vsubq_f32(vResult,T);
    vResult = vsubq_f32(vResult,T3);
    vResult = vmulq_f32(vResult,Position0);
    // Perform the Position1 term and add
    XMVECTOR vTemp = vmulq_f32(T3,Catmul3);
    vTemp = vmlsq_f32(vTemp, T2, Catmul5);
    vTemp = vaddq_f32(vTemp,Catmul2);
    vResult = vmlaq_f32(vResult, vTemp, Position1);
    // Perform the Position2 term and add
    vTemp = vmulq_f32(T2,Catmul4);
    vTemp = vmlsq_f32(vTemp, T3, Catmul3);
    vTemp = vaddq_f32(vTemp,T);
    vResult = vmlaq_f32(vResult, vTemp, Position2);
    // Position3 is the last term
    T3 = vsubq_f32(T3,T2);
    vResult = vmlaq_f32(vResult, T3, Position3);
    // Multiply by 0.5f and exit
    vResult = vmulq_f32(vResult,g_XMOneHalf);
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorCatmullRomV(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref Position3: TXMVECTOR; constref T: TXMVECTOR): TXMVECTOR; assembler;
const
    Catmul2: TXMVECTORF32 = (f: (2.0, 2.0, 2.0, 2.0));
    Catmul3: TXMVECTORF32 = (f: (3.0, 3.0, 3.0, 3.0));
    Catmul4: TXMVECTORF32 = (f: (4.0, 4.0, 4.0, 4.0));
    Catmul5: TXMVECTORF32 = (f: (5.0, 5.0, 5.0, 5.0));
asm

           // Cache T^2 and T^3
           //  T2 = _mm_mul_ps(T,T);
           MOVUPS  XMM0,[T]
           MULPS   XMM0, XMM0
           //  T3 = _mm_mul_ps(T,T2);
           MOVUPS  XMM1,XMM0
           MULPS   XMM1, [T]
           // Perform the Position0 term
           //  vResult = _mm_add_ps(T2,T2);
           MOVUPS  XMM3, XMM0
           ADDPS   XMM3, XMM0
           // vResult = _mm_sub_ps(vResult,T);
           SUBPS   XMM3, [T]
           // vResult = _mm_sub_ps(vResult,T3);
           SUBPS   XMM3, XMM1
           // vResult = _mm_mul_ps(vResult,Position0);
           MULPS   XMM3, [Position0]
           // Perform the Position1 term and add
           //  vTemp = _mm_mul_ps(T3,Catmul3);
           MOVUPS  XMM4, XMM1
           MULPS   XMM4, [Catmul3]
           //  vTemp2 = _mm_mul_ps(T2,Catmul5);
           MOVUPS  XMM5, XMM0
           MULPS   XMM5, [Catmul5]
           // vTemp = _mm_sub_ps(vTemp,vTemp2);
           SUBPS   XMM4, XMM5
           // vTemp = _mm_add_ps(vTemp,Catmul2);
           ADDPS   XMM4, [Catmul2]
           // vTemp = _mm_mul_ps(vTemp,Position1);
           MULPS   XMM4, [Position1]
           // vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM3, XMM4
           // Perform the Position2 term and add
           // vTemp = _mm_mul_ps(T2,Catmul4);
           MOVUPS  XMM4,XMM0
           MULPS   XMM4, [Catmul4]
           // vTemp2 = _mm_mul_ps(T3,Catmul3);
           MOVUPS  XMM5, XMM1
           MULPS   XMM5, [Catmul3]
           // vTemp = _mm_sub_ps(vTemp,vTemp2);
           SUBPS   XMM4, XMM5
           // vTemp = _mm_add_ps(vTemp,T);
           ADDPS   XMM4, [T]
           // vTemp = _mm_mul_ps(vTemp,Position2);
           MULPS   XMM4, [Position2]
           // vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM3, XMM4
           // Position3 is the last term
           // T3 = _mm_sub_ps(T3,T2);
           SUBPS   XMM1, XMM0
           // T3 = _mm_mul_ps(T3,Position3);
           MULPS   XMM1, [Position3]
           // vResult = _mm_add_ps(vResult,T3);
           ADDPS   XMM3, XMM1
           // Multiply by 0.5f and exit
           // vResult = _mm_mul_ps(vResult,g_XMOneHalf);
           MULPS   XMM3, [g_XMOneHalf]
           // return vResult;
           MOVUPS  [result], XMM3
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Result  :=  Position0 + f * (Position1 - Position0) + g * (Position2 - Position0)
function XMVectorBaryCentric(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref f: single; constref g: single): TXMVECTOR;
var
    P10, ScaleF, P20, ScaleG: TXMVECTOR;
begin
    P10 := XMVectorSubtract(Position1, Position0);
    ScaleF := XMVectorReplicate(f);

    P20 := XMVectorSubtract(Position2, Position0);
    ScaleG := XMVectorReplicate(g);

    Result := XMVectorMultiplyAdd(P10, ScaleF, Position0);
    Result := XMVectorMultiplyAdd(P20, ScaleG, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorBaryCentric(Position0: TXMVECTOR; Position1: TXMVECTOR; Position2: TXMVECTOR; f: single; g: single): TXMVECTOR;
begin
    (* ToDo
     XMVECTOR R1 = vsubq_f32(Position1,Position0);
    XMVECTOR R2 = vsubq_f32(Position2,Position0);
    R1 = vmlaq_n_f32( Position0, R1, f);
    return vmlaq_n_f32( R1, R2, g );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorBaryCentric(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref f: single; constref g: single): TXMVECTOR; assembler;
asm
           // R1 = _mm_sub_ps(Position1,Position0);
           MOVUPS  XMM0, [Position1]
           SUBPS   XMM0, [Position0]
           //  SF = _mm_set_ps1(f);
           MOVSS   XMM1, [f]
           SHUFPS  XMM1, XMM1, 0
           //  R2 = _mm_sub_ps(Position2,Position0);
           MOVUPS  XMM2, [Position2]
           SUBPS   XMM2, [Position0]
           //  SG = _mm_set_ps1(g);
           MOVSS   XMM3, [g]
           SHUFPS  XMM3, XMM3, 0
           // R1 = _mm_mul_ps(R1,SF);
           MULPS   XMM0, XMM1
           // R2 = _mm_mul_ps(R2,SG);
           MULPS   XMM2, XMM3
           // R1 = _mm_add_ps(R1,Position0);
           ADDPS   XMM0, [Position0]
           // R1 = _mm_add_ps(R1,R2);
           ADDPS   XMM0, XMM2
           // return R1;
           MOVUPS  [result], XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Result  :=  Position0 + f * (Position1 - Position0) + g * (Position2 - Position0)
function XMVectorBaryCentricV(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref F: TXMVECTOR; constref G: TXMVECTOR): TXMVECTOR;
var
    P10, P20: TXMVECTOR;
begin
    P10 := XMVectorSubtract(Position1, Position0);
    P20 := XMVectorSubtract(Position2, Position0);

    Result := XMVectorMultiplyAdd(P10, F, Position0);
    Result := XMVectorMultiplyAdd(P20, G, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVectorBaryCentricV(Position0: TXMVECTOR; Position1: TXMVECTOR; Position2: TXMVECTOR; F: TXMVECTOR; G: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    XMVECTOR R1 = vsubq_f32(Position1,Position0);
    XMVECTOR R2 = vsubq_f32(Position2,Position0);
    R1 = vmlaq_f32( Position0, R1, F );
    return vmlaq_f32( R1, R2, G);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVectorBaryCentricV(constref Position0: TXMVECTOR; constref Position1: TXMVECTOR; constref Position2: TXMVECTOR; constref F: TXMVECTOR; constref G: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  R1 = _mm_sub_ps(Position1,Position0);
           MOVUPS  XMM0, [Position1]
           SUBPS   XMM0, [Position0]
           //  R2 = _mm_sub_ps(Position2,Position0);
           MOVUPS  XMM1, [Position2]
           SUBPS   XMM1, [Position0]
           // R1 = _mm_mul_ps(R1,F);
           MULPS   XMM0, [F]
           // R2 = _mm_mul_ps(R2,G);
           MULPS   XMM1, [G]
           // R1 = _mm_add_ps(R1,Position0);
           ADDPS   XMM0, [Position0]
           // R1 = _mm_add_ps(R1,R2);
           ADDPS   XMM0, XMM1
           // return R1;
           MOVUPS  [result], XMM0
end;
{$ENDIF}

{***************************************************************************
 *
 * 2D Vector
 *
 ***************************************************************************}

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] = V2.f32[0]) and (V1.f32[1] = V2.f32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x2_t vTemp = vceq_f32( vget_low_f32(V1), vget_low_f32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) == 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmpeq_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           CMPPS   XMM0, [V2], 0
           // z and w are don't care
           // return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] = V2.f32[0]) and (V1.f32[1] = V2.f32[1])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.f32[0] <> V2.f32[0]) and (V1.f32[1] <> V2.f32[1])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x2_t vTemp = vceq_f32( vget_low_f32(V1), vget_low_f32(V2) );
    uint64_t r = vget_lane_u64( vTemp, 0 );
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           // vTemp = _mm_cmpeq_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 0
           // z and w are don't care
           // int iTest = _mm_movemask_ps(vTemp)&3;
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished

           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $3
           CMP     EAX, $3
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.u32[0] = V2.u32[0]) and (V1.u32[1] = V2.u32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x2_t vTemp = vceq_u32( vget_low_u32(V1), vget_low_u32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) == 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           MOVUPS  XMM0,[V1]
           PCMPEQD XMM0, [V2]
           // return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&3)==3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.u32[0] = V2.u32[0]) and (V1.u32[1] = V2.u32[1])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.u32[0] <> V2.u32[0]) and (V1.u32[1] <> V2.u32[1])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x2_t vTemp = vceq_u32( vget_low_u32(V1), vget_low_u32(V2) );
    uint64_t r = vget_lane_u64( vTemp, 0 );
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; // assembler
asm
           // vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           MOVUPS  XMM0, [V1]
           PCMPEQD XMM0, [V2]
           // int iTest = _mm_movemask_ps(_mm_castsi128_ps(vTemp))&3;
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished

           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $3
           CMP     EAX, $3
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
var
    dx, dy: single;
begin
    dx := abs(V1.f32[0] - V2.f32[0]);
    dy := abs(V1.f32[1] - V2.f32[1]);
    Result := ((dx <= Epsilon.f32[0]) and (dy <= Epsilon.f32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
begin
    (* ToDo
     float32x2_t vDelta = vsub_f32(vget_low_u32(V1), vget_low_u32(V2));
    uint32x2_t vTemp = vacle_f32( vDelta, vget_low_u32(Epsilon) );
    uint64_t r = vget_lane_u64( vTemp, 0 );
    return ( r == 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean; assembler;
asm
           // Get the difference
           //  vDelta = _mm_sub_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           SUBPS   XMM0, [V2]
           // Get the absolute value of the difference
           // vTemp = _mm_setzero_ps();
           XORPS   XMM1,XMM1
           // vTemp = _mm_sub_ps(vTemp,vDelta);
           SUBPS   XMM1, XMM0
           // vTemp = _mm_max_ps(vTemp,vDelta);
           MAXPS   XMM1, XMM0
           // vTemp = _mm_cmple_ps(vTemp,Epsilon);
           CMPPS   XMM1, [Epsilon], 2
           // z and w are don't care
           // return (((_mm_movemask_ps(vTemp)&3)==0x3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] <> V2.f32[0]) or (V1.f32[1] <> V2.f32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
     uint32x2_t vTemp = vceq_f32( vget_low_f32(V1), vget_low_f32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) != 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmpeq_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           CMPPS   XMM0, [V2], 0
           // z and w are don't care
           // return (((_mm_movemask_ps(vTemp)&3)!=3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.u32[0] <> V2.u32[0]) or (V1.u32[1] <> V2.u32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x2_t vTemp = vceq_u32( vget_low_u32(V1), vget_low_u32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) != 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           MOVUPS  XMM0,[V1]
           PCMPEQD XMM0, [V2]
           //return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&3)!=3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] > V2.f32[0]) and (V1.f32[1] > V2.f32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x2_t vTemp = vcgt_f32( vget_low_f32(V1), vget_low_f32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) == 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmpgt_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           CMPPS   XMM0, [V2], 1
           // z and w are don't care
           // return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] > V2.f32[0]) and (V1.f32[1] > V2.f32[1])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.f32[0] <= V2.f32[0]) and (V1.f32[1] <= V2.f32[1])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x2_t vTemp = vcgt_f32( vget_low_f32(V1), vget_low_f32(V2) );
    uint64_t r = vget_lane_u64( vTemp, 0 );
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           // vTemp = _mm_cmpgt_ps(V1,V2);
           MOVUPS  XMM0, [V2]
           CMPPS   XMM0, [V1], 1
           // int iTest = _mm_movemask_ps(vTemp)&3;
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished

           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $3
           CMP     EAX, $3
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] >= V2.f32[0]) and (V1.f32[1] >= V2.f32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
   uint32x2_t vTemp = vcge_f32( vget_low_f32(V1), vget_low_f32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) == 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmpge_ps(V1,V2);
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 2
           // return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] >= V2.f32[0]) and (V1.f32[1] >= V2.f32[1])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.f32[0] < V2.f32[0]) and (V1.f32[1] < V2.f32[1])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x2_t vTemp = vcge_f32( vget_low_f32(V1), vget_low_f32(V2) );
    uint64_t r = vget_lane_u64( vTemp, 0 );
    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           // vTemp = _mm_cmpge_ps(V1,V2);
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 2
           // int iTest = _mm_movemask_ps(vTemp)&3;
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished

           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $3
           CMP     EAX, $3
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := (V1.f32[0] < V2.f32[0]) and (V1.f32[1] < V2.f32[1]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
    uint32x2_t vTemp = vclt_f32( vget_low_f32(V1), vget_low_f32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) == 0xFFFFFFFFFFFFFFFFU );
    *)
end;

{$ELSE}
function XMVector2Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           MOVUPS  XMM0,[v1]
           CMPPS   XMM0, [v2] , 1
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := (V1.f32[0] <= V2.f32[0]) and (V1.f32[1] <= V2.f32[1]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
     uint32x2_t vTemp = vcle_f32( vget_low_f32(V1), vget_low_f32(V2) );
    return ( vget_lane_u64( vTemp, 0 ) == 0xFFFFFFFFFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVector2LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           VMOVUPS XMM0,[v1]
           VCMPPS  XMM0, XMM0, [v2], 2
           VMOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           MOVUPS  XMM1,[v1]
           CMPPS   XMM1, [v2], 2
           MOVMSKPS EAX, XMM1
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    Result := ((V.f32[0] <= Bounds.f32[0]) and (V.f32[0] >= -Bounds.f32[0]) and (V.f32[1] <= Bounds.f32[1]) and (V.f32[1] >= -Bounds.f32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    (* ToDo
    float32x2_t VL = vget_low_f32( V );
    float32x2_t B = vget_low_f32( Bounds );
    // Test if less than or equal
    uint32x2_t ivTemp1 = vcle_f32(VL,B);
    // Negate the bounds
    float32x2_t vTemp2 = vneg_f32(B);
    // Test if greater or equal (Reversed)
    uint32x2_t ivTemp2 = vcle_f32(vTemp2,VL);
    // Blend answers
    ivTemp1 = vand_u32(ivTemp1,ivTemp2);
    // x and y in bounds?
    return ( vget_lane_u64( ivTemp1, 0 ) == 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean; assembler;
asm
           // Test if less than or equal
           // XMVECTOR vTemp1 = _mm_cmple_ps(V,Bounds);
           MOVUPS  XMM0, [V]
           CMPPS   XMM0, [Bounds], 2
           // Negate the bounds
           // XMVECTOR vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
           MOVUPS  XMM1, [Bounds]
           MULPS   XMM1, [g_XMNegativeOne]
           // Test if greater or equal (Reversed)
           // vTemp2 = _mm_cmple_ps(vTemp2,V);
           CMPPS   XMM1, [V], 2
           // Blend answers
           // vTemp1 = _mm_and_ps(vTemp1,vTemp2);
           ANDPS   XMM0, XMM1
           // x and y in bounds? (z and w are don't care)
           // return (((_mm_movemask_ps(vTemp1)&0x3)==0x3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2IsNaN(constref V: TXMVECTOR): boolean;
begin
    Result := (XMISNAN(V.u32[0]) or XMISNAN(V.u32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2IsNaN(constref V: TXMVECTOR): boolean;
begin
    (* ToDo
    float32x2_t VL = vget_low_f32( V );
    // Test against itself. NaN is always not equal
    uint32x2_t vTempNan = vceq_f32( VL, VL );
    // If x or y are NaN, the mask is zero
    return ( vget_lane_u64( vTempNan, 0 ) != 0xFFFFFFFFFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2IsNaN(constref V: TXMVECTOR): boolean; assembler;
asm
           // Test against itself. NaN is always not equal
           //  vTempNan = _mm_cmpneq_ps(V,V);
           MOVUPS  XMM0,[V]
           CMPPS   XMM0, [V], 4
           // If x or y are NaN, the mask is non-zero
           // return ((_mm_movemask_ps(vTempNan)&3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2IsInfinite(constref V: TXMVECTOR): boolean;
begin
    Result := (XMISINF(V.u32[0]) or XMISINF(V.u32[1]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2IsInfinite(constref V: TXMVECTOR): boolean;
begin
    (* ToDo
    // Mask off the sign bit
    uint32x2_t vTemp = vand_u32( vget_low_f32( V ) , vget_low_f32( g_XMAbsMask ) );
    // Compare to infinity
    vTemp = vceq_f32(vTemp, vget_low_f32( g_XMInfinity) );
    // If any are infinity, the signs are true.
    return vget_lane_u64( vTemp, 0 ) != 0;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2IsInfinite(constref V: TXMVECTOR): boolean; assembler;
asm
           // Mask off the sign bit
           // vTemp = _mm_and_ps(V,g_XMAbsMask);
           MOVUPS  XMM0,[V]
           ANDPS   XMM0, [g_XMAbsMask]
           // Compare to infinity
           // vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 0
           // If x or z are infinity, the signs are true.
           // return ((_mm_movemask_ps(vTemp)&3) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $3
           CMP     EAX, $3
           SETE    [result]
end;
{$ENDIF}


//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    f: single;
begin
    f := V1.f32[0] * V2.f32[0] + V1.f32[1] * V2.f32[1];
    Result.f32[0] := f;
    Result.f32[1] := f;
    Result.f32[2] := f;
    Result.f32[3] := f;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Perform the dot product on x and y
    float32x2_t vTemp = vmul_f32( vget_low_f32(V1), vget_low_f32(V2) );
    vTemp = vpadd_f32( vTemp, vTemp );
    return vcombine_f32( vTemp, vTemp );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector2Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // return _mm_dp_ps( V1, V2, $3f );
           MOVUPS  XMM0,[V1]
           DPPS    XMM0, [V2], $3f
           MOVUPS  [result], XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector2Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vDot = _mm_mul_ps(V1, V2);
           MOVUPS  XMM0,[V1]
           MULPS   XMM0, [V2]
           // vDot = _mm_hadd_ps(vDot, vDot);
           HADDPS  XMM0, XMM0
           // vDot = _mm_moveldup_ps(vDot);
           MOVSLDUP XMM0, XMM0
           // return vDot;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y
           //  vLengthSq = _mm_mul_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           MULPS   XMM0, [V2]
           // vTemp has y splatted
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1, XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}


// [ V1.x*V2.y - V1.y*V2.x, V1.x*V2.y - V1.y*V2.x ]
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    fCross: single;
begin
    fCross := (V1.f32[0] * V2.f32[1]) - (V1.f32[1] * V2.f32[0]);
    Result.f32[0] := fCross;
    Result.f32[1] := fCross;
    Result.f32[2] := fCross;
    Result.f32[3] := fCross;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    static const XMVECTORF32 Negate = { { { 1.f, -1.f, 0, 0 } } };

    float32x2_t vTemp = vmul_f32( vget_low_f32( V1 ), vrev64_f32( vget_low_f32( V2 ) ) );
    vTemp = vmul_f32( vTemp, vget_low_f32( Negate ) );
    vTemp = vpadd_f32( vTemp, vTemp );
    return vcombine_f32( vTemp, vTemp );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Swap x and y
           // vResult = XM_PERMUTE_PS(V2,_MM_SHUFFLE(0,1,0,1));
           MOVUPS  XMM0, [V2]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_1_0_1
           // Perform the muls
           // vResult = _mm_mul_ps(vResult,V1);
           MULPS   XMM0, [V1]
           // Splat y
           //  vTemp = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // Sub the values
           // vResult = _mm_sub_ss(vResult,vTemp);
           SUBPS   XMM0, XMM1
           // Splat the cross product
           // vResult = XM_PERMUTE_PS(vResult,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}


function XMVector2LengthSq(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2Dot(V, V);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2LengthSq(V);
    Result := XMVectorReciprocalSqrtEst(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     float32x2_t VL = vget_low_f32(V);
    // Dot2
    float32x2_t vTemp = vmul_f32( VL, VL );
    vTemp = vpadd_f32( vTemp, vTemp );
    // Reciprocal sqrt (estimate)
    vTemp = vrsqrte_f32( vTemp );
    return vcombine_f32( vTemp, vTemp );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector2ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x3f );
           MOVUPS  XMM0, [V]
           DPPS    XMM0, XMM0, $3F
           // return _mm_rsqrt_ps( vTemp );
           RSQRTPS XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector2ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           //  vTemp = _mm_hadd_ps(vLengthSq, vLengthSq);
           MOVUPS  XMM1,XMM0
           HADDPS  XMM1, XMM1
           // vLengthSq = _mm_rsqrt_ss(vTemp);
           RSQRTSS XMM0, XMM1
           // vLengthSq = XM_PERMUTE_PS(vLengthSq, _MM_SHUFFLE(0, 0, 0, 0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has y splatted
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1, XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDPS   XMM0,XMM1
           // vLengthSq = _mm_rsqrt_ss(vLengthSq);
           RSQRTSS XMM0, XMM0
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2LengthSq(V);
    Result := XMVectorReciprocalSqrt(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    float32x2_t VL = vget_low_f32(V);
    // Dot2
    float32x2_t vTemp = vmul_f32( VL, VL );
    vTemp = vpadd_f32( vTemp, vTemp );
    // Reciprocal sqrt
    float32x2_t  S0 = vrsqrte_f32(vTemp);
    float32x2_t  P0 = vmul_f32( vTemp, S0 );
    float32x2_t  R0 = vrsqrts_f32( P0, S0 );
    float32x2_t  S1 = vmul_f32( S0, R0 );
    float32x2_t  P1 = vmul_f32( vTemp, S1 );
    float32x2_t  R1 = vrsqrts_f32( P1, S1 );
    float32x2_t Result = vmul_f32( S1, R1 );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector2ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x3f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $3F
           //  vLengthSq = _mm_sqrt_ps( vTemp );
           SQRTPS  XMM1, XMM0
           // return _mm_div_ps( g_XMOne, vLengthSq );
           MOVUPS  XMM0,[g_XMOne]
           DIVPS   XMM0, XMM1
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector2ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp = _mm_hadd_ps(vLengthSq, vLengthSq);
           MOVUPS  XMM1,XMM0
           HADDPS  XMM1, XMM1
           //vLengthSq = _mm_sqrt_ss(vTemp);
           SQRTSS  XMM1, XMM1
           //vLengthSq = _mm_div_ss(g_XMOne, vLengthSq);
           MOVUPS  XMM0 , [g_XMOne]
           DIVPS   XMM0 ,XMM1
           //vLengthSq = XM_PERMUTE_PS(vLengthSq, _MM_SHUFFLE(0, 0, 0, 0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           //return vLengthSq;
           MOVUPS  [result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y
           // vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0, [V]
           MULPS   XMM0, XMM0
           // vTemp has y splatted
           // vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1, XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDPS   XMM0,XMM1
           // vLengthSq = _mm_sqrt_ss(vLengthSq);
           SQRTSS  XMM0, XMM0
           // vLengthSq = _mm_div_ss(g_XMOne,vLengthSq);
           MOVUPS  XMM1, [g_XMOne]
           DIVSS   XMM1, XMM0
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0
           // return vLengthSq;
           MOVUPS  [result],XMM1
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2LengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2LengthSq(V);
    Result := XMVectorSqrtEst(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2LengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    float32x2_t VL = vget_low_f32(V);
    // Dot2
    float32x2_t vTemp = vmul_f32( VL, VL );
    vTemp = vpadd_f32( vTemp, vTemp );
    const float32x2_t zero = vdup_n_f32(0);
    uint32x2_t VEqualsZero = vceq_f32( vTemp, zero );
    // Sqrt (estimate)
    float32x2_t Result = vrsqrte_f32( vTemp );
    Result = vmul_f32( vTemp, Result );
    Result = vbsl_f32( VEqualsZero, zero, Result );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector2LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x3f );
           MOVUPS  XMM0, [V]
           DPPS    XMM0, XMM0, $3F
           // return _mm_sqrt_ps( vTemp );
           SQRTPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector2LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp = _mm_hadd_ps(vLengthSq, vLengthSq);
           MOVUPS  XMM1,XMM0
           HADDPS  XMM1, XMM1
           //vLengthSq = _mm_sqrt_ss(vTemp);
           SQRTSS  XMM0, XMM1
           //vLengthSq = XM_PERMUTE_PS(vLengthSq, _MM_SHUFFLE(0, 0, 0, 0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           //return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y
           // vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0, [V]
           MULPS   XMM0, XMM0
           // vTemp has y splatted
           //XMVECTOR vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+y
           //vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           //vLengthSq = _mm_sqrt_ss(vLengthSq);
           SQRTSS  XMM0, XMM0
           //vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           //return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Length(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2LengthSq(V);
    Result := XMVectorSqrt(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Length(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     float32x2_t VL = vget_low_f32(V);
    // Dot2
    float32x2_t vTemp = vmul_f32( VL, VL );
    vTemp = vpadd_f32( vTemp, vTemp );
    const float32x2_t zero = vdup_n_f32(0);
    uint32x2_t VEqualsZero = vceq_f32( vTemp, zero );
    // Sqrt
    float32x2_t S0 = vrsqrte_f32( vTemp );
    float32x2_t P0 = vmul_f32( vTemp, S0 );
    float32x2_t R0 = vrsqrts_f32( P0, S0 );
    float32x2_t S1 = vmul_f32( S0, R0 );
    float32x2_t P1 = vmul_f32( vTemp, S1 );
    float32x2_t R1 = vrsqrts_f32( P1, S1 );
    float32x2_t Result = vmul_f32( S1, R1 );
    Result = vmul_f32( vTemp, Result );
    Result = vbsl_f32( VEqualsZero, zero, Result );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector2Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x3f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $3F
           // return _mm_sqrt_ps( vTemp );
           SQRTPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector2Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           //vLengthSq = _mm_sqrt_ss(vTemp);
           SQRTSS  XMM0, XMM0
           //vLengthSq = XM_PERMUTE_PS(vLengthSq, _MM_SHUFFLE(0, 0, 0, 0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           //return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y
           // vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0, [V]
           MULPS   XMM0, XMM0
           // vTemp has y splatted
           // vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}


//------------------------------------------------------------------------------
// XMVector2NormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2ReciprocalLength(V);
    Result := XMVectorMultiply(V, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
   float32x2_t VL = vget_low_f32(V);
    // Dot2
    float32x2_t vTemp = vmul_f32( VL, VL );
    vTemp = vpadd_f32( vTemp, vTemp );
    // Reciprocal sqrt (estimate)
    vTemp = vrsqrte_f32( vTemp );
    // Normalize
    float32x2_t Result = vmul_f32( VL, vTemp );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector2NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x3f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $3F
           //  vResult = _mm_rsqrt_ps( vTemp );
           RSQRTPS XMM0, XMM0
           // return _mm_mul_ps(vResult, V);
           MULPS   XMM0, [V]
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector2NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_rsqrt_ss(vLengthSq);
           RSQRTSS XMM0, XMM0
           // vLengthSq = XM_PERMUTE_PS(vLengthSq, _MM_SHUFFLE(0, 0, 0, 0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vLengthSq = _mm_mul_ps(vLengthSq, V);
           MULPS   XMM0, [V]
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y
           // vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has y splatted
           // vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // vLengthSq = _mm_rsqrt_ss(vLengthSq);
           RSQRTSS XMM0, XMM0
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vLengthSq = _mm_mul_ps(vLengthSq,V);
           MULPS   XMM0, [V]
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Normalize(constref V: TXMVECTOR): TXMVECTOR;
var
    fLength: single;
    vResult: TXMVECTOR;
begin
    vResult := XMVector2Length(V);
    fLength := vResult.f32[0];

    // Prevent divide by zero
    if (fLength > 0) then
        fLength := 1.0 / fLength;

    Result.f32[0] := V.f32[0] * fLength;
    Result.f32[1] := V.f32[1] * fLength;
    Result.f32[2] := V.f32[2] * fLength;
    Result.f32[3] := V.f32[3] * fLength;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Normalize(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    float32x2_t VL = vget_low_f32(V);
    // Dot2
    float32x2_t vTemp = vmul_f32( VL, VL );
    vTemp = vpadd_f32( vTemp, vTemp );
    uint32x2_t VEqualsZero = vceq_f32( vTemp, vdup_n_f32(0) );
    uint32x2_t VEqualsInf = vceq_f32( vTemp, vget_low_f32(g_XMInfinity) );
    // Reciprocal sqrt (2 iterations of Newton-Raphson)
    float32x2_t S0 = vrsqrte_f32( vTemp );
    float32x2_t P0 = vmul_f32( vTemp, S0 );
    float32x2_t R0 = vrsqrts_f32( P0, S0 );
    float32x2_t S1 = vmul_f32( S0, R0 );
    float32x2_t P1 = vmul_f32( vTemp, S1 );
    float32x2_t R1 = vrsqrts_f32( P1, S1 );
    vTemp = vmul_f32( S1, R1 );
    // Normalize
    float32x2_t Result = vmul_f32( VL, vTemp );
    Result = vbsl_f32( VEqualsZero, vdup_n_f32(0), Result );
    Result = vbsl_f32( VEqualsInf, vget_low_f32(g_XMQNaN), Result );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector2Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_dp_ps( V, V, 0x3f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $3F
           // Prepare for the division
           // vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM2, XMM0
           // Create zero with a single instruction
           // vZeroMask = _mm_setzero_ps();
           XORPS   XMM1,XMM1
           // Test for a divide by zero (Must be FP to detect -0.0)
           //vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM1, XMM2, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           //vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Reciprocal mul to perform the normalization
           //vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM3, [V]
           DIVPS   XMM3, XMM2
           // Any that are infinity, set to zero
           //vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM3, XMM1
           // Select qnan or result based on infinite length
           // vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM1,[g_XMQNaN]
           ANDNPS  XMM1, XMM0
           // vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM3, XMM0
           //vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM1, XMM3
           // return vResult;
           MOVUPS  [result],XMM3
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector2Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y only
           // vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0 ,[V]
           MULPS   XMM0, XMM0
           //vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           //vLengthSq = _mm_moveldup_ps(vLengthSq);
           MOVSLDUP XMM0, XMM0
           // Prepare for the division
           // vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Create zero with a single instruction
           // vZeroMask = _mm_setzero_ps();
           XORPS   XMM2,XMM2
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask, vResult);
           CMPPS   XMM2, XMM1, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           //vLengthSq = _mm_cmpneq_ps(vLengthSq, g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Reciprocal mul to perform the normalization
           //vResult = _mm_div_ps(V, vResult);
           MOVUPS  XMM3, [V]
           DIVPS   XMM3, XMM1
           // Any that are infinity, set to zero
           //vResult = _mm_and_ps(vResult, vZeroMask);
           ANDPS   XMM1, XMM2
           // Select qnan or result based on infinite length
           // vTemp1 = _mm_andnot_ps(vLengthSq, g_XMQNaN);
           MOVUPS  XMM2, [g_XMQNaN]
           ANDNPS  XMM2, XMM0
           // vTemp2 = _mm_and_ps(vResult, vLengthSq);
           ANDPS   XMM0, XMM1
           //vResult = _mm_or_ps(vTemp1, vTemp2);
           ORPS    XMM0, XMM2
           //return vResult;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x and y only
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Create zero with a single instruction
           //  vZeroMask = _mm_setzero_ps();
           XORPS   XMM2,XMM2
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM2, XMM1, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Reciprocal mul to perform the normalization
           // vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM3, [V]
           DIVPS   XMM3, XMM1
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM3, XMM2
           // Select qnan or result based on infinite length
           //  vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM7,XMM0
           ANDNPS  XMM7, [g_XMQNaN]
           //  vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM3, XMM0
           // vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM3, XMM7
           // return vResult;
           MOVUPS  [result],XMM3
end;
{$ENDIF}

function XMVector2ClampLength(constref V: TXMVECTOR; constref LengthMin: single; constref LengthMax: single): TXMVECTOR;
var
    ClampMax, ClampMin: TXMVECTOR;
begin
    ClampMax := XMVectorReplicate(LengthMax);
    ClampMin := XMVectorReplicate(LengthMin);
    Result := XMVector2ClampLengthV(V, ClampMin, ClampMax);
end;


function XMVector2ClampLengthV(constref V: TXMVECTOR; constref LengthMin: TXMVECTOR; constref LengthMax: TXMVECTOR): TXMVECTOR;
var
    LengthSq, Zero, RcpLength, InfiniteLength, ZeroLength, Length, Normal, Select: TXMVECTOR;
    ControlMax, ControlMin, ClampLength, Control: TXMVECTOR;
begin
    assert((XMVectorGetY(LengthMin) = XMVectorGetX(LengthMin)));
    assert((XMVectorGetY(LengthMax) = XMVectorGetX(LengthMax)));
    assert(XMVector2GreaterOrEqual(LengthMin, g_XMZero));
    assert(XMVector2GreaterOrEqual(LengthMax, g_XMZero));
    assert(XMVector2GreaterOrEqual(LengthMax, LengthMin));

    LengthSq := XMVector2LengthSq(V);

    Zero := XMVectorZero();

    RcpLength := XMVectorReciprocalSqrt(LengthSq);

    InfiniteLength := XMVectorEqualInt(LengthSq, g_XMInfinity.v);
    ZeroLength := XMVectorEqual(LengthSq, Zero);

    Length := XMVectorMultiply(LengthSq, RcpLength);

    Normal := XMVectorMultiply(V, RcpLength);

    Select := XMVectorEqualInt(InfiniteLength, ZeroLength);
    Length := XMVectorSelect(LengthSq, Length, Select);
    Normal := XMVectorSelect(LengthSq, Normal, Select);


    ControlMax := XMVectorGreater(Length, LengthMax);
    ControlMin := XMVectorLess(Length, LengthMin);

    ClampLength := XMVectorSelect(Length, LengthMax, ControlMax);
    ClampLength := XMVectorSelect(ClampLength, LengthMin, ControlMin);

    Result := XMVectorMultiply(Normal, ClampLength);

    // Preserve the original vector (with no precision loss) if the length falls within the given range
    Control := XMVectorEqualInt(ControlMax, ControlMin);
    Result := XMVectorSelect(Result, V, Control);
end;


function XMVector2Reflect(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR): TXMVECTOR;
begin
    // Result  :=  Incident - (2 * dot(Incident, Normal)) * Normal
    Result := XMVector2Dot(Incident, Normal);
    Result := XMVectorAdd(Result, Result);
    Result := XMVectorNegativeMultiplySubtract(Result, Normal, Incident);
end;


function XMVector2Refract(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: single): TXMVECTOR;
var
    Index: TXMVECTOR;
begin
    Index := XMVectorReplicate(RefractionIndex);
    Result := XMVector2RefractV(Incident, Normal, Index);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
// Return the refraction of a 2D vector
// Result  :=  RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
// sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))
function XMVector2RefractV(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: TXMVECTOR): TXMVECTOR;
var
    IDotN, RX, RY: single;
begin
    IDotN := (Incident.f32[0] * Normal.f32[0]) + (Incident.f32[1] * Normal.f32[1]);
    // R  :=  1.0  - RefractionIndex * RefractionIndex * (1.0  - IDotN * IDotN)
    RY := 1.0 - (IDotN * IDotN);
    RX := 1.0 - (RY * RefractionIndex.f32[0] * RefractionIndex.f32[0]);
    RY := 1.0 - (RY * RefractionIndex.f32[1] * RefractionIndex.f32[1]);
    if (RX >= 0.0) then
    begin
        RX := (RefractionIndex.f32[0] * Incident.f32[0]) - (Normal.f32[0] * ((RefractionIndex.f32[0] * IDotN) + sqrt(RX)));
    end
    else
    begin
        RX := 0.0;
    end;
    if (RY >= 0.0) then
    begin
        RY := (RefractionIndex.f32[1] * Incident.f32[1]) - (Normal.f32[1] * ((RefractionIndex.f32[1] * IDotN) + sqrt(RY)));
    end
    else
    begin
        RY := 0.0;
    end;

    Result.f32[0] := RX;
    Result.f32[1] := RY;
    Result.f32[2] := 0.0;
    Result.f32[3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2RefractV(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     float32x2_t IL = vget_low_f32( Incident );
    float32x2_t NL = vget_low_f32( Normal );
    float32x2_t RIL = vget_low_f32( RefractionIndex );
    // Get the 2D Dot product of Incident-Normal
    float32x2_t vTemp = vmul_f32(IL, NL);
    float32x2_t IDotN = vpadd_f32( vTemp, vTemp );
    // vTemp = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    vTemp = vmls_f32( vget_low_f32( g_XMOne ), IDotN, IDotN);
    vTemp = vmul_f32(vTemp,RIL);
    vTemp = vmls_f32(vget_low_f32( g_XMOne ), vTemp, RIL );
    // If any terms are <=0, sqrt() will fail, punt to zero
    uint32x2_t vMask = vcgt_f32(vTemp, vget_low_f32(g_XMZero) );
    // Sqrt(vTemp)
    float32x2_t S0 = vrsqrte_f32(vTemp);
    float32x2_t P0 = vmul_f32( vTemp, S0 );
    float32x2_t R0 = vrsqrts_f32( P0, S0 );
    float32x2_t S1 = vmul_f32( S0, R0 );
    float32x2_t P1 = vmul_f32( vTemp, S1 );
    float32x2_t R1 = vrsqrts_f32( P1, S1 );
    float32x2_t S2 = vmul_f32( S1, R1 );
    vTemp = vmul_f32( vTemp, S2 );
    // R = RefractionIndex * IDotN + sqrt(R)
    vTemp = vmla_f32( vTemp, RIL, IDotN );
    // Result = RefractionIndex * Incident - Normal * R
    float32x2_t vResult = vmul_f32(RIL,IL);
    vResult = vmls_f32( vResult, vTemp, NL );
    vResult = vand_u32(vResult,vMask);
    return vcombine_f32(vResult, vResult);
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2RefractV(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: TXMVECTOR): TXMVECTOR; inline;
var
    IDotN: TXMVECTOR;
begin
    // Result = RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))
    // Get the 2D Dot product of Incident-Normal
    IDotN := XMVector2Dot(Incident, Normal);
    asm
               // vTemp = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
               //  vTemp = _mm_mul_ps(IDotN,IDotN);
               MOVUPS  XMM0, [IDotN]
               MULPS   XMM0, XMM0
               // vTemp = _mm_sub_ps(g_XMOne,vTemp);
               MOVUPS  XMM1, [g_XMOne]
               SUBPS   XMM1, XMM0
               // vTemp = _mm_mul_ps(vTemp,RefractionIndex);
               MULPS   XMM1, [RefractionIndex]
               // vTemp = _mm_mul_ps(vTemp,RefractionIndex);
               MULPS   XMM1, [RefractionIndex]
               // vTemp = _mm_sub_ps(g_XMOne,vTemp);
               MOVUPS  XMM0, [g_XMOne]
               SUBPS   XMM0, XMM1
               // If any terms are <=0, sqrt() will fail, punt to zero
               //  vMask = _mm_cmpgt_ps(vTemp,g_XMZero);
               MOVUPS  XMM2,[g_XMZero]
               CMPPS   XMM2, XMM0, 1
               // R = RefractionIndex * IDotN + sqrt(R)
               // vTemp = _mm_sqrt_ps(vTemp);
               SQRTPS  XMM0, XMM0
               //  vResult = _mm_mul_ps(RefractionIndex,IDotN);
               MOVUPS  XMM3, [RefractionIndex]
               MULPS   XMM2, [IDotN]
               // vTemp = _mm_add_ps(vTemp,vResult);
               ADDPS   XMM0, XMM3
               // Result = RefractionIndex * Incident - Normal * R
               // vResult = _mm_mul_ps(RefractionIndex,Incident);
               MOVUPS  XMM3, [RefractionIndex]
               MULPS   XMM3, [Incident]
               // vTemp = _mm_mul_ps(vTemp,Normal);
               MULPS   XMM0, [Normal]
               // vResult = _mm_sub_ps(vResult,vTemp);
               SUBPS   XMM3, XMM0
               // vResult = _mm_and_ps(vResult,vMask);
               ANDPS   XMM3, XMM2
               // return vResult;
               MOVUPS  [result],XMM3

    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Orthogonal(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := -V.f32[1];
    Result.f32[1] := V.f32[0];
    Result.f32[2] := 0.0;
    Result.f32[3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Orthogonal(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    static const XMVECTORF32 Negate = { { { -1.f, 1.f, 0, 0 } } };
    const float32x2_t zero = vdup_n_f32(0);

    float32x2_t VL = vget_low_f32( V );
    float32x2_t Result = vmul_f32( vrev64_f32( VL ), vget_low_f32( Negate ) );
    return vcombine_f32( Result, zero );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Orthogonal(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,2,0,1));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_2_0_1
           // vResult = _mm_mul_ps(vResult,g_XMNegateX);
           MULPS   XMM0, [g_XMNegateX]
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}




function XMVector2AngleBetweenNormalsEst(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2Dot(N1, N2);
    Result := XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result := XMVectorACosEst(Result);
end;


function XMVector2AngleBetweenNormals(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector2Dot(N1, N2);
    Result := XMVectorClamp(Result, g_XMNegativeOne, g_XMOne);
    Result := XMVectorACos(Result);
end;



function XMVector2AngleBetweenVectors(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    L1, L2, Dot, CosAngle: TXMVECTOR;
begin
    L1 := XMVector2ReciprocalLength(V1);
    L2 := XMVector2ReciprocalLength(V2);

    Dot := XMVector2Dot(V1, V2);

    L1 := XMVectorMultiply(L1, L2);

    CosAngle := XMVectorMultiply(Dot, L1);
    CosAngle := XMVectorClamp(CosAngle, g_XMNegativeOne.v, g_XMOne.v);

    Result := XMVectorACos(CosAngle);
end;


function XMVector2LinePointDistance(constref LinePoint1: TXMVECTOR; constref LinePoint2: TXMVECTOR; constref Point: TXMVECTOR): TXMVECTOR;
var
    PointVector, LineVector, LengthSq, PointProjectionScale, DistanceVector: TXMVECTOR;
begin
    // Given a vector PointVector from LinePoint1 to Point and a vector
    // LineVector from LinePoint1 to LinePoint2, the scaled distance
    // PointProjectionScale from LinePoint1 to the perpendicular projection
    // of PointVector onto the line is defined as:

    //     PointProjectionScale  :=  dot(PointVector, LineVector) / LengthSq(LineVector)

    PointVector := XMVectorSubtract(Point, LinePoint1);
    LineVector := XMVectorSubtract(LinePoint2, LinePoint1);

    LengthSq := XMVector2LengthSq(LineVector);

    PointProjectionScale := XMVector2Dot(PointVector, LineVector);
    PointProjectionScale := XMVectorDivide(PointProjectionScale, LengthSq);

    DistanceVector := XMVectorMultiply(LineVector, PointProjectionScale);
    DistanceVector := XMVectorSubtract(PointVector, DistanceVector);

    Result := XMVector2Length(DistanceVector);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_) or DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2IntersectLine(constref Line1Point1: TXMVECTOR; constref Line1Point2: TXMVECTOR; constref Line2Point1: TXMVECTOR; constref Line2Point2: TXMVECTOR): TXMVECTOR;
var
    V1, V2, V3, C1, C2, Zero, Scale: TXMVECTOR;
begin
    V1 := XMVectorSubtract(Line1Point2, Line1Point1);
    V2 := XMVectorSubtract(Line2Point2, Line2Point1);
    V3 := XMVectorSubtract(Line1Point1, Line2Point1);

    C1 := XMVector2Cross(V1, V2);
    C2 := XMVector2Cross(V2, V3);


    Zero := XMVectorZero();
    if (XMVector2NearEqual(C1, Zero, g_XMEpsilon.v)) then
    begin
        if (XMVector2NearEqual(C2, Zero, g_XMEpsilon.v)) then
        begin
            // Coincident
            Result := g_XMInfinity.v;
        end
        else
        begin
            // Parallel
            Result := g_XMQNaN.v;
        end;
    end
    else
    begin
        // Intersection point  :=  Line1Point1 + V1 * (C2 / C1)
        Scale := XMVectorReciprocal(C1);
        Scale := XMVectorMultiply(C2, Scale);
        Result := XMVectorMultiplyAdd(V1, Scale, Line1Point1);
    end;
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2IntersectLine(constref Line1Point1: TXMVECTOR; constref Line1Point2: TXMVECTOR; constref Line2Point1: TXMVECTOR; constref Line2Point2: TXMVECTOR): TXMVECTOR;
var
    V1, V2, V3, C1, C2: TXMVECTOR;
begin
    asm
               //  V1 = _mm_sub_ps(Line1Point2, Line1Point1);
               MOVUPS  XMM0,[Line1Point2]
               SUBPS   XMM0, [Line1Point1]
               MOVUPS  [V1],XMM0
               //  V2 = _mm_sub_ps(Line2Point2, Line2Point1);
               MOVUPS  XMM0,[Line2Point2]
               SUBPS   XMM0, [Line2Point1]
               MOVUPS  [V2],XMM0
               //  V3 = _mm_sub_ps(Line1Point1, Line2Point1);
               MOVUPS  XMM0,[Line1Point1]
               SUBPS   XMM0, [Line2Point1]
               MOVUPS  [V3],XMM0
    end;
    // Generate the cross products
    C1 := XMVector2Cross(V1, V2);
    C2 := XMVector2Cross(V2, V3);
    asm
               // If C1 is not close to epsilon, use the calculated value
               //  vResultMask = _mm_setzero_ps();
               XORPS   XMM0,XMM0
               // vResultMask = _mm_sub_ps(vResultMask,C1);
               SUBPS   XMM0, [C1]
               //  vResultMask = _mm_max_ps(vResultMask,C1);
               MAXPS   XMM0, [C1]
               // 0xFFFFFFFF if the calculated value is to be used
               //   vResultMask = _mm_cmpgt_ps(vResultMask,g_XMEpsilon);
               CMPPS   XMM0, [g_XMEpsilon], 1
               // If C1 is close to epsilon, which fail type is it? INFINITY or NAN?
               //   vFailMask = _mm_setzero_ps();
               XORPS   XMM1 ,XMM1
               //  vFailMask = _mm_sub_ps(vFailMask,C2);
               SUBPS   XMM1, [C2]
               // vFailMask = _mm_max_ps(vFailMask,C2);
               MAXPS   XMM1, [C2]
               // vFailMask = _mm_cmple_ps(vFailMask,g_XMEpsilon);
               CMPPS   XMM1, [g_XMEpsilon], 2 // XMM1 = vFailMask


               //  vFail = _mm_and_ps(vFailMask,g_XMInfinity);
               MOVUPS  XMM2, XMM1
               ANDPS   XMM2, [g_XMInfinity] // XMM2 = vFail
               // vFailMask = _mm_andnot_ps(vFailMask,g_XMQNaN);
               ANDNPS  XMM1, [g_XMQNaN]
               // vFail is NAN or INF
               // vFail = _mm_or_ps(vFail,vFailMask);
               ORPS    XMM2, XMM1
               // Intersection point = Line1Point1 + V1 * (C2 / C1)
               //  vResult = _mm_div_ps(C2,C1);
               MOVUPS  XMM3, [C2]
               DIVPS   XMM3, [C1]
               // vResult = _mm_mul_ps(vResult,V1);
               MULPS   XMM3, [V1]
               // vResult = _mm_add_ps(vResult,Line1Point1);
               ADDPS   XMM3, [Line1Point1]
               // Use result, or failure value
               // vResult = _mm_and_ps(vResult,vResultMask);
               ANDPS   XMM3, XMM0
               // vResultMask = _mm_andnot_ps(vResultMask,vFail);
               ANDNPS  XMM0, XMM2
               // vResult = _mm_or_ps(vResult,vResultMask);
               ORPS    XMM3, XMM0
               // return vResult;
               MOVUPS  [result],XMM3
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
var
    Y, X: TXMVECTOR;
begin
    Y := XMVectorSplatY(V);
    X := XMVectorSplatX(V);

    Result := XMVectorMultiplyAdd(Y, M.r[1], M.r[3]);
    Result := XMVectorMultiplyAdd(X, M.r[0], Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
begin
    (* ToDo
    float32x2_t VL = vget_low_f32( V );
    float32x4_t Result = vmlaq_lane_f32( M.r[3], M.r[1], VL, 1 ); // Y
    return vmlaq_lane_f32( Result, M.r[0], VL, 0 ); // X
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR; assembler;
asm
           //  vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vResult = _mm_mul_ps(vResult,M.r[0]);
           MULPS   XMM0, TXMMATRIX([M]).r0
           //  vTemp = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vTemp = _mm_mul_ps(vTemp,M.r[1]);
           MULPS   XMM1, TXMMATRIX([M]).r1
           // vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM0, XMM1
           // vResult = _mm_add_ps(vResult,M.r[3]);
           ADDPS   XMM0, TXMMATRIX([M]).r3
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2TransformStream(out pOutputStream: PXMFLOAT4; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT4;
var
    pInputVector: array of TXMFLOAT2 absolute pInputStream;
    pOutputVector: array of TXMFLOAT4;
    row0, row1, row3: TXMVECTOR;
    i: size_t;
    V, Y, X, vResult: TXMVECTOR;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);

    assert(InputStride >= sizeof(TXMFLOAT2));
    assert(OutputStride >= sizeof(TXMFLOAT4));

    row0 := M.r[0];
    row1 := M.r[1];
    row3 := M.r[3];

    setLength(pOutputVector, VectorCount);

    for i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat2(pInputVector[i]);
        Y := XMVectorSplatY(V);
        X := XMVectorSplatX(V);

        vResult := XMVectorMultiplyAdd(Y, row1, row3);
        vResult := XMVectorMultiplyAdd(X, row0, vResult);

        XMStoreFloat4(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := pOutputStream;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2TransformStream(out pOutputStream: PXMFLOAT4; OutputStride: size_t; pInputStream: PXMFLOAT2; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT4;
begin
    (* ToDo
    const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT2)) && (OutputStride == sizeof(XMFLOAT4)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x2_t V = vld2q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT2)*4;

                float32x2_t r3 = vget_low_f32( row3 );
                float32x2_t r = vget_low_f32( row0 );
                XMVECTOR vResult0 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Ax+M
                XMVECTOR vResult1 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Bx+N

                __prefetch( pInputVector );

                r3 = vget_high_f32( row3 );
                r = vget_high_f32( row0 );
                XMVECTOR vResult2 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Cx+O
                XMVECTOR vResult3 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Dx+P

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( row1 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( row1 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[1], r, 0 ); // Cx+Gy+O
                vResult3 = vmlaq_lane_f32( vResult3, V.val[1], r, 1 ); // Dx+Hy+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                float32x4x4_t R;
                R.val[0] = vResult0;
                R.val[1] = vResult1;
                R.val[2] = vResult2;
                R.val[3] = vResult3;

                vst4q_f32( reinterpret_cast<float*>(pOutputVector), R );
                pOutputVector += sizeof(XMFLOAT4)*4;

                i += 4;
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        float32x2_t V = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
        pInputVector += InputStride;

        XMVECTOR vResult = vmlaq_lane_f32( row3, row0, V, 0 ); // X
        vResult = vmlaq_lane_f32( vResult, row1, V, 1 ); // Y

        vst1q_f32( reinterpret_cast<float*>(pOutputVector), vResult );
        pOutputVector += OutputStride;
    }

    return pOutputStream;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2TransformStream(out pOutputStream: PXMFLOAT4; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT4;
var
    row0, row1, row3: TXMVECTOR;
    i, two: size_t;
    j: size_t;
    s: integer;
    pInputVector: pointer;
    pOutputVector: pointer;
begin
    // const uint8_t* pInputVector = (const uint8_t*)pInputStream;
    pInputVector := pInputStream;
    // uint8_t* pOutputVector = (uint8_t*)pOutputStream;
    pOutputVector := pOutputStream;

    row0 := M.r[0];
    row1 := M.r[1];
    row3 := M.r[3];

    i := 0;
    two := VectorCount shr 1;
    if (two > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT2)) then
        begin
            if (not ((uintptr(pOutputStream) and $F) = $F) and not ((OutputStride and $F) = $F)) then
            begin
                // Packed input, aligned output
                s := sizeof(TXMFLOAT2) * 2;
                for  j := 0 to two - 1 do
                begin
                    asm
                               // V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM0, [pInputVector]
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX // pInputVector += s;

                               //  Y = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               //  vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1

                               // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVAPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Y = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_3_3_3_3
                               // X = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2

                               // vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               // vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1

                               // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVAPS  [pOutputVector], XMM2

                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 2);
                end;
            end
            else
            begin
                // Packed input, unaligned output
                s := sizeof(TXMFLOAT2) * 2;
                for  j := 0 to two - 1 do
                begin
                    asm
                               // V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM0, [pInputVector]
                               // pInputVector += sizeof(XMFLOAT2)*2;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               //  Y = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               //  vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1

                               // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Y = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_3_3_3_3
                               // X = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2

                               // vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               // vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1

                               // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 2);
                end;
            end;
        end;
    end;

    // if ( !((uintptr_t)pInputVector & 0xF) && !(InputStride & 0xF) )   then
    if (not (uintptr(pInputVector) and $F = $F) and not (InputStride and $F = $F)) then
    begin
        if (not (uintptr(pOutputStream) and $F = $F) and not (OutputStride and $F = $F)) then
        begin
            // Aligned input, aligned output
            while i < VectorCount do
                // for  i < VectorCount; i++) do
            begin
                asm
                           // V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
                           MOVQ    XMM0, [pInputVector]


                           // pInputVector += InputStride;
                           MOV     EDX, pInputVector
                           ADD     EDX, [InputStride]
                           MOV     pInputVector,EDX

                           //  Y = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                           MOVUPS  XMM1,XMM0
                           MOVUPS  XMM2,XMM0
                           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                           //  X = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
                           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                           //  vTemp = _mm_mul_ps( Y, row1 );
                           MULPS   XMM2, [row1]
                           //  vTemp2 = _mm_mul_ps( X, row0 );
                           MULPS   XMM1, [row0]
                           // vTemp = _mm_add_ps( vTemp, row3 );
                           ADDPS   XMM2, [row3]
                           // vTemp = _mm_add_ps( vTemp, vTemp2 );
                           ADDPS   XMM2, XMM1

                           // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                           MOVUPS  [pOutputVector], XMM2
                           // pOutputVector += OutputStride;
                           MOV     EDX, pOutputVector
                           ADD     EDX, [OutputStride]
                           MOV     pOutputVector,EDX
                end;
                Inc(i);
            end;
        end
        else
        begin
            // Aligned input, unaligned output
            // for (; i < VectorCount; i++) do
            while i < VectorCount do
            begin
                asm
                           // V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
                           MOVQ    XMM0, [pInputVector]
                           // pInputVector += InputStride;
                           MOV     EDX, pInputVector
                           ADD     EDX, [InputStride]
                           MOV     pInputVector,EDX

                           //  Y = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                           MOVUPS  XMM1,XMM0
                           MOVUPS  XMM2,XMM0
                           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                           //  X = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
                           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                           //  vTemp = _mm_mul_ps( Y, row1 );
                           MULPS   XMM2, [row1]
                           //  vTemp2 = _mm_mul_ps( X, row0 );
                           MULPS   XMM1, [row0]
                           // vTemp = _mm_add_ps( vTemp, row3 );
                           ADDPS   XMM2, [row3]
                           // vTemp = _mm_add_ps( vTemp, vTemp2 );
                           ADDPS   XMM2, XMM1

                           // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                           MOVUPS  [pOutputVector], XMM2
                           //  pOutputVector += OutputStride;
                           MOV     EDX, pOutputVector
                           ADD     EDX, [OutputStride]
                           MOV     pOutputVector,EDX
                end;
                Inc(i);
            end;
        end;
    end
    else
    begin
        // Unaligned input
        // for (; i < VectorCount; i++) do
        while i < VectorCount do
        begin
            asm
                       // __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pInputVector) );
                       MOVSS   XMM0, [pInputVector]
                       // __m128 y = _mm_load_ss( reinterpret_cast<const float*>(pInputVector+4) );
                       MOVSS   XMM1, [pInputVector+4]
                       // pInputVector += InputStride;
                       MOV     EDX, pInputVector
                       ADD     EDX, [InputStride]
                       MOV     pInputVector,EDX

                       //  Y = XM_PERMUTE_PS(y,_MM_SHUFFLE(0,0,0,0));
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0
                       //  X = XM_PERMUTE_PS(x,_MM_SHUFFLE(0,0,0,0));
                       SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                       //  vTemp = _mm_mul_ps( Y, row1 );
                       MULPS   XMM1, [row1]
                       //  vTemp2 = _mm_mul_ps( X, row0 );
                       MULPS   XMM0, [row0]
                       // vTemp = _mm_add_ps( vTemp, row3 );
                       ADDPS   XMM1, [row3]
                       // vTemp = _mm_add_ps( vTemp, vTemp2 );
                       ADDPS   XMM1, XMM0

                       // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                       MOVUPS  [pOutputVector], XMM1
                       // pOutputVector += OutputStride;
                       MOV     EDX, pOutputVector
                       ADD     EDX, [OutputStride]
                       MOV     pOutputVector,EDX
            end;
            Inc(i);
        end;
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}

function XMVector2TransformCoord(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
var
    X, Y, W: TXMVECTOR;
begin
    Y := XMVectorSplatY(V);
    X := XMVectorSplatX(V);

    Result := XMVectorMultiplyAdd(Y, M.r[1], M.r[3]);
    Result := XMVectorMultiplyAdd(X, M.r[0], Result);

    W := XMVectorSplatW(Result);
    Result := XMVectorDivide(Result, W);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2TransformCoordStream(out pOutputStream: PXMFLOAT2; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT2;
var
    pInputVector: array of TXMFLOAT2 absolute pInputStream;
    pOutputVector: array of TXMFLOAT2;
    row0, row1, row3: TXMVECTOR;
    i: size_t;
    vResult, W, V, Y, X: TXMVECTOR;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);

    assert(InputStride >= sizeof(TXMFLOAT2));


    assert(OutputStride >= sizeof(TXMFLOAT2));
    SetLength(pOutputVector, VectorCount);


    row0 := M.r[0];
    row1 := M.r[1];
    row3 := M.r[3];


    for i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat2(pInputVector[i]);
        Y := XMVectorSplatY(V);
        X := XMVectorSplatX(V);

        vResult := XMVectorMultiplyAdd(Y, row1, row3);
        vResult := XMVectorMultiplyAdd(X, row0, vResult);

        W := XMVectorSplatW(vResult);

        vResult := XMVectorDivide(vResult, W);

        XMStoreFloat2(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := pOutputStream;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2TransformCoordStream(out pOutputStream: PXMFLOAT2; OutputStride: size_t; constref pInputStream: PXMFLOAT2; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT2;
begin
    (* ToDo
    const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT2)) && (OutputStride == sizeof(XMFLOAT2)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x2_t V = vld2q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT2)*4;

                float32x2_t r3 = vget_low_f32( row3 );
                float32x2_t r = vget_low_f32( row0 );
                XMVECTOR vResult0 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Ax+M
                XMVECTOR vResult1 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Bx+N

                __prefetch( pInputVector );

                r3 = vget_high_f32( row3 );
                r = vget_high_f32( row0 );
                XMVECTOR W = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Dx+P

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( row1 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( row1 );
                W = vmlaq_lane_f32( W, V.val[1], r, 1 ); // Dx+Hy+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
                V.val[0] = vdivq_f32( vResult0, W );
                V.val[1] = vdivq_f32( vResult1, W );
#else
                // 2 iterations of Newton-Raphson refinement of reciprocal
                float32x4_t Reciprocal = vrecpeq_f32(W);
                float32x4_t S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );
                S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );

                V.val[0] = vmulq_f32( vResult0, Reciprocal );
                V.val[1] = vmulq_f32( vResult1, Reciprocal );
#endif

                vst2q_f32( reinterpret_cast<float*>(pOutputVector),V );
                pOutputVector += sizeof(XMFLOAT2)*4;

                i += 4;
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        float32x2_t V = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
        pInputVector += InputStride;

        XMVECTOR vResult = vmlaq_lane_f32( row3, row0, V, 0 ); // X
        vResult = vmlaq_lane_f32( vResult, row1, V, 1 ); // Y

        V = vget_high_f32( vResult );
        float32x2_t W = vdup_lane_f32( V, 1 );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
        V = vget_low_f32( vResult );
        V = vdiv_f32( V, W );
#else
        // 2 iterations of Newton-Raphson refinement of reciprocal for W
        float32x2_t Reciprocal = vrecpe_f32( W );
        float32x2_t S = vrecps_f32( Reciprocal, W );
        Reciprocal = vmul_f32( S, Reciprocal );
        S = vrecps_f32( Reciprocal, W );
        Reciprocal = vmul_f32( S, Reciprocal );

        V = vget_low_f32( vResult );
        V = vmul_f32( V, Reciprocal );
#endif

        vst1_f32( reinterpret_cast<float*>(pOutputVector), V );
        pOutputVector += OutputStride;
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2TransformCoordStream(out pOutputStream: PXMFLOAT2; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT2;
var
    pInputVector: pointer;
    pOutputVector: pointer;
    row0, row1, row3: TXMVECTOR;
    i, j, two: size_t;
    s: uint32;
begin
    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    row0 := M.r[0];
    row1 := M.r[1];
    row3 := M.r[3];

    i := 0;
    two := VectorCount shr 1;
    if (two > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT2)) then
        begin
            if (OutputStride = sizeof(TXMFLOAT2)) then
            begin
                if (not ((uintptr(pOutputStream) and $F = $F))) then
                begin
                    // Packed input, aligned & packed output
                    s := sizeof(TXMFLOAT2) * 2;
                    for  j := 0 to two - 1 do
                    begin
                        asm
                                   //  V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM0, [pInputVector]
                                   // pInputVector += sizeof(XMFLOAT2)*2;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Result 1
                                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM2, [row1]
                                   //  vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM1, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1

                                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   //  V1 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // Result 2
                                   // Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM0
                                   MOVUPS  XMM4,XMM0
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3
                                   // X = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_2_2_2_2

                                   // vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM4, [row1]
                                   // vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM3, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM4, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM4, XMM3

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3, XMM4
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   //  V2 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM4, XMM3

                                   // vTemp = _mm_movelh_ps( V1, V2 );
                                   MOVLHPS XMM2, XMM4
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                                   MOVUPS  [pOutputVector],XMM2
                                   // pOutputVector += sizeof(XMFLOAT2)*2;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 2);
                    end;
                end
                else
                begin
                    // Packed input, unaligned & packed output
                    s := sizeof(TXMFLOAT2) * 2;
                    for j := 0 to two - 1 do
                    begin
                        asm
                                   //  V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM0, [pInputVector]
                                   // pInputVector += sizeof(TXMFLOAT2)*2;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Result 1
                                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM2, [row1]
                                   //  vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM1, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1

                                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3, XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   //  V1 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // Result 2
                                   // Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM0
                                   MOVUPS  XMM4,XMM0
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3

                                   // X = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_2_2_2_2

                                   // vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM4, [row1]
                                   // vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM3, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM4, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM4, XMM3

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM4
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   //  V2 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM4, XMM3

                                   // vTemp = _mm_movelh_ps( V1, V2 );
                                   MOVLHPS XMM2, XMM4

                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                                   MOVUPS  [pOutputVector], XMM2
                                   // pOutputVector += sizeof(XMFLOAT2)*2;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 2);
                    end;
                end;
            end
            else
            begin
                // Packed input, unpacked output
                s := sizeof(TXMFLOAT2) * 2;
                for j := 0 to two - 1 do
                begin
                    asm
                               //  V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM0, [pInputVector]
                               // pInputVector += sizeof(XMFLOAT2)*2;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Result 1
                               //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               //  vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               //  vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1

                               //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3, XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                               //  vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3
                               // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                               MOVUPS  XMM4,XMM2
                               SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_1_1_1

                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVSS   [pOutputVector], XMM2
                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                               MOVSS   [pOutputVector+4], XMM4
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_3_3_3_3
                               // X = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2

                               // vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               // vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3
                               // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                               MOVUPS  XMM4, XMM2
                               SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_1_1_1

                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVSS   [pOutputVector], XMM2
                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                               MOVSS   [pOutputVector+4], XMM4
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 2);
                end;
            end;
        end;
    end;

    if (not (uintptr(pInputVector) and $F = $F) and not (InputStride and $F = $F)) then
    begin
        // Aligned input
        //for (; i < VectorCount; i++)
        while i < VectorCount do
        begin
            asm
                       // V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
                       MOVQ    XMM0, [pInputVector]
                       // pInputVector += InputStride;
                       MOV     EDX, pInputVector
                       ADD     EDX, [InputStride]
                       MOV     pInputVector,EDX

                       // Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                       MOVUPS  XMM1,XMM0
                       MOVUPS  XMM2,XMM0
                       SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                       // X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                       // vTemp = _mm_mul_ps( Y, row1 );
                       MULPS   XMM2, [row1]
                       // vTemp2 = _mm_mul_ps( X, row0 );
                       MULPS   XMM1,[row0]
                       // vTemp = _mm_add_ps( vTemp, row3 );
                       ADDPS   XMM2, [row3]
                       // vTemp2 = _mm_add_ps( vTemp, vTemp2 );
                       ADDPS   XMM2, XMM1

                       // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                       MOVUPS  XMM3,XMM2
                       SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                       // vTemp = _mm_div_ps( vTemp, W );
                       DIVPS   XMM2, XMM3
                       // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                       MOVUPS  XMM4, XMM2
                       SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_1_1_1

                       // _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                       MOVSS   [pOutputVector], XMM2
                       // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                       MOVSS   [pOutputVector+4], XMM4
                       // pOutputVector += OutputStride;
                       MOV     EDX, pOutputVector
                       ADD     EDX, [OutputStride]
                       MOV     pOutputVector,EDX
            end;
            Inc(i);
        end;
    end
    else
    begin
        // Unaligned input
        // for (; i < VectorCount; i++)
        while i < VectorCount do
        begin
            asm
                       //  x = _mm_load_ss( reinterpret_cast<const float*>(pInputVector) );
                       MOVSS   XMM0, [pInputVector]
                       //  y = _mm_load_ss( reinterpret_cast<const float*>(pInputVector+4) );
                       MOVSS   XMM1, [pInputVector+4]
                       // pInputVector += InputStride;
                       MOV     EDX, pInputVector
                       ADD     EDX, [InputStride]
                       MOV     pInputVector,EDX

                       //  Y = XM_PERMUTE_PS( y, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0
                       //  X = XM_PERMUTE_PS( x, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                       //  vTemp = _mm_mul_ps( Y, row1 );
                       MULPS   XMM1, [row1]
                       //  vTemp2 = _mm_mul_ps( X, row0 );
                       MULPS   XMM0,[row0]
                       // vTemp = _mm_add_ps( vTemp, row3 );
                       ADDPS   XMM1, [row3]
                       // vTemp = _mm_add_ps( vTemp, vTemp2 );
                       ADDPS   XMM1, XMM0

                       //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                       MOVUPS  XMM3,XMM1
                       SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                       // vTemp = _mm_div_ps( vTemp, W );
                       DIVPS   XMM1, XMM3
                       // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                       MOVUPS  XMM3,XMM1
                       SHUFPS  XMM3, XMM3, _MM_SHUFFLE_1_1_1_1

                       //  _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                       MOVSS   [pOutputVector], XMM1
                       // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                       MOVSS   [pOutputVector+4], XMM3
                       // pOutputVector += OutputStride;
                       MOV     EDX, pOutputVector
                       ADD     EDX, [OutputStride]
                       MOV     pOutputVector,EDX
            end;
            Inc(i);
        end;
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2TransformNormal(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
var
    X, Y: TXMVECTOR;
begin
    Y := XMVectorSplatY(V);
    X := XMVectorSplatX(V);

    Result := XMVectorMultiply(Y, M.r[1]);
    Result := XMVectorMultiplyAdd(X, M.r[0], Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2TransformNormal(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
begin
    (* ToDo
      float32x2_t VL = vget_low_f32( V );
    float32x4_t Result = vmulq_lane_f32( M.r[1], VL, 1 ); // Y
    return vmlaq_lane_f32( Result, M.r[0], VL, 0 ); // X
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2TransformNormal(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR; assembler;
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vResult = _mm_mul_ps(vResult,M.r[0]);
           MULPS   XMM0 , TXMMATRIX([M]).r0
           //  vTemp = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vTemp = _mm_mul_ps(vTemp,M.r[1]);
           MULPS   XMM1 , TXMMATRIX([M]).r1
           // vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM0, XMM1
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector2TransformNormalStream(out pOutputStream: PXMFLOAT2; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT2;
var
    pInputVector: array of TXMFLOAT2 absolute pInputStream;
    pOutputVector: array of TXMFLOAT2;
    row0, row1: TXMVECTOR;
    i: size_t;
    vResult, W, V, Y, X: TXMVECTOR;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);

    assert(InputStride >= sizeof(TXMFLOAT2));

    assert(OutputStride >= sizeof(TXMFLOAT2));
    SetLength(pOutputVector, VectorCount);

    row0 := M.r[0];
    row1 := M.r[1];

    for  i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat2(pInputVector[i]);
        Y := XMVectorSplatY(V);
        X := XMVectorSplatX(V);

        vResult := XMVectorMultiply(Y, row1);
        vResult := XMVectorMultiplyAdd(X, row0, vResult);


        XMStoreFloat2(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := pOutputStream;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector2TransformNormalStream(out pOutputStream: PXMFLOAT2; OutputStride: size_t; pInputStream: PXMFLOAT2; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT2;
begin
    (* ToDo
    const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT2)) && (OutputStride == sizeof(XMFLOAT2)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x2_t V = vld2q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT2)*4;

                float32x2_t r = vget_low_f32( row0 );
                XMVECTOR vResult0 = vmulq_lane_f32( V.val[0], r, 0 ); // Ax
                XMVECTOR vResult1 = vmulq_lane_f32( V.val[0], r, 1 ); // Bx

                __prefetch( pInputVector );
                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( row1 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );
                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                V.val[0] = vResult0;
                V.val[1] = vResult1;

                vst2q_f32( reinterpret_cast<float*>(pOutputVector), V );
                pOutputVector += sizeof(XMFLOAT2)*4;

                i += 4;
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        float32x2_t V = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
        pInputVector += InputStride;

        XMVECTOR vResult = vmulq_lane_f32( row0, V, 0 ); // X
        vResult = vmlaq_lane_f32( vResult, row1, V, 1 ); // Y

        V = vget_low_f32( vResult );
        vst1_f32( reinterpret_cast<float*>(pOutputVector), V );
        pOutputVector += OutputStride;
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector2TransformNormalStream(out pOutputStream: PXMFLOAT2; constref OutputStride: size_t; constref pInputStream: PXMFLOAT2; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT2;
var
    pInputVector: pointer;
    pOutputVector: pointer;
    row0, row1: TXMVECTOR;
    i, two, j: size_t;
    s: uint32;
begin
    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    row0 := M.r[0];
    row1 := M.r[1];

    i := 0;
    two := VectorCount shr 1;
    if (two > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT2)) then
        begin
            if (OutputStride = sizeof(TXMFLOAT2)) then
            begin
                if (not (uintptr(pOutputStream) and $F = $F)) then
                begin
                    // Packed input, aligned & packed output
                    s := sizeof(TXMFLOAT2) * 2;
                    for  j := 0 to two - 1 do
                    begin
                        asm
                                   // V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM0, [pInputVector]
                                   // pInputVector += sizeof(XMFLOAT2)*2;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Result 1
                                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM2, [row1]
                                   //  vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM1, [row0]
                                   //  V1 = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1

                                   // Result 2
                                   // Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM0
                                   MOVUPS  XMM4,XMM0
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3
                                   // X = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_2_2_2_2

                                   // vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM4, [row1]
                                   // vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM3, [row0]
                                   //  V2 = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM4, XMM3

                                   // vTemp = _mm_movelh_ps( V1, V2 );
                                   MOVLHPS XMM2, XMM4

                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                                   MOVUPS  [pOutputVector], XMM2
                                   // pOutputVector += sizeof(XMFLOAT2)*2;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 2);
                    end;
                end
                else
                begin
                    // Packed input, unaligned & packed output
                    s := sizeof(TXMFLOAT2) * 2;
                    for  j := 0 to two - 1 do
                    begin
                        asm
                                   //  V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM0, [pInputVector]
                                   // pInputVector += sizeof(XMFLOAT2)*2;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [InputStride]
                                   MOV     pInputVector,EDX

                                   // Result 1
                                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM2, [row1]
                                   //  vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM1, [row0]
                                   //  V1 = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1

                                   // Result 2
                                   // Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM0
                                   MOVUPS  XMM4,XMM0
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3
                                   // X = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_2_2_2_2

                                   // vTemp = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM4, [row1]
                                   // vTemp2 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM3, [row0]
                                   //  V2 = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM4, XMM3

                                   // vTemp = _mm_movelh_ps( V1, V2 );
                                   MOVLHPS XMM2, XMM4

                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                                   MOVUPS  [pOutputVector], XMM2
                                   // pOutputVector += sizeof(XMFLOAT2)*2;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 2);
                    end;
                end;
            end
            else
            begin
                // Packed input, unpacked output
                s := sizeof(TXMFLOAT2) * 2;
                for  j := 0 to two - 1 do
                begin
                    asm
                               //  V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM0, [pInputVector]
                               // pInputVector += sizeof(XMFLOAT2)*2;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Result 1
                               //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               //  vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                               MOVUPS  XMM1,XMM2
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1

                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVSS   [pOutputVector], XMM1
                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                               MOVSS   [pOutputVector+4], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_3_3_3_3
                               // X = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2

                               // vTemp = _mm_mul_ps( Y, row1 );
                               MULPS   XMM2, [row1]
                               // vTemp2 = _mm_mul_ps( X, row0 );
                               MULPS   XMM1, [row0]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                               MOVUPS  XMM1,XMM2
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1

                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVSS   [pOutputVector], XMM1
                               // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                               MOVSS   [pOutputVector+4], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 2);
                end;
            end;
        end;
    end;

    if (not ((uintptr(pInputVector) and $F = $F) and not (InputStride and $F = $F))) then
    begin
        // Aligned input
        // for (; i < VectorCount; i++) do
        while i < VectorCount do
        begin
            asm
                       //  V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
                       MOVQ    XMM0, [pInputVector]
                       // pInputVector += InputStride;
                       MOV     EDX, pInputVector
                       ADD     EDX, [InputStride]
                       MOV     pInputVector,EDX

                       //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                       MOVUPS  XMM1,XMM0
                       MOVUPS  XMM2,XMM0
                       SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1
                       //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0

                       //  vTemp = _mm_mul_ps( Y, row1 );
                       MULPS   XMM2, [row1]
                       //  vTemp2 = _mm_mul_ps( X, row0 );
                       MULPS   XMM1, [row0]
                       //  vTemp = _mm_add_ps( vTemp, vTemp2 );
                       ADDPS   XMM2, XMM1
                       // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                       MOVUPS  XMM1,XMM2
                       SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_1_1

                       // _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                       MOVSS   [pOutputVector], XMM1
                       // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                       MOVSS   [pOutputVector+4], XMM2
                       // pOutputVector += OutputStride;
            end;
            Inc(i);
        end;
    end
    else
    begin
        // Unaligned input
        // for (; i < VectorCount; i++) do
        while i < VectorCount do
        begin
            asm
                       //  x = _mm_load_ss( reinterpret_cast<const float*>(pInputVector) );
                       MOVSS   XMM0, [pInputVector]
                       //  y = _mm_load_ss( reinterpret_cast<const float*>(pInputVector+4) );
                       MOVSS   XMM1, [pInputVector+4]
                       // pInputVector += InputStride;
                       MOV     EDX, pInputVector
                       ADD     EDX, [InputStride]
                       MOV     pInputVector,EDX

                       //  Y = XM_PERMUTE_PS( y, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_0
                       //  X = XM_PERMUTE_PS( x, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                       //  vTemp = _mm_mul_ps( Y, row1 );
                       MULPS   XMM1, [row1]
                       //  vTemp2 = _mm_mul_ps( X, row0 );
                       MULPS   XMM0, [row0]
                       // vTemp = _mm_add_ps( vTemp, vTemp2 );
                       ADDPS   XMM1, XMM0
                       // vTemp2 = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(1, 1, 1, 1) );
                       MOVUPS  XMM0,XMM1
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1

                       // _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                       MOVSS   [pOutputVector], XMM0
                       // _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                       MOVSS   [pOutputVector+4], XMM1
                       // pOutputVector += OutputStride;
                       MOV     EDX, pOutputVector
                       ADD     EDX, [OutputStride]
                       MOV     pOutputVector,EDX
            end;
            Inc(i);
        end;
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}

{***************************************************************************
 *
 * 3D Vector
 *
 ***************************************************************************}

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] = V2.f32[0]) and (V1.f32[1] = V2.f32[1]) and (V1.f32[2] = V2.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x4_t vResult = vceqq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           //XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           CMPPS   XMM0, [V2], 0
           //return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] = V2.f32[0]) and (V1.f32[1] = V2.f32[1]) and (V1.f32[2] = V2.f32[2])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.f32[0] <> V2.f32[0]) and (V1.f32[1] <> V2.f32[1]) and (V1.f32[2] <> V2.f32[2])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x4_t vResult = vceqq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU;

    uint32_t CR = 0;
    if ( r == 0xFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
asm
           // vTemp = _mm_cmpeq_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 0
           // int iTest = _mm_movemask_ps(vTemp)&7;
    {movmskps r32, xmm
    uint32_t CR = 0;
    if (iTest==7) then
       CR = XM_CRMASK_CR6TRUE;

    else if (!iTest)

        CR = XM_CRMASK_CR6FALSE;

    return CR; }

           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished
           MOV     EDX,XM_CRMASK_CR6TRUE
           CMP     EAX, $7
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.u32[0] = V2.u32[0]) and (V1.u32[1] = V2.u32[1]) and (V1.u32[2] = V2.u32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
     uint32x4_t vResult = vceqq_u32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
asm
           // vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           MOVUPS  XMM0,[V1]
           PCMPEQD XMM0, [V2]
           //return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&7)==7) != 0);

           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.u32[0] = V2.u32[0]) and (V1.u32[1] = V2.u32[1]) and (V1.u32[2] = V2.u32[2])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.u32[0] <> V2.u32[0]) and (V1.u32[1] <> V2.u32[1]) and (V1.u32[2] <> V2.u32[2])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x4_t vResult = vceqq_u32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU;

    uint32_t CR = 0;
    if ( r == 0xFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
asm
           // vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           MOVUPS  XMM0,[V1]
           PCMPEQD XMM0, [V2]
           // int iTemp = _mm_movemask_ps(_mm_castsi128_ps(vTemp))&7;
   { movmskps r32, xmm
    uint32_t CR = 0;
    if (iTemp==7)

        CR = XM_CRMASK_CR6TRUE;

    else if (!iTemp)

        CR = XM_CRMASK_CR6FALSE;

    return CR; }
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished
           MOV     EDX,XM_CRMASK_CR6TRUE
           CMP     EAX, $7
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
var
    dx, dy, dz: single;
begin
    dx := abs(V1.f32[0] - V2.f32[0]);
    dy := abs(V1.f32[1] - V2.f32[1]);
    dz := abs(V1.f32[2] - V2.f32[2]);
    Result := ((dx <= Epsilon.f32[0]) and (dy <= Epsilon.f32[1]) and (dz <= Epsilon.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
begin
    (* ToDo
    float32x4_t vDelta = vsubq_f32( V1, V2 );
    uint32x4_t vResult = vacleq_f32( vDelta, Epsilon );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
asm
           // Get the difference
           // vDelta = _mm_sub_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           SUBPS   XMM0, [V2]
           // Get the absolute value of the difference
           //  vTemp = _mm_setzero_ps();
           XORPS   XMM1,XMM1
           // vTemp = _mm_sub_ps(vTemp,vDelta);
           SUBPS   XMM1, XMM0
           // vTemp = _mm_max_ps(vTemp,vDelta);
           MAXPS   XMM1, XMM0
           // vTemp = _mm_cmple_ps(vTemp,Epsilon);
           CMPPS   XMM1, [Epsilon], 2
           // w is don't care
           // return (((_mm_movemask_ps(vTemp)&7)==0x7) != 0);

           MOVMSKPS EAX, XMM1
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] <> V2.f32[0]) or (V1.f32[1] <> V2.f32[1]) or (V1.f32[2] <> V2.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x4_t vResult = vceqq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1)  & 0xFFFFFFU) != 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
asm
           // vTemp = _mm_cmpeq_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           CMPPS   XMM0, [V2], 0
           // return (((_mm_movemask_ps(vTemp)&7)!=7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.u32[0] <> V2.u32[0]) or (V1.u32[1] <> V2.u32[1]) or (V1.u32[2] <> V2.u32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x4_t vResult = vceqq_u32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1)  & 0xFFFFFFU) != 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
asm
           // vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           MOVUPS  XMM0, [V1]
           PCMPEQD XMM0, [V2]
           // return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&7)!=7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] > V2.f32[0]) and (V1.f32[1] > V2.f32[1]) and (V1.f32[2] > V2.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
   uint32x4_t vResult = vcgtq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
asm
           // vTemp = _mm_cmpgt_ps(V1,V2);
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 1
           //return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] > V2.f32[0]) and (V1.f32[1] > V2.f32[1]) and (V1.f32[2] > V2.f32[2])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.f32[0] <= V2.f32[0]) and (V1.f32[1] <= V2.f32[1]) and (V1.f32[2] <= V2.f32[2])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x4_t vResult = vcgtq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU;

    uint32_t CR = 0;
    if ( r == 0xFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
asm
           // vTemp = _mm_cmpgt_ps(V1,V2);
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 1
 {   uint32_t CR = 0;
    int iTest = _mm_movemask_ps(vTemp)&7;
    movmskps r32, xmm
    if (iTest==7)

        CR =  XM_CRMASK_CR6TRUE;

    else if (!iTest)

        CR = XM_CRMASK_CR6FALSE;

    return CR;   }

           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished

           MOV     EDX,XM_CRMASK_CR6TRUE
           CMP     EAX, $7
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] >= V2.f32[0]) and (V1.f32[1] >= V2.f32[1]) and (V1.f32[2] >= V2.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (* ToDo
    uint32x4_t vResult = vcgeq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
asm
           // vTemp = _mm_cmpge_ps(V1,V2);
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 2
           //return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] >= V2.f32[0]) and (V1.f32[1] >= V2.f32[1]) and (V1.f32[2] >= V2.f32[2])) then
    begin
        Result := XM_CRMASK_CR6TRUE;
    end
    else if ((V1.f32[0] < V2.f32[0]) and (V1.f32[1] < V2.f32[1]) and (V1.f32[2] < V2.f32[2])) then
    begin
        Result := XM_CRMASK_CR6FALSE;
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (* ToDo
    uint32x4_t vResult = vcgeq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU;

    uint32_t CR = 0;
    if ( r == 0xFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           // vTemp = _mm_cmpge_ps(V1,V2);
           MOVUPS  XMM0, [V2]
           CMPPS   XMM0, [V1], 2
    {uint32_t CR = 0;
    int iTest = _mm_movemask_ps(vTemp)&7;
    movmskps r32, xmm
    if (iTest==7)
        CR =  XM_CRMASK_CR6TRUE;
    else if (!iTest)
        CR = XM_CRMASK_CR6FALSE;

    return CR;  }

           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished
           MOV     EDX,XM_CRMASK_CR6TRUE
           CMP     EAX, $7
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] < V2.f32[0]) and (V1.f32[1] < V2.f32[1]) and (V1.f32[2] < V2.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
     uint32x4_t vResult = vcltq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
    *)
end;

{$ELSE}
function XMVector3Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           MOVUPS  XMM0,[v1]
           CMPPS   XMM0, [v2],  1
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] <= V2.f32[0]) and (V1.f32[1] <= V2.f32[1]) and (V1.f32[2] <= V2.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
     uint32x4_t vResult = vcleq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           MOVUPS  XMM0,[v1]
           CMPPS   XMM0, [v2], 2
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    Result := ((V.f32[0] <= Bounds.f32[0]) and (V.f32[0] >= -Bounds.f32[0]) and (V.f32[1] <= Bounds.f32[1]) and (V.f32[1] >= -Bounds.f32[1]) and (V.f32[2] <= Bounds.f32[2]) and (V.f32[2] >= -Bounds.f32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    (* ToDo
     // Test if less than or equal
    uint32x4_t ivTemp1 = vcleq_f32(V,Bounds);
    // Negate the bounds
    float32x4_t vTemp2 = vnegq_f32(Bounds);
    // Test if greater or equal (Reversed)
    uint32x4_t ivTemp2 = vcleq_f32(vTemp2,V);
    // Blend answers
    ivTemp1 = vandq_u32(ivTemp1,ivTemp2);
    // in bounds?
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(ivTemp1), vget_high_u8(ivTemp1));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) == 0xFFFFFFU );
*)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector3InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean; assembler;
asm
           // Test if less than or equal
           // vTemp1 = _mm_cmple_ps(V,Bounds);
           MOVUPS  XMM0,[V]
           CMPPS   XMM0, [Bounds], 2
           // Negate the bounds
           // vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
           MOVUPS  XMM1,[Bounds]
           MULPS   XMM1, [g_XMNegativeOne]
           // Test if greater or equal (Reversed)
           // vTemp2 = _mm_cmple_ps(vTemp2,V);
           CMPPS   XMM1, [V], 2
           // Blend answers
           // vTemp1 = _mm_and_ps(vTemp1,vTemp2);
           ANDPS   XMM0, XMM1
           // x,y and z in bounds? (w is don't care)
           // return (((_mm_movemask_ps(vTemp1)&0x7)==0x7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ELSE}
function XMVector3InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllInBounds(XMVector3InBoundsR(V, Bounds));
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3IsNaN(constref V: TXMVECTOR): boolean;
begin
    Result := (XMISNAN(V.u32[0]) or XMISNAN(V.u32[1]) or XMISNAN(V.u32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3IsNaN(constref V: TXMVECTOR): boolean;
begin
    (* ToDo
    // Test against itself. NaN is always not equal
    uint32x4_t vTempNan = vceqq_f32( V, V );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vTempNan), vget_high_u8(vTempNan));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    // If x or y or z are NaN, the mask is zero
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) != 0xFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3IsNaN(constref V: TXMVECTOR): boolean; assembler;
asm
           // Test against itself. NaN is always not equal
           // vTempNan = _mm_cmpneq_ps(V,V);
           MOVUPS  XMM0, [V]
           CMPPS   XMM0, XMM0, 4
           // If x or y or z are NaN, the mask is non-zero
           // return ((_mm_movemask_ps(vTempNan)&7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3IsInfinite(constref V: TXMVECTOR): boolean;
begin
    Result := (XMISINF(V.u32[0]) or XMISINF(V.u32[1]) or XMISINF(V.u32[2]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3IsInfinite(constref V: TXMVECTOR): boolean;
begin
    (* ToDo
    // Mask off the sign bit
    uint32x4_t vTempInf = vandq_u32( V, g_XMAbsMask );
    // Compare to infinity
    vTempInf = vceqq_f32(vTempInf, g_XMInfinity );
    // If any are infinity, the signs are true.
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vTempInf), vget_high_u8(vTempInf));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( (vget_lane_u32(vTemp.val[1], 1) & 0xFFFFFFU) != 0 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3IsInfinite(constref V: TXMVECTOR): boolean; assembler;
asm
           // Mask off the sign bit
           // __m128 vTemp = _mm_and_ps(V,g_XMAbsMask);
           MOVUPS  XMM0,[V]
           ANDPS   XMM0, [g_XMAbsMask]
           // Compare to infinity
           // vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 0
           // If x,y or z are infinity, the signs are true.
           // return ((_mm_movemask_ps(vTemp)&7) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $7
           CMP     EAX, $7
           SETE    [result]
end;
{$ENDIF}


//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    fValue: single;
begin
    fValue := V1.f32[0] * V2.f32[0] + V1.f32[1] * V2.f32[1] + V1.f32[2] * V2.f32[2];
    Result.f32[0] := fValue;
    Result.f32[1] := fValue;
    Result.f32[2] := fValue;
    Result.f32[3] := fValue;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    float32x4_t vTemp = vmulq_f32( V1, V2 );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vpadd_f32( v1, v1 );
    v2 = vdup_lane_f32( v2, 0 );
    v1 = vadd_f32( v1, v2 );
    return vcombine_f32( v1, v1 );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector3Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // _mm_dp_ps( V1, V2, 0x7f );
           MOVUPS  XMM0, [V1]
           DPPS    XMM0, [V2], $7F
           MOVUPS  [result], XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector3Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vTemp = _mm_mul_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           MULPS   XMM0, [V2]
           // vTemp = _mm_and_ps(vTemp, g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           // vTemp = _mm_hadd_ps(vTemp,vTemp);
           HADDPS  XMM0, XMM0
           // return _mm_hadd_ps(vTemp,vTemp);
           HADDPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product
           // vDot = _mm_mul_ps(V1,V2);
           MOVUPS  XMM0, [v1]
           MULPS   XMM0,[V2]
           // x=Dot.f32[1], y=Dot.f32[2]
           // vTemp = XM_PERMUTE_PS(vDot,_MM_SHUFFLE(2,1,2,1));
           MOVUPS  XMM1, XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_1_2_1
           // Result.f32[0] = x+y
           // vDot = _mm_add_ss(vDot,vTemp);
           ADDSS   XMM0, XMM1
           // x=Dot.f32[2]
           // vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // Result.f32[0] = (x+y)+z
           // vDot = _mm_add_ss(vDot,vTemp);
           ADDSS   XMM0, XMM1
           // Splat x
           // return XM_PERMUTE_PS(vDot,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           MOVUPS  [result], XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// [ V1.y*V2.z - V1.z*V2.y, V1.z*V2.x - V1.x*V2.z, V1.x*V2.y - V1.y*V2.x ]
function XMVector3Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := (V1.f32[1] * V2.f32[2]) - (V1.f32[2] * V2.f32[1]);
    Result.f32[1] := (V1.f32[2] * V2.f32[0]) - (V1.f32[0] * V2.f32[2]);
    Result.f32[2] := (V1.f32[0] * V2.f32[1]) - (V1.f32[1] * V2.f32[0]);
    Result.f32[2] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    float32x2_t v1xy = vget_low_f32(V1);
    float32x2_t v2xy = vget_low_f32(V2);

    float32x2_t v1yx = vrev64_f32( v1xy );
    float32x2_t v2yx = vrev64_f32( v2xy );

    float32x2_t v1zz = vdup_lane_f32( vget_high_f32(V1), 0 );
    float32x2_t v2zz = vdup_lane_f32( vget_high_f32(V2), 0 );

    XMVECTOR vResult = vmulq_f32( vcombine_f32(v1yx,v1xy), vcombine_f32(v2zz,v2yx) );
    vResult = vmlsq_f32( vResult, vcombine_f32(v1zz,v1yx), vcombine_f32(v2yx,v2xy) );
    vResult = veorq_u32( vResult, g_XMFlipY );
    return vandq_u32( vResult, g_XMMask3 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // y1,z1,x1,w1
           // vTemp1 = XM_PERMUTE_PS(V1,_MM_SHUFFLE(3,0,2,1));
           MOVUPS  XMM1,[V1]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_0_2_1
           // z2,x2,y2,w2
           //  vTemp2 = XM_PERMUTE_PS(V2,_MM_SHUFFLE(3,1,0,2));
           MOVUPS  XMM2,[V2]
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_3_1_0_2
           // Perform the left operation
           //  vResult = _mm_mul_ps(vTemp1,vTemp2);
           MOVUPS  XMM0,XMM1
           MULPS   XMM0, XMM2
           // z1,x1,y1,w1
           // vTemp1 = XM_PERMUTE_PS(vTemp1,_MM_SHUFFLE(3,0,2,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_0_2_1
           // y2,z2,x2,w2
           // vTemp2 = XM_PERMUTE_PS(vTemp2,_MM_SHUFFLE(3,1,0,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_3_1_0_2
           // Perform the right operation
           // vTemp1 = _mm_mul_ps(vTemp1,vTemp2);
           MULPS   XMM1, XMM2
           // Subract the right from left, and return answer
           // vResult = _mm_sub_ps(vResult,vTemp1);
           SUBPS   XMM0, XMM1
           // Set w to zero
           // return _mm_and_ps(vResult,g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           MOVUPS  [result],XMM0
end;
{$ENDIF}

function XMVector3LengthSq(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3Dot(V, V);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3LengthSq(V);
    Result := XMVectorReciprocalSqrtEst(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot3
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vpadd_f32( v1, v1 );
    v2 = vdup_lane_f32( v2, 0 );
    v1 = vadd_f32( v1, v2 );
    // Reciprocal sqrt (estimate)
    v2 = vrsqrte_f32( v1 );
    return vcombine_f32(v2, v2);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector3ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x7f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $7F
           // return _mm_rsqrt_ps( vTemp );
           RSQRTPS XMM0, XMM0
           MOVUPS  [result], XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector3ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq  = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_and_ps(vLengthSq, g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_rsqrt_ps(vLengthSq);
           RSQRTPS XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y and z
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and y
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,2,1,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_2_1_2
           // x+z, y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1

           // y,y,y,y
           // vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+z+y,??,??,??
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // Splat the length squared
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Get the reciprocal
           // vLengthSq = _mm_rsqrt_ps(vLengthSq);
           RSQRTPS XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result], XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3LengthSq(V);
    Result := XMVectorReciprocalSqrt(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot3
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vpadd_f32( v1, v1 );
    v2 = vdup_lane_f32( v2, 0 );
    v1 = vadd_f32( v1, v2 );
    // Reciprocal sqrt
    float32x2_t  S0 = vrsqrte_f32(v1);
    float32x2_t  P0 = vmul_f32( v1, S0 );
    float32x2_t  R0 = vrsqrts_f32( P0, S0 );
    float32x2_t  S1 = vmul_f32( S0, R0 );
    float32x2_t  P1 = vmul_f32( v1, S1 );
    float32x2_t  R1 = vrsqrts_f32( P1, S1 );
    float32x2_t Result = vmul_f32( S1, R1 );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector3ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x7f );
           MOVUPS  XMM1,[V]
           DPPS    XMM1, XMM1, $7F
           //  vLengthSq = _mm_sqrt_ps( vTemp );
           SQRTPS  XMM1, XMM1
           // return _mm_div_ps( g_XMOne, vLengthSq );
           MOVUPS  XMM0,[g_XMOne]
           DIVPS   XMM0, XMM1
           MOVUPS  [result], XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector3ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vDot = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vDot = _mm_and_ps(vDot, g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           // vDot = _mm_hadd_ps(vDot, vDot);
           HADDPS  XMM0, XMM0
           // vDot = _mm_hadd_ps(vDot, vDot);
           HADDPS  XMM0, XMM0
           // vDot = _mm_sqrt_ps(vDot);
           SQRTPS  XMM1, XMM0
           // vDot = _mm_div_ps(g_XMOne,vDot);
           MOVUPS  XMM0,[g_XMOne]
           DIVPS   XMM0, XMM1
           // return vDot;
           MOVUPS  [result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product
           // vDot = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // x=Dot.y, y=Dot.z
           // vTemp = XM_PERMUTE_PS(vDot,_MM_SHUFFLE(2,1,2,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_1_2_1
           // Result.x = x+y
           //vDot = _mm_add_ss(vDot,vTemp);
           ADDSS   XMM0, XMM1
           // x=Dot.z
           //vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // Result.x = (x+y)+z
           //vDot = _mm_add_ss(vDot,vTemp);
           ADDSS   XMM0, XMM1
           // Splat x
           //vDot = XM_PERMUTE_PS(vDot,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Get the reciprocal
           //vDot = _mm_sqrt_ps(vDot);
           SQRTPS  XMM0, XMM0
           // Get the reciprocal
           //vDot = _mm_div_ps(g_XMOne,vDot);
           MOVUPS  XMM1,[g_XMOne]
           DIVPS   XMM1, XMM0
           // return vDot;
           MOVUPS  [result] ,XMM1
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3LengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3LengthSq(V);
    Result := XMVectorSqrtEst(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3LengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Dot3
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vpadd_f32( v1, v1 );
    v2 = vdup_lane_f32( v2, 0 );
    v1 = vadd_f32( v1, v2 );
    const float32x2_t zero = vdup_n_f32(0);
    uint32x2_t VEqualsZero = vceq_f32( v1, zero );
    // Sqrt (estimate)
    float32x2_t Result = vrsqrte_f32( v1 );
    Result = vmul_f32( v1, Result );
    Result = vbsl_f32( VEqualsZero, zero, Result );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector3LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x7f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $7F
           // return _mm_sqrt_ps( vTemp );
           SQRTPS  XMM0, XMM0
           MOVUPS  [result] ,XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector3LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_and_ps(vLengthSq, g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result] ,XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y and z
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and y
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,2,1,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_2_1_2
           // x+z, y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // y,y,y,y
           // vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+z+y,??,??,??
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // Splat the length squared
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Get the length
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result] ,XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3Length(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3LengthSq(V);
    Result := XMVectorSqrt(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Length(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot3
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vpadd_f32( v1, v1 );
    v2 = vdup_lane_f32( v2, 0 );
    v1 = vadd_f32( v1, v2 );
    const float32x2_t zero = vdup_n_f32(0);
    uint32x2_t VEqualsZero = vceq_f32( v1, zero );
    // Sqrt
    float32x2_t S0 = vrsqrte_f32( v1 );
    float32x2_t P0 = vmul_f32( v1, S0 );
    float32x2_t R0 = vrsqrts_f32( P0, S0 );
    float32x2_t S1 = vmul_f32( S0, R0 );
    float32x2_t P1 = vmul_f32( v1, S1 );
    float32x2_t R1 = vrsqrts_f32( P1, S1 );
    float32x2_t Result = vmul_f32( S1, R1 );
    Result = vmul_f32( v1, Result );
    Result = vbsl_f32( VEqualsZero, zero, Result );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector3Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x7f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $7F
           // return _mm_sqrt_ps( vTemp );
           SQRTPS  XMM0, XMM0
           MOVUPS  [result] ,XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector3Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_and_ps(vLengthSq, g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result] ,XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y and z
           // vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and y
           // vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,2,1,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_2_1_2
           // x+z, y
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // y,y,y,y
           // vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // x+z+y,??,??,??
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // Splat the length squared
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Get the length
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result] ,XMM0
end;
{$ENDIF}




//------------------------------------------------------------------------------
// XMVector3NormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3ReciprocalLength(V);
    Result := XMVectorMultiply(V, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot3
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vpadd_f32( v1, v1 );
    v2 = vdup_lane_f32( v2, 0 );
    v1 = vadd_f32( v1, v2 );
    // Reciprocal sqrt (estimate)
    v2 = vrsqrte_f32( v1 );
    // Normalize
    return vmulq_f32( V, vcombine_f32(v2,v2) );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector3NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0x7f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $7F
           //  vResult = _mm_rsqrt_ps( vTemp );
           RSQRTPS XMM0, XMM0
           // return _mm_mul_ps(vResult, V);
           MULPS   XMM0, [V]
           MOVUPS  [result] ,XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector3NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vDot = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vDot = _mm_and_ps(vDot, g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           // vDot = _mm_hadd_ps(vDot, vDot);
           HADDPS  XMM0, XMM0
           // vDot = _mm_hadd_ps(vDot, vDot);
           HADDPS  XMM0, XMM0
           // vDot = _mm_rsqrt_ps(vDot);
           RSQRTPS XMM0, XMM0
           // vDot = _mm_mul_ps(vDot,V);
           MULPS   XMM0, [V]
           // return vDot;
           MOVUPS  [result] ,XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product
           //  vDot = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // x=Dot.y, y=Dot.z
           //  vTemp = XM_PERMUTE_PS(vDot,_MM_SHUFFLE(2,1,2,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_1_2_1
           // Result.x = x+y
           // vDot = _mm_add_ss(vDot,vTemp);
           ADDSS   XMM0, XMM1
           // x=Dot.z
           // vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // Result.x = (x+y)+z
           // vDot = _mm_add_ss(vDot,vTemp);
           ADDSS   XMM0, XMM1
           // Splat x
           // vDot = XM_PERMUTE_PS(vDot,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Get the reciprocal
           // vDot = _mm_rsqrt_ps(vDot);
           RSQRTPS XMM0, XMM0
           // Perform the normalization
           // vDot = _mm_mul_ps(vDot,V);
           MULPS   XMM0, [V]
           // return vDot;
           MOVUPS  [result] ,XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3Normalize(constref V: TXMVECTOR): TXMVECTOR;
var
    fLength: single;
    vResult: TXMVECTOR;
begin
    vResult := XMVector3Length(V);
    fLength := vResult.f32[0];

    // Prevent divide by zero
    if (fLength > 0) then
        fLength := 1.0 / fLength;

    Result.f32[0] := V.f32[0] * fLength;
    Result.f32[1] := V.f32[1] * fLength;
    Result.f32[2] := V.f32[2] * fLength;
    Result.f32[3] := V.f32[3] * fLength;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Normalize(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Dot3
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vpadd_f32( v1, v1 );
    v2 = vdup_lane_f32( v2, 0 );
    v1 = vadd_f32( v1, v2 );
    uint32x2_t VEqualsZero = vceq_f32( v1, vdup_n_f32(0) );
    uint32x2_t VEqualsInf = vceq_f32( v1, vget_low_f32(g_XMInfinity) );
    // Reciprocal sqrt (2 iterations of Newton-Raphson)
    float32x2_t S0 = vrsqrte_f32( v1 );
    float32x2_t P0 = vmul_f32( v1, S0 );
    float32x2_t R0 = vrsqrts_f32( P0, S0 );
    float32x2_t S1 = vmul_f32( S0, R0 );
    float32x2_t P1 = vmul_f32( v1, S1 );
    float32x2_t R1 = vrsqrts_f32( P1, S1 );
    v2 = vmul_f32( S1, R1 );
    // Normalize
    XMVECTOR vResult = vmulq_f32( V, vcombine_f32(v2,v2) );
    vResult = vbslq_f32( vcombine_f32(VEqualsZero,VEqualsZero), vdupq_n_f32(0), vResult );
    return vbslq_f32( vcombine_f32(VEqualsInf,VEqualsInf), g_XMQNaN, vResult );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector3Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vLengthSq = _mm_dp_ps( V, V, 0x7f );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $7F
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Create zero with a single instruction
           //  vZeroMask = _mm_setzero_ps();
           XORPS   XMM2, XMM2
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM2, XMM1, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Divide to perform the normalization
           // vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM3,[V]
           DIVPS   XMM3, XMM1
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM3, XMM2
           // Select qnan or result based on infinite length
           //  vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM2, [g_XMQNaN]
           ANDNPS  XMM2, XMM0
           //  vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM3, XMM0
           // vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM3, XMM2
           // return vResult;
           MOVUPS  [result] ,XMM3
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector3Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y and z only
           //  vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_and_ps(vLengthSq, g_XMMask3);
           ANDPS   XMM0, [g_XMMask3]
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           //  vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Create zero with a single instruction
           //  vZeroMask = _mm_setzero_ps();
           XORPS   XMM2, XMM2
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM2, XMM1, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Divide to perform the normalization
           // vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM3, [V]
           DIVPS   XMM3, XMM1
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM1, XMM2
           // Select qnan or result based on infinite length
           //  vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM2, [g_XMQNaN]
           ANDNPS  XMM2, XMM0
           //  vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM0, XMM1
           // vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM0, XMM2
           // return vResult;
           MOVUPS  [result] ,XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y and z only
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,1,2,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_1_2_1
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM2, XMM0
           // Create zero with a single instruction
           //  vZeroMask = _mm_setzero_ps();
           XORPS   XMM1, XMM1
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM1, XMM2, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Divide to perform the normalization
           // vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM3,[V]
           DIVPS   XMM3, XMM2
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM3, XMM1
           // Select qnan or result based on infinite length
           //  vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM1,[g_XMQNaN]
           ANDNPS  XMM1, XMM0
           //  vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM3, XMM0
           // vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM1, XMM3
           // return vResult;
           MOVUPS  [result] ,XMM1
end;
{$ENDIF}


function XMVector3ClampLength(constref V: TXMVECTOR; constref LengthMin: single; constref LengthMax: single): TXMVECTOR;
var
    ClampMax, ClampMin: TXMVECTOR;
begin
    ClampMax := XMVectorReplicate(LengthMax);
    ClampMin := XMVectorReplicate(LengthMin);

    Result := XMVector3ClampLengthV(V, ClampMin, ClampMax);
end;


function XMVector3ClampLengthV(constref V: TXMVECTOR; constref LengthMin: TXMVECTOR; constref LengthMax: TXMVECTOR): TXMVECTOR;
var
    LengthSq, Zero, RcpLength, InfiniteLength, ZeroLength, Normal, Length, Select: TXMVECTOR;
    ControlMax, ControlMin, ClampLength, Control: TXMVECTOR;
begin
    assert((XMVectorGetY(LengthMin) = XMVectorGetX(LengthMin)) and (XMVectorGetZ(LengthMin) = XMVectorGetX(LengthMin)));
    assert((XMVectorGetY(LengthMax) = XMVectorGetX(LengthMax)) and (XMVectorGetZ(LengthMax) = XMVectorGetX(LengthMax)));
    assert(XMVector3GreaterOrEqual(LengthMin, XMVectorZero()));
    assert(XMVector3GreaterOrEqual(LengthMax, XMVectorZero()));
    assert(XMVector3GreaterOrEqual(LengthMax, LengthMin));

    LengthSq := XMVector3LengthSq(V);

    Zero := XMVectorZero();

    RcpLength := XMVectorReciprocalSqrt(LengthSq);

    InfiniteLength := XMVectorEqualInt(LengthSq, g_XMInfinity.v);
    ZeroLength := XMVectorEqual(LengthSq, Zero);

    Normal := XMVectorMultiply(V, RcpLength);

    Length := XMVectorMultiply(LengthSq, RcpLength);

    Select := XMVectorEqualInt(InfiniteLength, ZeroLength);
    Length := XMVectorSelect(LengthSq, Length, Select);
    Normal := XMVectorSelect(LengthSq, Normal, Select);


    ControlMax := XMVectorGreater(Length, LengthMax);
    ControlMin := XMVectorLess(Length, LengthMin);

    ClampLength := XMVectorSelect(Length, LengthMax, ControlMax);
    ClampLength := XMVectorSelect(ClampLength, LengthMin, ControlMin);

    Result := XMVectorMultiply(Normal, ClampLength);

    // Preserve the original vector (with no precision loss) if the length falls within the given range
    Control := XMVectorEqualInt(ControlMax, ControlMin);
    Result := XMVectorSelect(Result, V, Control);
end;



function XMVector3Reflect(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR): TXMVECTOR;
begin
    // Result  :=  Incident - (2 * dot(Incident, Normal)) * Normal
    Result := XMVector3Dot(Incident, Normal);
    Result := XMVectorAdd(Result, Result);
    Result := XMVectorNegativeMultiplySubtract(Result, Normal, Incident);
end;



function XMVector3Refract(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: single): TXMVECTOR;
var
    Index: TXMVECTOR;
begin
    Index := XMVectorReplicate(RefractionIndex);
    Result := XMVector3RefractV(Incident, Normal, Index);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3RefractV(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: TXMVECTOR): TXMVECTOR;
var
    Zero, IDotN, R: TXMVECTOR;
begin
    // Result  :=  RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))
    Zero := XMVectorZero();

    IDotN := XMVector3Dot(Incident, Normal);

    // R  :=  1.0  - RefractionIndex * RefractionIndex * (1.0  - IDotN * IDotN)
    R := XMVectorNegativeMultiplySubtract(IDotN, IDotN, g_XMOne.v);
    R := XMVectorMultiply(R, RefractionIndex);
    R := XMVectorNegativeMultiplySubtract(R, RefractionIndex, g_XMOne.v);

    if (XMVector4LessOrEqual(R, Zero)) then
    begin
        // Total internal reflection
        Result := Zero;
    end
    else
    begin
        // R  :=  RefractionIndex * IDotN + sqrt(R)
        R := XMVectorSqrt(R);
        R := XMVectorMultiplyAdd(RefractionIndex, IDotN, R);

        // Result  :=  RefractionIndex * Incident - Normal * R
        Result := XMVectorMultiply(RefractionIndex, Incident);
        Result := XMVectorNegativeMultiplySubtract(Normal, R, Result);
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3RefractV(Incident: TXMVECTOR; Normal: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    XMVECTOR IDotN = XMVector3Dot(Incident,Normal);

    // R = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    float32x4_t R = vmlsq_f32( g_XMOne, IDotN, IDotN);
    R = vmulq_f32(R, RefractionIndex);
    R = vmlsq_f32(g_XMOne, R, RefractionIndex );

    uint32x4_t vResult = vcleq_f32(R,g_XMZero);
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    if ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU )
    {
        // Total internal reflection
        vResult = g_XMZero;
    }
    else
    {
        // Sqrt(R)
        float32x4_t S0 = vrsqrteq_f32(R);
        float32x4_t P0 = vmulq_f32( R, S0 );
        float32x4_t R0 = vrsqrtsq_f32( P0, S0 );
        float32x4_t S1 = vmulq_f32( S0, R0 );
        float32x4_t P1 = vmulq_f32( R, S1 );
        float32x4_t R1 = vrsqrtsq_f32( P1, S1 );
        float32x4_t S2 = vmulq_f32( S1, R1 );
        R = vmulq_f32( R, S2 );
        // R = RefractionIndex * IDotN + sqrt(R)
        R = vmlaq_f32( R, RefractionIndex, IDotN );
        // Result = RefractionIndex * Incident - Normal * R
        vResult = vmulq_f32(RefractionIndex, Incident);
        vResult = vmlsq_f32( vResult, R, Normal );
    }
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3RefractV(constref Incident: TXMVECTOR; constref Normal: TXMVECTOR; constref RefractionIndex: TXMVECTOR): TXMVECTOR; inline;
var
    IDotN: TXMVECTOR;
begin
    // Result = RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))
    IDotN := XMVector3Dot(Incident, Normal);
    // R = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    asm
               //  R = _mm_mul_ps(IDotN, IDotN);
               MOVUPS  XMM0, [IDotN]
               MULPS   XMM0, XMM0
               // R = _mm_sub_ps(g_XMOne,R);
               MOVUPS  XMM1, [g_XMOne]
               SUBPS   XMM1, XMM0
               // R = _mm_mul_ps(R, RefractionIndex);
               MULPS   XMM1, [RefractionIndex]
               // R = _mm_mul_ps(R, RefractionIndex);
               MULPS   XMM1, [RefractionIndex]
               // R = _mm_sub_ps(g_XMOne,R);
               MOVUPS  XMM0, [g_XMOne]
               SUBPS   XMM0, XMM1
               MOVUPS  XMM1,XMM0 // XMM1 = R
               //  vResult = _mm_cmple_ps(R,g_XMZero);
               CMPPS   XMM0, [g_XMZero], 2


               MOVMSKPS EAX, XMM0
               CMP     EAX, $0F
               JE      @InternalReflection  // if (_mm_movemask_ps(vResult)==0x0f) -> Total internal reflection
               // R = RefractionIndex * IDotN + sqrt(R)
               // R = _mm_sqrt_ps(R);
               sqrtps XMM1, XMM1
               // vResult = _mm_mul_ps(RefractionIndex,IDotN);
               MOVUPS  XMM2, [RefractionIndex]
               MULPS   XMM2, [IDotN]
               // R = _mm_add_ps(R,vResult);
               ADDPS   XMM1, XMM2
               // Result = RefractionIndex * Incident - Normal * R
               // vResult = _mm_mul_ps(RefractionIndex, Incident);
               MOVUPS  XMM0, [RefractionIndex]
               MULPS   XMM0, [Incident]
               // R = _mm_mul_ps(R,Normal);
               MULPS   XMM1, [Normal]
               // vResult = _mm_sub_ps(vResult,R);
               SUBPS   XMM0, XMM1
               JMP     @Finished

               @InternalReflection:
               MOVUPS  XMM0, [g_XMZero] // vResult = g_XMZero;
               @Finished:
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}


function XMVector3Orthogonal(constref V: TXMVECTOR): TXMVECTOR;
var
    Zero, Z, YZYY, NegativeV, ZIsNegative, YZYYIsNegative, S, D, Select, R0, R1: TXMVECTOR;
begin
    Zero := XMVectorZero();
    Z := XMVectorSplatZ(V);
    YZYY := XMVectorSwizzle(V, XM_SWIZZLE_Y, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y);

    NegativeV := XMVectorSubtract(Zero, V);

    ZIsNegative := XMVectorLess(Z, Zero);
    YZYYIsNegative := XMVectorLess(YZYY, Zero);

    S := XMVectorAdd(YZYY, Z);
    D := XMVectorSubtract(YZYY, Z);

    Select := XMVectorEqualInt(ZIsNegative, YZYYIsNegative);

    R0 := XMVectorPermute(NegativeV, S, XM_PERMUTE_1X, XM_PERMUTE_0X, XM_PERMUTE_0X, XM_PERMUTE_0X);
    R1 := XMVectorPermute(V, D, XM_PERMUTE_1X, XM_PERMUTE_0X, XM_PERMUTE_0X, XM_PERMUTE_0X);

    Result := XMVectorSelect(R1, R0, Select);
end;



function XMVector3AngleBetweenNormalsEst(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3Dot(N1, N2);
    Result := XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result := XMVectorACosEst(Result);
end;



function XMVector3AngleBetweenNormals(constref N1: TXMVECTOR; constref N2: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector3Dot(N1, N2);
    Result := XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result := XMVectorACos(Result);
end;


function XMVector3AngleBetweenVectors(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    L1, L2, Dot, CosAngle: TXMVECTOR;
begin
    L1 := XMVector3ReciprocalLength(V1);
    L2 := XMVector3ReciprocalLength(V2);

    Dot := XMVector3Dot(V1, V2);

    L1 := XMVectorMultiply(L1, L2);

    CosAngle := XMVectorMultiply(Dot, L1);
    CosAngle := XMVectorClamp(CosAngle, g_XMNegativeOne.v, g_XMOne.v);

    Result := XMVectorACos(CosAngle);
end;


function XMVector3LinePointDistance(constref LinePoint1: TXMVECTOR; constref LinePoint2: TXMVECTOR; constref Point: TXMVECTOR): TXMVECTOR;
var
    PointVector, LineVector, LengthSq, PointProjectionScale: TXMVECTOR;
    DistanceVector: TXMVECTOR;
begin
    // Given a vector PointVector from LinePoint1 to Point and a vector
    // LineVector from LinePoint1 to LinePoint2, the scaled distance
    // PointProjectionScale from LinePoint1 to the perpendicular projection
    // of PointVector onto the line is defined as:

    //     PointProjectionScale  :=  dot(PointVector, LineVector) / LengthSq(LineVector)

    PointVector := XMVectorSubtract(Point, LinePoint1);
    LineVector := XMVectorSubtract(LinePoint2, LinePoint1);

    LengthSq := XMVector3LengthSq(LineVector);

    PointProjectionScale := XMVector3Dot(PointVector, LineVector);
    PointProjectionScale := XMVectorDivide(PointProjectionScale, LengthSq);

    DistanceVector := XMVectorMultiply(LineVector, PointProjectionScale);
    DistanceVector := XMVectorSubtract(PointVector, DistanceVector);

    Result := XMVector3Length(DistanceVector);
end;


procedure XMVector3ComponentsFromNormal(out pParallel: TXMVECTOR; out pPerpendicular: TXMVECTOR; constref V: TXMVECTOR; constref Normal: TXMVECTOR);
var
    Scale: TXMVECTOR;
begin
    Scale := XMVector3Dot(V, Normal);

    pParallel := XMVectorMultiply(Normal, Scale);
    pPerpendicular := XMVectorSubtract(V, pParallel);
end;



//------------------------------------------------------------------------------
// Transform a vector using a rotation expressed as a unit quaternion
function XMVector3Rotate(constref V: TXMVECTOR; constref RotationQuaternion: TXMVECTOR): TXMVECTOR;
var
    A, Q: TXMVECTOR;
begin
    A := XMVectorSelect(g_XMSelect1110.v, V, g_XMSelect1110.v);
    Q := XMQuaternionConjugate(RotationQuaternion);
    Result := XMQuaternionMultiply(Q, A);
    Result := XMQuaternionMultiply(Result, RotationQuaternion);
end;


//------------------------------------------------------------------------------
// Transform a vector using the inverse of a rotation expressed as a unit quaternion
function XMVector3InverseRotate(constref V: TXMVECTOR; constref RotationQuaternion: TXMVECTOR): TXMVECTOR;
var
    A, Q: TXMVECTOR;
begin
    A := XMVectorSelect(g_XMSelect1110.v, V, g_XMSelect1110.v);
    Result := XMQuaternionMultiply(RotationQuaternion, A);
    Q := XMQuaternionConjugate(RotationQuaternion);
    Result := XMQuaternionMultiply(Result, Q);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
var
    X, Y, Z: TXMVECTOR;
begin
    Z := XMVectorSplatZ(V);
    Y := XMVectorSplatY(V);
    X := XMVectorSplatX(V);

    Result := XMVectorMultiplyAdd(Z, M.r[2], M.r[3]);
    Result := XMVectorMultiplyAdd(Y, M.r[1], Result);
    Result := XMVectorMultiplyAdd(X, M.r[0], Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3Transform(V: TXMVECTOR; M: TXMMATRIX): TXMVECTOR;
begin
    (* ToDo
     float32x2_t VL = vget_low_f32( V );
    XMVECTOR vResult = vmlaq_lane_f32( M.r[3], M.r[0], VL, 0 ); // X
    vResult = vmlaq_lane_f32( vResult, M.r[1], VL, 1 ); // Y
    return vmlaq_lane_f32( vResult, M.r[2], vget_high_f32( V ), 0 ); // Z
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR; assembler;
asm
           //  vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vResult = _mm_mul_ps(vResult,M.r[0]);
           MULPS   XMM0, TXMMATRIX([M]).r0
           //  vTemp = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM1, [V]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vTemp = _mm_mul_ps(vTemp,M.r[1]);
           MULPS   XMM1, TXMMATRIX([M]).r1
           // vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM0, XMM1
           // vTemp = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           MOVUPS  XMM1, [V]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_2_2_2
           // vTemp = _mm_mul_ps(vTemp,M.r[2]);
           MULPS   XMM1, TXMMATRIX([M]).r2
           //  vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM0, XMM1
           // vResult = _mm_add_ps(vResult,M.r[3]);
           ADDPS   XMM0, TXMMATRIX([M]).r3
           // return vResult;
           MOVUPS  [result] ,XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3TransformStream(out pOutputStream: PXMFLOAT4; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT4;
var
    pInputVector: array of TXMFLOAT3 absolute pInputStream;
    pOutputVector: array of TXMFLOAT4;
    row0, row1, row2, row3: TXMVECTOR;
    i: size_t;
    V, Z, X, Y: TXMVECTOR;
    vResult: TXMVECTOR;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);

    assert(InputStride >= sizeof(TXMFLOAT3));

    assert(OutputStride >= sizeof(TXMFLOAT4));


    SetLength(pOutputVector, VectorCount);
    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];
    row3 := M.r[3];

    for i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat3(pInputVector[i]);
        Z := XMVectorSplatZ(V);
        Y := XMVectorSplatY(V);
        X := XMVectorSplatX(V);

        vResult := XMVectorMultiplyAdd(Z, row2, row3);
        vResult := XMVectorMultiplyAdd(Y, row1, vResult);
        vResult := XMVectorMultiplyAdd(X, row0, vResult);

        XMStoreFloat4(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := pOutputStream;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3TransformStream(out pOutputStream: PXMFLOAT4; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT4;
begin
    (* ToDo
     const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT3)) && (OutputStride == sizeof(XMFLOAT4)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x3_t V = vld3q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT3)*4;

                float32x2_t r3 = vget_low_f32( row3 );
                float32x2_t r = vget_low_f32( row0 );
                XMVECTOR vResult0 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Ax+M
                XMVECTOR vResult1 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Bx+N

                __prefetch( pInputVector );

                r3 = vget_high_f32( row3 );
                r = vget_high_f32( row0 );
                XMVECTOR vResult2 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Cx+O
                XMVECTOR vResult3 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Dx+P

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( row1 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( row1 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[1], r, 0 ); // Cx+Gy+O
                vResult3 = vmlaq_lane_f32( vResult3, V.val[1], r, 1 ); // Dx+Hy+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                r = vget_low_f32( row2 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[2], r, 0 ); // Ax+Ey+Iz+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[2], r, 1 ); // Bx+Fy+Jz+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*4) );

                r = vget_high_f32( row2 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[2], r, 0 ); // Cx+Gy+Kz+O
                vResult3 = vmlaq_lane_f32( vResult3, V.val[2], r, 1 ); // Dx+Hy+Lz+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*5) );

                float32x4x4_t R;
                R.val[0] = vResult0;
                R.val[1] = vResult1;
                R.val[2] = vResult2;
                R.val[3] = vResult3;

                vst4q_f32( reinterpret_cast<float*>(pOutputVector), R );
                pOutputVector += sizeof(XMFLOAT4)*4;

                i += 4;
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        float32x2_t VL = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
        float32x2_t zero = vdup_n_f32(0);
        float32x2_t VH = vld1_lane_f32( reinterpret_cast<const float*>(pInputVector)+2, zero, 0 );
        pInputVector += InputStride;

        XMVECTOR vResult = vmlaq_lane_f32( row3, row0, VL, 0 ); // X
        vResult = vmlaq_lane_f32( vResult, row1, VL, 1); // Y
        vResult = vmlaq_lane_f32( vResult, row2, VH, 0 ); // Z

        vst1q_f32( reinterpret_cast<float*>(pOutputVector), vResult );
        pOutputVector += OutputStride;
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3TransformStream(out pOutputStream: PXMFLOAT4; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT4;
var
    pInputVector, pOutputVector: pointer;
    row0, row1, row2, row3: TXMVECTOR;
    i, four, j: size_t;
    s: uint32;
    V, V1, L2, L3, V2, V3, V4: TXMVECTOR;
begin
    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];
    row3 := M.r[3];

    i := 0;
    four := VectorCount shr 2;
    if (four > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT3)) then
        begin
            if (not (uintptr(pOutputStream) and $F = $F) and not (OutputStride and $F = $F)) then
            begin
                // Packed input, aligned output
                s := sizeof(TXMFLOAT3) * 4;
                for j := 0 to four - 1 do
                begin
                    asm
                               // V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM0, [pInputVector]
                               MOVUPS  [V1],XMM0
                               //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                               MOVUPS  XMM1, [pInputVector+16]
                               //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                               MOVUPS  XMM2, [pInputVector+32]
                               // pInputVector += sizeof(XMFLOAT3)*4;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Unpack the 4 vectors (.w components are junk)
                               // XM3UNPACK3INTO4(V1,L2,L3);
                               MOVUPS  XMM3, XMM1
                               SHUFPS  XMM3, XMM2, _MM_SHUFFLE_0_0_3_2 // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                               MOVUPS  [V3],XMM3
                               SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_1_0 // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));

                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_0_2 // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                               MOVUPS  [V2] , XMM1

                               PSRLDQ  XMM2, 4 //  V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                               MOVUPS  [V4] , XMM2

                               // Result 1
                               //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM2, [V1]
                               MOVUPS  XMM1, XMM2
                               MOVUPS  XMM0, XMM2
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               //  vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               //  vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM2, [V2]
                               MOVUPS  XMM1, XMM2
                               MOVUPS  XMM0, XMM2
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 3
                               // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM2, [V3]
                               MOVUPS  XMM1, XMM2
                               MOVUPS  XMM0, XMM2
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 4
                               // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM2, [V4]
                               MOVUPS  XMM1, XMM2
                               MOVUPS  XMM0, XMM2
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1

                               // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 4);
                end;
            end
            else
            begin
                // Packed input, unaligned output
                s := sizeof(TXMFLOAT3) * 4;
                for j := 0 to four - 1 do
                begin
                    asm
                               //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM0, [pInputVector]
                               MOVUPS  [V1],XMM0
                               //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                               MOVUPS  XMM1, [pInputVector+16]
                               //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                               MOVUPS  XMM2, [pInputVector+32]
                               // pInputVector += sizeof(XMFLOAT3)*4;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Unpack the 4 vectors (.w components are junk)
                               // XM3UNPACK3INTO4(V1,L2,L3);
                               MOVUPS  XMM3, XMM1
                               SHUFPS  XMM3, XMM2, _MM_SHUFFLE_0_0_3_2  // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                               MOVUPS  [V3],XMM3
                               SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_1_0  // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_0_2 // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                               MOVUPS  [V2],XMM1
                               PSRLDQ  XMM2, 4   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                               MOVUPS  [V4],XMM2

                               // Result 1
                               MOVUPS  XMM0,[V1]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               // Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               //  vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               //  vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V2]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector], XMM2
                               //  pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 3
                               // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V3]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 4
                               // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V4]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0
                               // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 4);
                end;
            end;
        end;
    end;

    if (not (uintptr(pOutputStream) and $F = $F) and not (OutputStride and $F = $F)) then
    begin
        // Aligned output
        // for (; i < VectorCount; ++i)
        while i < VectorCount do
        begin
            V := XMLoadFloat3(pInputVector);
            asm

                       // pInputVector += InputStride;
                       MOV     EDX, pInputVector
                       ADD     EDX, [InputStride]
                       MOV     pInputVector,EDX

                       //  Z = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                       MOVUPS  XMM0,[V]
                       MOVUPS  XMM1,XMM0
                       MOVUPS  XMM2,XMM0
                       SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                       //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                       //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                       //  vTemp = _mm_mul_ps( Z, row2 );
                       MULPS   XMM2, [row2]
                       //  vTemp2 = _mm_mul_ps( Y, row1 );
                       MULPS   XMM1, [row1]
                       //  vTemp3 = _mm_mul_ps( X, row0 );
                       MULPS   XMM0, [row0]
                       // vTemp = _mm_add_ps( vTemp, row3 );
                       ADDPS   XMM2, [row3]
                       // vTemp = _mm_add_ps( vTemp, vTemp2 );
                       ADDPS   XMM2, XMM1
                       // vTemp = _mm_add_ps( vTemp, vTemp3 );
                       ADDPS   XMM2, XMM0

                       // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTemp );
                       MOVUPS  [pOutputVector],XMM2
                       // pOutputVector += OutputStride;
                       MOV     EDX, pOutputVector
                       ADD     EDX, [OutputStride]
                       MOV     pOutputVector,EDX
            end;
            Inc(i);
        end;
    end
    else
    begin
        // Unaligned output
        // for (; i < VectorCount; ++i)
        while i < VectorCount do
        begin
            V := XMLoadFloat3(pInputVector);
            asm

                       // pInputVector += InputStride;
                       MOV     EDX, pInputVector
                       ADD     EDX, [InputStride]
                       MOV     pInputVector,EDX

                       //  Z = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                       MOVUPS  XMM0,[V]
                       MOVUPS  XMM1,XMM0
                       MOVUPS  XMM2,XMM0
                       SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                       //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                       SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                       //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                       SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                       //  vTemp = _mm_mul_ps( Z, row2 );
                       MULPS   XMM2, [row2]
                       //  vTemp2 = _mm_mul_ps( Y, row1 );
                       MULPS   XMM1, [row1]
                       //  vTemp3 = _mm_mul_ps( X, row0 );
                       MULPS   XMM0, [row0]
                       // vTemp = _mm_add_ps( vTemp, row3 );
                       ADDPS   XMM2, [row3]
                       // vTemp = _mm_add_ps( vTemp, vTemp2 );
                       ADDPS   XMM2, XMM1
                       // vTemp = _mm_add_ps( vTemp, vTemp3 );
                       ADDPS   XMM2, XMM0

                       // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                       MOVUPS  [pOutputVector],XMM2
                       // pOutputVector += OutputStride;
                       MOV     EDX, pOutputVector
                       ADD     EDX, [OutputStride]
                       MOV     pOutputVector,EDX
            end;
            Inc(i);
        end;
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}

function XMVector3TransformCoord(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
var
    X, Y, Z, W: TXMVECTOR;
begin
    Z := XMVectorSplatZ(V);
    Y := XMVectorSplatY(V);
    X := XMVectorSplatX(V);

    Result := XMVectorMultiplyAdd(Z, M.r[2], M.r[3]);
    Result := XMVectorMultiplyAdd(Y, M.r[1], Result);
    Result := XMVectorMultiplyAdd(X, M.r[0], Result);

    W := XMVectorSplatW(Result);
    Result := XMVectorDivide(Result, W);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3TransformCoordStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT3;
var
    pInputVector: array of TXMFLOAT3 absolute pInputStream;
    pOutputVector: array of TXMFLOAT3;
    row0, row1, row2, row3: TXMVECTOR;
    i: size_t;
    V, Z, X, Y, W: TXMVECTOR;
    vResult: TXMVECTOR;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);

    assert(InputStride >= sizeof(TXMFLOAT3));

    assert(OutputStride >= sizeof(TXMFLOAT3));

    SetLength(pOutputVector, VectorCount);


    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];
    row3 := M.r[3];

    for  i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat3(pInputVector[i]);
        Z := XMVectorSplatZ(V);
        Y := XMVectorSplatY(V);
        X := XMVectorSplatX(V);

        vResult := XMVectorMultiplyAdd(Z, row2, row3);
        vResult := XMVectorMultiplyAdd(Y, row1, vResult);
        vResult := XMVectorMultiplyAdd(X, row0, vResult);

        W := XMVectorSplatW(vResult);

        vResult := XMVectorDivide(vResult, W);

        XMStoreFloat3(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := pOutputStream;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3TransformCoordStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT3;
begin
    (* ToDo
     const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT3)) && (OutputStride == sizeof(XMFLOAT3)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x3_t V = vld3q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT3)*4;

                float32x2_t r3 = vget_low_f32( row3 );
                float32x2_t r = vget_low_f32( row0 );
                XMVECTOR vResult0 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Ax+M
                XMVECTOR vResult1 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Bx+N

                __prefetch( pInputVector );

                r3 = vget_high_f32( row3 );
                r = vget_high_f32( row0 );
                XMVECTOR vResult2 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Cx+O
                XMVECTOR W = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Dx+P

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( row1 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( row1 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[1], r, 0 ); // Cx+Gy+O
                W = vmlaq_lane_f32( W, V.val[1], r, 1 ); // Dx+Hy+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                r = vget_low_f32( row2 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[2], r, 0 ); // Ax+Ey+Iz+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[2], r, 1 ); // Bx+Fy+Jz+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*4) );

                r = vget_high_f32( row2 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[2], r, 0 ); // Cx+Gy+Kz+O
                W = vmlaq_lane_f32( W, V.val[2], r, 1 ); // Dx+Hy+Lz+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*5) );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
                V.val[0] = vdivq_f32( vResult0, W );
                V.val[1] = vdivq_f32( vResult1, W );
                V.val[2] = vdivq_f32( vResult2, W );
#else
                // 2 iterations of Newton-Raphson refinement of reciprocal
                float32x4_t Reciprocal = vrecpeq_f32(W);
                float32x4_t S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );
                S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );

                V.val[0] = vmulq_f32( vResult0, Reciprocal );
                V.val[1] = vmulq_f32( vResult1, Reciprocal );
                V.val[2] = vmulq_f32( vResult2, Reciprocal );
#endif

                vst3q_f32( reinterpret_cast<float*>(pOutputVector),V );
                pOutputVector += sizeof(XMFLOAT3)*4;

                i += 4;
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        float32x2_t VL = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
        float32x2_t zero = vdup_n_f32(0);
        float32x2_t VH = vld1_lane_f32( reinterpret_cast<const float*>(pInputVector)+2, zero, 0 );
        pInputVector += InputStride;

        XMVECTOR vResult = vmlaq_lane_f32( row3, row0, VL, 0 ); // X
        vResult = vmlaq_lane_f32( vResult, row1, VL, 1 ); // Y
        vResult = vmlaq_lane_f32( vResult, row2, VH, 0 ); // Z

        VH = vget_high_f32(vResult);
        XMVECTOR W = vdupq_lane_f32( VH, 1 );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
        vResult = vdivq_f32( vResult, W );
#else
        // 2 iterations of Newton-Raphson refinement of reciprocal for W
        float32x4_t Reciprocal = vrecpeq_f32( W );
        float32x4_t S = vrecpsq_f32( Reciprocal, W );
        Reciprocal = vmulq_f32( S, Reciprocal );
        S = vrecpsq_f32( Reciprocal, W );
        Reciprocal = vmulq_f32( S, Reciprocal );

        vResult = vmulq_f32( vResult, Reciprocal );
#endif

        VL = vget_low_f32( vResult );
        vst1_f32( reinterpret_cast<float*>(pOutputVector), VL );
        vst1q_lane_f32( reinterpret_cast<float*>(pOutputVector)+2, vResult, 2 );
        pOutputVector += OutputStride;
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3TransformCoordStream(out pOutputStream: PXMFLOAT3; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT3;
var
    pInputVector, pOutputVector: pointer;
    row0, row1, row2, row3: TXMVECTOR;
    i, j, four: size_t;
    s: uint32;
    V, V1, V2, V3, V4: TXMVECTOR;
begin
    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];
    row3 := M.r[3];

    i := 0;
    four := VectorCount shr 2;
    if (four > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT3)) then
        begin
            if (OutputStride = sizeof(TXMFLOAT3)) then
            begin
                if (not (uintptr(pOutputStream) and $F = $F)) then
                begin
                    // Packed input, aligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for j := 0 to four - 1 do
                    begin
                        asm
                                   //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]
                                   MOVUPS  [V1], XMM1
                                   //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   // pInputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   // XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3], XMM4
                                   // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [V2], XMM2
                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [V4], XMM3

                                   // Result 1
                                   //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V1]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   //  vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   //  vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3

                                   //  V1 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM4
                                   MOVUPS  [V1], XMM2

                                   // Result 2
                                   // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V2]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3

                                   // V2 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM4
                                   MOVUPS  [V2], XMM2

                                   // Result 3
                                   // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V3]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2

                                   // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3

                                   // V3 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM4
                                   MOVUPS  [V3], XMM2

                                   // Result 4
                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V4]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM4, _MM_SHUFFLE_3_3_3_3

                                   // V4 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM4
                                   MOVUPS  [V4], XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);


                                   MOVUPS  XMM0, [v2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // vTemp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));



                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector], XMM1
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector+16], XMM0
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector+32], XMM3
                                   //  pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        i += 4;
                    end;
                end
                else
                begin
                    // Packed input, unaligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for j := 0 to four - 1 do
                    begin
                        asm
                                   //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]
                                   //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   //  pInputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   // XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4, XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3], XMM4
                                   // V2 := _mm_shuffle_ps(l2,V1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [v2],XMM2
                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [v4],XMM3

                                   // Result 1
                                   //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V1]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   //  vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   //  vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   // V1 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V1],XMM2

                                   // Result 2
                                   // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V2]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   // V2 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V2],XMM2

                                   // Result 3
                                   // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V3]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3, XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   // V3 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V3],XMM2

                                   // Result 4
                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V4]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, row3 );
                                   ADDPS   XMM2, [row3]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM2,XMM3
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                                   // V4 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V4],XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);
                                   MOVUPS  XMM0, [V2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // temp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));



                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector], XMM1
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector+16], XMM0
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector+32], XMM3
                                   // pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 4);
                    end;
                end;
            end
            else
            begin
                // Packed input, unpacked output
                s := sizeof(TXMFLOAT3) * 4;
                for  j := 0 to four - 1 do
                begin
                    asm
                               //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM1, [pInputVector]
                               MOVUPS  [V1],XMM1
                               //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                               MOVUPS  XMM2, [pInputVector+16]
                               //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                               MOVUPS  XMM3, [pInputVector+32]
                               // pInputVector += sizeof(TXMFLOAT3)*4;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Unpack the 4 vectors (.w components are junk)
                               // XM3UNPACK3INTO4(V1,L2,L3);
                               // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                               MOVUPS  XMM4,XMM2
                               SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                               MOVUPS  [V3],XMM4
                               // V2 := _mm_shuffle_ps(l2,V1,_MM_SHUFFLE(3,3,1,0));
                               SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                               // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                               MOVUPS  [V2],XMM2
                               // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                               PSRLDQ  XMM3, 4
                               MOVUPS  [V4],XMM3

                               // Result 1
                               //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V1]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               //  vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               //  vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3
                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V2]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3
                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector],XMM2
                               //  pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 3
                               // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V3]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3
                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 4
                               // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V4]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, row3 );
                               ADDPS   XMM2, [row3]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3
                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    i += 4;
                end;
            end;
        end;
    end;

    //for (; i < VectorCount; i++)
    while i < VectorCount do
    begin
        V := XMLoadFloat3(pInputVector);
        asm
                   // pInputVector += InputStride;
                   MOV     EDX, pInputVector
                   ADD     EDX, [InputStride]
                   MOV     pInputVector,EDX

                   //  Z = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                   MOVUPS  XMM2, [V]
                   MOVUPS  XMM1, XMM2
                   MOVUPS  XMM0, XMM2
                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                   //  vTemp = _mm_mul_ps( Z, row2 );
                   MULPS   XMM2, [row2]
                   //  vTemp2 = _mm_mul_ps( Y, row1 );
                   MULPS   XMM1, [row1]
                   //  vTemp3 = _mm_mul_ps( X, row0 );
                   MULPS   XMM0, [row0]
                   // vTemp = _mm_add_ps( vTemp, row3 );
                   ADDPS   XMM2, [row3]
                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                   ADDPS   XMM2, XMM1
                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                   ADDPS   XMM2, XMM0

                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                   MOVUPS  XMM3,XMM2
                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                   // vTemp = _mm_div_ps( vTemp, W );
                   DIVPS   XMM2, XMM3

                   // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                   MOVUPS  [pOutputVector], XMM2
                   // pOutputVector += OutputStride;
                   MOV     EDX, pOutputVector
                   ADD     EDX, [OutputStride]
                   MOV     pOutputVector,EDX
        end;
        Inc(i);
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3TransformNormal(V: TXMVECTOR; M: TXMMATRIX): TXMVECTOR;
var
    X, Y, Z: TXMVECTOR;
begin
    Z := XMVectorSplatZ(V);
    Y := XMVectorSplatY(V);
    X := XMVectorSplatX(V);

    Result := XMVectorMultiply(Z, M.r[2]);
    Result := XMVectorMultiplyAdd(Y, M.r[1], Result);
    Result := XMVectorMultiplyAdd(X, M.r[0], Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3TransformNormal(V: TXMVECTOR; M: TXMMATRIX): TXMVECTOR;
begin
    (* ToDo
    float32x2_t VL = vget_low_f32( V );
    XMVECTOR vResult = vmulq_lane_f32( M.r[0], VL, 0 ); // X
    vResult = vmlaq_lane_f32( vResult, M.r[1], VL, 1 ); // Y
    return vmlaq_lane_f32( vResult, M.r[2], vget_high_f32( V ), 0 ); // Z
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3TransformNormal(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR; assembler;
asm

           //  vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0,[V]
           MOVUPS  XMM1,XMM0
           MOVUPS  XMM2,XMM0

           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vResult = _mm_mul_ps(vResult,M.r[0]);
           MULPS   XMM0, TXMMATRIX([M]).r0
           //  vTemp = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vTemp = _mm_mul_ps(vTemp,M.r[1]);
           MULPS   XMM1, TXMMATRIX([M]).r1
           // vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM0, XMM1
           // vTemp = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // vTemp = _mm_mul_ps(vTemp,M.r[2]);
           MULPS   XMM2, TXMMATRIX([M]).r2
           // vResult = _mm_add_ps(vResult,vTemp);
           ADDPS   XMM0, XMM2
           // return vResult;
           MOVUPS  [result] ,XMM0
end;
{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3TransformNormalStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT3;
var
    pInputVector: array of TXMFLOAT3 absolute pInputStream;
    pOutputVector: array of TXMFLOAT3;
    row0, row1, row2, row3: TXMVECTOR;
    i: size_t;
    V, Z, X, Y: TXMVECTOR;
    vResult: TXMVECTOR;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);

    assert(InputStride >= sizeof(TXMFLOAT3));

    assert(OutputStride >= sizeof(TXMFLOAT3));

    SetLength(pOutputVector, VectorCount);


    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];

    for i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat3(pInputVector[i]);
        Z := XMVectorSplatZ(V);
        Y := XMVectorSplatY(V);
        X := XMVectorSplatX(V);

        vResult := XMVectorMultiply(Z, row2);
        vResult := XMVectorMultiplyAdd(Y, row1, vResult);
        vResult := XMVectorMultiplyAdd(X, row0, vResult);

        XMStoreFloat3(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := pOutputStream;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3TransformNormalStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT3;
begin
    (* ToDo
    const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT3)) && (OutputStride == sizeof(XMFLOAT3)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x3_t V = vld3q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT3)*4;

                float32x2_t r = vget_low_f32( row0 );
                XMVECTOR vResult0 = vmulq_lane_f32( V.val[0], r, 0 ); // Ax
                XMVECTOR vResult1 = vmulq_lane_f32( V.val[0], r, 1 ); // Bx

                __prefetch( pInputVector );

                r = vget_high_f32( row0 );
                XMVECTOR vResult2 = vmulq_lane_f32( V.val[0], r, 0 ); // Cx

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( row1 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( row1 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[1], r, 0 ); // Cx+Gy

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                r = vget_low_f32( row2 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[2], r, 0 ); // Ax+Ey+Iz
                vResult1 = vmlaq_lane_f32( vResult1, V.val[2], r, 1 ); // Bx+Fy+Jz

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*4) );

                r = vget_high_f32( row2 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[2], r, 0 ); // Cx+Gy+Kz

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*5) );

                V.val[0] = vResult0;
                V.val[1] = vResult1;
                V.val[2] = vResult2;

                vst3q_f32( reinterpret_cast<float*>(pOutputVector), V );
                pOutputVector += sizeof(XMFLOAT3)*4;

                i += 4;
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        float32x2_t VL = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
        float32x2_t zero = vdup_n_f32(0);
        float32x2_t VH = vld1_lane_f32( reinterpret_cast<const float*>(pInputVector)+2, zero, 0 );
        pInputVector += InputStride;

        XMVECTOR vResult = vmulq_lane_f32( row0, VL, 0 ); // X
        vResult = vmlaq_lane_f32( vResult, row1, VL, 1 ); // Y
        vResult = vmlaq_lane_f32( vResult, row2, VH, 0 ); // Z

        VL = vget_low_f32( vResult );
        vst1_f32( reinterpret_cast<float*>(pOutputVector), VL );
        vst1q_lane_f32( reinterpret_cast<float*>(pOutputVector)+2, vResult, 2 );
        pOutputVector += OutputStride;
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3TransformNormalStream(out pOutputStream: PXMFLOAT3; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT3;
var
    pInputVector, pOutputVector: pointer;
    row0, row1, row2: TXMVECTOR;
    i, j, four: size_t;
    s: uint32;
    V, V1, V2, V3, V4: TXMVECTOR;
begin
    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];

    i := 0;
    four := VectorCount shr 2;
    if (four > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT3)) then
        begin
            if (OutputStride = sizeof(TXMFLOAT3)) then
            begin
                if (not (uintptr(pOutputStream) and $F = $F)) then
                begin
                    // Packed input, aligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for  j := 0 to four - 1 do
                    begin
                        asm
                                   // V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]
                                   MOVUPS  [V1],XMM1
                                   //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   // pInputVector += sizeof(TXMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   //XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4, XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3],XMM4
                                   // V2 := _mm_shuffle_ps(l2,V1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [V2],XMM2
                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [V4],XMM3

                                   // Result 1
                                   //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V1]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   //  vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   //  vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V1 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V1],XMM2

                                   // Result 2
                                   // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V2]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, [row2] );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V2 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V2],XMM2

                                   // Result 3
                                   // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V3]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, [row2] );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V3 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V3],XMM2

                                   // Result 4
                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V4]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V4 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V4],XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);
                                   MOVUPS  XMM0, [v2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // vTemp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));\
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));\


                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector], XMM1
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector], XMM0
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector], XMM3
                                   // pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 4);
                    end;
                end
                else
                begin
                    // Packed input, unaligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for  j := 0 to four - 1 do
                    begin
                        asm
                                   //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]
                                   MOVUPS  [V1],XMM1
                                   //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   // pInputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   // XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3],XMM4
                                   // V2 := _mm_shuffle_ps(l2,V1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [V2],XMM2
                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [V4],XMM3

                                   // Result 1
                                   //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0, [V1]
                                   MOVUPS  XMM1, XMM0
                                   MOVUPS  XMM2, XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   //  vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   //  vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V1 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V1],XMM2

                                   // Result 2
                                   // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0, [V2]
                                   MOVUPS  XMM1, XMM0
                                   MOVUPS  XMM2, XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V2 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V2],XMM2

                                   // Result 3
                                   // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0, [V3]
                                   MOVUPS  XMM1, XMM0
                                   MOVUPS  XMM2, XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V3 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V3],XMM2

                                   // Result 4
                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0, [V4]
                                   MOVUPS  XMM1, XMM0
                                   MOVUPS  XMM2, XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, row2 );
                                   MULPS   XMM2, [row2]
                                   // vTemp2 = _mm_mul_ps( Y, row1 );
                                   MULPS   XMM1, [row1]
                                   // vTemp3 = _mm_mul_ps( X, row0 );
                                   MULPS   XMM0, [row0]
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // V4 = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0
                                   MOVUPS  [V4],XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);
                                   MOVUPS  XMM0, [v2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // vTemp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));\
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));\

                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector], XMM1
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector+16], XMM0
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector+32], XMM3
                                   //  pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 4);
                    end;
                end;
            end
            else
            begin
                // Packed input, unpacked output
                s := sizeof(TXMFLOAT3) * 4;
                for j := 0 to four - 1 do
                begin
                    asm
                               //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM1, [pInputVector]
                               MOVUPS  [V1],XMM1
                               //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                               MOVUPS  XMM2, [pInputVector+16]
                               //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                               MOVUPS  XMM3, [pInputVector+32]
                               // pInputVector += sizeof(XMFLOAT3)*4;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Unpack the 4 vectors (.w components are junk)
                               // XM3UNPACK3INTO4(V1,L2,L3);
                               // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                               MOVUPS  XMM4,XMM2
                               SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                               MOVUPS  [V3],XMM4
                               // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));
                               SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                               // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                               MOVUPS  [V2],XMM2
                               // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                               PSRLDQ  XMM3, 4
                               MOVUPS  [V4],XMM3

                               // Result 1
                               //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0, [V1]
                               MOVUPS  XMM1, XMM0
                               MOVUPS  XMM2, XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               //  vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               //  vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0, [V2]
                               MOVUPS  XMM1, XMM0
                               MOVUPS  XMM2, XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 3
                               // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0, [V3]
                               MOVUPS  XMM1, XMM0
                               MOVUPS  XMM2, XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 4
                               // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0, [V4]
                               MOVUPS  XMM1, XMM0
                               MOVUPS  XMM2, XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, row2 );
                               MULPS   XMM2, [row2]
                               // vTemp2 = _mm_mul_ps( Y, row1 );
                               MULPS   XMM1, [row1]
                               // vTemp3 = _mm_mul_ps( X, row0 );
                               MULPS   XMM0, [row0]
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 4);
                end;
            end;
        end;
    end;

    // for (; i < VectorCount; i++)
    while i < VectorCount do
    begin
        asm
                   //  V = XMLoadFloat3(reinterpret_cast<const XMFLOAT3*>(pInputVector));
                   // pInputVector += InputStride;
                   MOV     EDX, pInputVector
                   ADD     EDX, [InputStride]
                   MOV     pInputVector,EDX

                   //  Z = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                   MOVUPS  XMM0, [V]
                   MOVUPS  XMM1, XMM0
                   MOVUPS  XMM2, XMM0

                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                   //  vTemp = _mm_mul_ps( Z, row2 );
                   MULPS   XMM2, [row2]
                   //  vTemp2 = _mm_mul_ps( Y, row1 );
                   MULPS   XMM1, [row1]
                   //  vTemp3 = _mm_mul_ps( X, row0 );
                   MULPS   XMM0, [row0]
                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                   ADDPS   XMM2, XMM1
                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                   ADDPS   XMM2, XMM0

                   // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                   MOVUPS  [pOutputVector], XMM2
                   // pOutputVector += OutputStride;
                   MOV     EDX, pOutputVector
                   ADD     EDX, [OutputStride]
                   MOV     pOutputVector,EDX
        end;
        Inc(i);
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}


function XMVector3Project(V: TXMVECTOR; ViewportX: single; ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX;
    View: TXMMATRIX; World: TXMMATRIX): TXMVECTOR;
var
    HalfViewportWidth, HalfViewportHeight: single;
    Scale, Offset: TXMVECTOR;
    Transform: TXMMATRIX;
begin
    HalfViewportWidth := ViewportWidth * 0.5;
    HalfViewportHeight := ViewportHeight * 0.5;

    Scale := XMVectorSet(HalfViewportWidth, -HalfViewportHeight, ViewportMaxZ - ViewportMinZ, 0.0);
    Offset := XMVectorSet(ViewportX + HalfViewportWidth, ViewportY + HalfViewportHeight, ViewportMinZ, 0.0);

    Transform := XMMatrixMultiply(World, View);
    Transform := XMMatrixMultiply(Transform, Projection);

    Result := XMVector3TransformCoord(V, Transform);

    Result := XMVectorMultiplyAdd(Result, Scale, Offset);
end;

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3ProjectStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; ViewportX: single;
    ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX; View: TXMMATRIX; World: TXMMATRIX): PXMFLOAT3;
var
    HalfViewportWidth: single;
    HalfViewportHeight: single;

    Scale, Offset, V, vResult: TXMVECTOR;
    Transform: TXMMATRIX;
    i: size_t;

    pInputVector: array of TXMFLOAT3 absolute pInputStream;
    pOutputVector: array of TXMFLOAT3;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);
    assert(InputStride >= sizeof(TXMFLOAT3));
    assert(OutputStride >= sizeof(TXMFLOAT3));

    HalfViewportWidth := ViewportWidth * 0.5;
    HalfViewportHeight := ViewportHeight * 0.5;

    Scale := XMVectorSet(HalfViewportWidth, -HalfViewportHeight, ViewportMaxZ - ViewportMinZ, 1.0);
    Offset := XMVectorSet(ViewportX + HalfViewportWidth, ViewportY + HalfViewportHeight, ViewportMinZ, 0.0);

    Transform := XMMatrixMultiply(World, View);
    Transform := XMMatrixMultiply(Transform, Projection);

    SetLength(pOutputVector, VectorCount);

    for i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat3(pInputVector[i]);
        vResult := XMVector3TransformCoord(V, Transform);
        vResult := XMVectorMultiplyAdd(vResult, Scale, Offset);
        XMStoreFloat3(pOutputVector[i], vResult);
    end;

    pOutputStream := @pOutputVector[0];
    Result := @pOutputVector[0];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3ProjectStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; const pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; ViewportX: single;
    ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX; View: TXMMATRIX; World: TXMMATRIX): PXMFLOAT3;
begin
    (* ToDo
     const float HalfViewportWidth = ViewportWidth * 0.5f;
    const float HalfViewportHeight = ViewportHeight * 0.5f;

    XMMATRIX Transform = XMMatrixMultiply(World, View);
    Transform = XMMatrixMultiply(Transform, Projection);
    const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT3)) && (OutputStride == sizeof(XMFLOAT3)))
        {
            XMVECTOR ScaleX = vdupq_n_f32(HalfViewportWidth);
            XMVECTOR ScaleY = vdupq_n_f32(-HalfViewportHeight);
            XMVECTOR ScaleZ = vdupq_n_f32(ViewportMaxZ - ViewportMinZ);

            XMVECTOR OffsetX = vdupq_n_f32(ViewportX + HalfViewportWidth);
            XMVECTOR OffsetY = vdupq_n_f32(ViewportY + HalfViewportHeight);
            XMVECTOR OffsetZ = vdupq_n_f32(ViewportMinZ);

            for (size_t j = 0; j < four; ++j)
            {
                float32x4x3_t V = vld3q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT3)*4;

                float32x2_t r3 = vget_low_f32( Transform.r[3] );
                float32x2_t r = vget_low_f32( Transform.r[0] );
                XMVECTOR vResult0 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Ax+M
                XMVECTOR vResult1 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Bx+N

                __prefetch( pInputVector );

                r3 = vget_high_f32( Transform.r[3] );
                r = vget_high_f32( Transform.r[0] );
                XMVECTOR vResult2 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), V.val[0], r, 0 ); // Cx+O
                XMVECTOR W = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), V.val[0], r, 1 ); // Dx+P

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( Transform.r[1] );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( Transform.r[1] );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[1], r, 0 ); // Cx+Gy+O
                W = vmlaq_lane_f32( W, V.val[1], r, 1 ); // Dx+Hy+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                r = vget_low_f32( Transform.r[2] );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[2], r, 0 ); // Ax+Ey+Iz+M
                vResult1 = vmlaq_lane_f32( vResult1, V.val[2], r, 1 ); // Bx+Fy+Jz+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*4) );

                r = vget_high_f32( Transform.r[2] );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[2], r, 0 ); // Cx+Gy+Kz+O
                W = vmlaq_lane_f32( W, V.val[2], r, 1 ); // Dx+Hy+Lz+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*5) );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
                vResult0 = vdivq_f32( vResult0, W );
                vResult1 = vdivq_f32( vResult1, W );
                vResult2 = vdivq_f32( vResult2, W );
#else
                // 2 iterations of Newton-Raphson refinement of reciprocal
                float32x4_t Reciprocal = vrecpeq_f32(W);
                float32x4_t S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );
                S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );

                vResult0 = vmulq_f32( vResult0, Reciprocal );
                vResult1 = vmulq_f32( vResult1, Reciprocal );
                vResult2 = vmulq_f32( vResult2, Reciprocal );
#endif

                V.val[0] = vmlaq_f32( OffsetX, vResult0, ScaleX );
                V.val[1] = vmlaq_f32( OffsetY, vResult1, ScaleY );
                V.val[2] = vmlaq_f32( OffsetZ, vResult2, ScaleZ );

                vst3q_f32( reinterpret_cast<float*>(pOutputVector),V );
                pOutputVector += sizeof(XMFLOAT3)*4;

                i += 4;
            }
        }
    }

    if ( i < VectorCount)
    {
        XMVECTOR Scale = XMVectorSet(HalfViewportWidth, -HalfViewportHeight, ViewportMaxZ - ViewportMinZ, 1.0f);
        XMVECTOR Offset = XMVectorSet(ViewportX + HalfViewportWidth, ViewportY + HalfViewportHeight, ViewportMinZ, 0.0f);

        for (; i < VectorCount; i++)
        {
            float32x2_t VL = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
            float32x2_t zero = vdup_n_f32(0);
            float32x2_t VH = vld1_lane_f32( reinterpret_cast<const float*>(pInputVector)+2, zero, 0 );
            pInputVector += InputStride;

            XMVECTOR vResult = vmlaq_lane_f32( Transform.r[3], Transform.r[0], VL, 0 ); // X
            vResult = vmlaq_lane_f32( vResult, Transform.r[1], VL, 1 ); // Y
            vResult = vmlaq_lane_f32( vResult, Transform.r[2], VH, 0 ); // Z

            VH = vget_high_f32(vResult);
            XMVECTOR W = vdupq_lane_f32( VH, 1 );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
            vResult = vdivq_f32( vResult, W );
#else
            // 2 iterations of Newton-Raphson refinement of reciprocal for W
            float32x4_t Reciprocal = vrecpeq_f32( W );
            float32x4_t S = vrecpsq_f32( Reciprocal, W );
            Reciprocal = vmulq_f32( S, Reciprocal );
            S = vrecpsq_f32( Reciprocal, W );
            Reciprocal = vmulq_f32( S, Reciprocal );

            vResult = vmulq_f32( vResult, Reciprocal );
#endif

            vResult = vmlaq_f32( Offset, vResult, Scale );

            VL = vget_low_f32( vResult );
            vst1_f32( reinterpret_cast<float*>(pOutputVector), VL );
            vst1q_lane_f32( reinterpret_cast<float*>(pOutputVector)+2, vResult, 2 );
            pOutputVector += OutputStride;
        }
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3ProjectStream(out pOutputStream: PXMFLOAT3; constref OutputStride: size_t; constref pInputStream: PXMFLOAT3; constref InputStride: size_t; constref VectorCount: size_t;
    constref ViewportX: single; constref ViewportY: single; constref ViewportWidth: single; constref ViewportHeight: single; constref ViewportMinZ: single; constref ViewportMaxZ: single;
    constref Projection: TXMMATRIX; constref View: TXMMATRIX; constref World: TXMMATRIX): PXMFLOAT3;
var
    HalfViewportWidth: single;
    HalfViewportHeight: single;
    Scale, _Offset: TXMVECTOR;
    Transform: TXMMATRIX;
    pInputVector, pOutputVector: pointer;
    i, j, four: size_t;
    s: uint32;
    V, V1, V2, V3, V4: TXMVECTOR;
begin
    HalfViewportWidth := ViewportWidth * 0.5;
    HalfViewportHeight := ViewportHeight * 0.5;

    Scale := XMVectorSet(HalfViewportWidth, -HalfViewportHeight, ViewportMaxZ - ViewportMinZ, 1.0);
    _Offset := XMVectorSet(ViewportX + HalfViewportWidth, ViewportY + HalfViewportHeight, ViewportMinZ, 0.0);

    Transform := XMMatrixMultiply(World, View);
    Transform := XMMatrixMultiply(Transform, Projection);

    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    i := 0;
    four := VectorCount shr 2;
    if (four > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT3)) then
        begin
            if (OutputStride = sizeof(TXMFLOAT3)) then
            begin
                if (not (uintptr(pOutputStream) and $F = $F)) then
                begin
                    // Packed input, aligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for j := 0 to four - 1 do
                    begin
                        asm
                                   // V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]
                                   MOVUPS  [V1], XMM1
                                   // L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   // L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   //pInputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   // XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3], XMM4
                                   // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [V2], XMM2
                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [V4], XMM3

                                   // Result 1
                                   // Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V1]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   //vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   //vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   //vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   //vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   //vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]
                                   //V1 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V1], XMM2

                                   // Result 2
                                   //Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V2]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   //vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   //vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   //vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   //vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   //vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   //W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   //vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   //vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]
                                   //V2 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V2], XMM2

                                   // Result 3
                                   //Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V3]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]
                                   // V3 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V3], XMM2

                                   // Result 4
                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V4]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]
                                   // V4 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V4], XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);
                                   MOVUPS  XMM0, [v2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // vTemp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));\
                                   SHUFPS  XMM4, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));\

                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector], XMM1
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector+16], XMM0
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector+32], XMM3
                                   // pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 4);
                    end;
                end
                else
                begin
                    // Packed input, unaligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for j := 0 to four - 1 do
                    begin
                        asm
                                   //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]
                                   MOVUPS  [V1],XMM1
                                   //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   // pInputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   // XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3],XMM4

                                   // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0

                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [V2],XMM2

                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [V4],XMM3

                                   // Result 1
                                   //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V1]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   //  vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   //  vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]

                                   // V1 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V1],XMM2

                                   // Result 2
                                   // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V2]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]
                                   // V2 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V2],XMM2

                                   // Result 3
                                   // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V3]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]
                                   // V3 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V3],XMM2

                                   // Result 4
                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,[V4]
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // vTemp = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3

                                   // vTemp = _mm_mul_ps( vTemp, Scale );
                                   MULPS   XMM2, [Scale]
                                   // V4 = _mm_add_ps( vTemp, Offset );
                                   ADDPS   XMM2, [_Offset]
                                   MOVUPS  [V4],XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);
                                   MOVUPS  XMM0, [v2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // vTemp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));\
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));\
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector], XMM1
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector+16], XMM0
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector+32], XMM3
                                   // pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 4);
                    end;
                end;
            end
            else
            begin
                // Packed input, unpacked output
                s := sizeof(TXMFLOAT3) * 4;
                for j := 0 to four - 1 do
                begin
                    asm
                               //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM1, [pInputVector]
                               MOVUPS  [V1],XMM1
                               //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                               MOVUPS  XMM2, [pInputVector+16]
                               //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                               MOVUPS  XMM3, [pInputVector+32]
                               // pInputVector += sizeof(XMFLOAT3)*4;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Unpack the 4 vectors (.w components are junk)
                               // XM3UNPACK3INTO4(V1,L2,L3);
                               // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                               MOVUPS  XMM4,XMM2
                               SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                               MOVUPS  [V3],XMM4
                               // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));
                               SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                               // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                               MOVUPS  [V2],XMM2
                               // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                               PSRLDQ  XMM3, 4
                               MOVUPS  [V4],XMM3

                               // Result 1
                               //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V1]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               //  vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               //  vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // vTemp = _mm_mul_ps( vTemp, Scale );
                               MULPS   XMM2, [Scale]
                               // vTemp = _mm_add_ps( vTemp, Offset );
                               ADDPS   XMM2, [_Offset]

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector],XMM2
                               //  pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V2]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // vTemp = _mm_mul_ps( vTemp, Scale );
                               MULPS   XMM2, [Scale]
                               // vTemp = _mm_add_ps( vTemp, Offset );
                               ADDPS   XMM2, [_Offset]

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 3
                               // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V3]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
                               // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // vTemp = _mm_mul_ps( vTemp, Scale );
                               MULPS   XMM2, [Scale]
                               // vTemp = _mm_add_ps( vTemp, Offset );
                               ADDPS   XMM2, [_Offset]

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 4
                               //  Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,[V4]
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3,XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // vTemp = _mm_mul_ps( vTemp, Scale );
                               MULPS   XMM2, [Scale]
                               // vTemp = _mm_add_ps( vTemp, Offset );
                               ADDPS   XMM2, [_Offset]

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector],XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 4);
                end;
            end;
        end;
    end;

    // for (; i < VectorCount; i++)
    while i < VectorCount do
    begin
        V := XMLoadFloat3(pInputVector);
        asm
                   // pInputVector += InputStride;
                   MOV     EDX, pInputVector
                   ADD     EDX, [InputStride]
                   MOV     pInputVector,EDX

                   //  Z = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                   MOVUPS  XMM0,[V]
                   MOVUPS  XMM1,XMM0
                   MOVUPS  XMM2,XMM0
                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                   //  vTemp = _mm_mul_ps( Z, Transform.r[2] );
                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                   //  vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                   //  vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                   ADDPS   XMM2, XMM1
                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                   ADDPS   XMM2, XMM0

                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                   MOVUPS  XMM3,XMM2
                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                   // vTemp = _mm_div_ps( vTemp, W );
                   DIVPS   XMM2, XMM3

                   // vTemp = _mm_mul_ps( vTemp, Scale );
                   MULPS   XMM2, [Scale]
                   // vTemp = _mm_add_ps( vTemp, Offset );
                   ADDPS   XMM2, [_Offset]

                   // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                   MOVUPS  [pOutputVector],XMM2
                   // pOutputVector += OutputStride;
                   MOV     EDX, pOutputVector
                   ADD     EDX, [OutputStride]
                   MOV     pOutputVector,EDX
        end;
        Inc(i);
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}



function XMVector3Unproject(V: TXMVECTOR; ViewportX: single; ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX;
    View: TXMMATRIX; World: TXMMATRIX): TXMVECTOR;
const
    D: TXMVECTORF32 = (f: (-1.0, 1.0, 0.0, 0.0));
var
    Scale, Offset, Det: TXMVECTOR;
    Transform: TXMMATRIX;
begin
    Scale := XMVectorSet(ViewportWidth * 0.5, -ViewportHeight * 0.5, ViewportMaxZ - ViewportMinZ, 1.0);
    Scale := XMVectorReciprocal(Scale);

    Offset := XMVectorSet(-ViewportX, -ViewportY, -ViewportMinZ, 0.0);
    Offset := XMVectorMultiplyAdd(Scale, Offset, D.v);

    Transform := XMMatrixMultiply(World, View);
    Transform := XMMatrixMultiply(Transform, Projection);
    Transform := XMMatrixInverse(Det, Transform);

    Result := XMVectorMultiplyAdd(V, Scale, Offset);

    Result := XMVector3TransformCoord(Result, Transform);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector3UnprojectStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; constref pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; ViewportX: single;
    ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX; View: TXMMATRIX; World: TXMMATRIX): PXMFLOAT3;
const
    D: TXMVECTORF32 = (f: (-1.0, 1.0, 0.0, 0.0));
var
    i: size_t;
    Scale, Offset: TXMVECTOR;
    Transform: TXMMATRIX;
    V, vResult: TXMVECTOR;
    det: TXMVECTOR;
    pInputVector: array of TXMFLOAT3 absolute pInputStream;
    pOutputVector: array of TXMFLOAT3;

begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);
    assert(InputStride >= sizeof(TXMFLOAT3));
    assert(OutputStride >= sizeof(TXMFLOAT3));

    Scale := XMVectorSet(ViewportWidth * 0.5, -ViewportHeight * 0.5, ViewportMaxZ - ViewportMinZ, 1.0);
    Scale := XMVectorReciprocal(Scale);

    Offset := XMVectorSet(-ViewportX, -ViewportY, -ViewportMinZ, 0.0);
    Offset := XMVectorMultiplyAdd(Scale, Offset, D.v);

    Transform := XMMatrixMultiply(World, View);
    Transform := XMMatrixMultiply(Transform, Projection);
    Transform := XMMatrixInverse(det, Transform);


    SetLength(pOutputVector, VectorCount);

    for i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat3(pInputVector[i]);
        vResult := XMVectorMultiplyAdd(V, Scale, Offset);
        vResult := XMVector3TransformCoord(vResult, Transform);
        XMStoreFloat3(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := @pOutputVector;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector3UnprojectStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; constref pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; ViewportX: single;
    ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX; View: TXMMATRIX; World: TXMMATRIX): PXMFLOAT3;

begin
    (* ToDo
    XMMATRIX Transform = XMMatrixMultiply(World, View);
    Transform = XMMatrixMultiply(Transform, Projection);
    Transform = XMMatrixInverse(nullptr, Transform);

    const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    float sx = 1.f / (ViewportWidth * 0.5f);
    float sy = 1.f / (-ViewportHeight * 0.5f);
    float sz = 1.f / (ViewportMaxZ - ViewportMinZ);

    float ox = (-ViewportX * sx) - 1.f;
    float oy = (-ViewportY * sy) + 1.f;
    float oz = (-ViewportMinZ * sz);

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT3)) && (OutputStride == sizeof(XMFLOAT3)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x3_t V = vld3q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT3)*4;

                XMVECTOR ScaleX = vdupq_n_f32(sx);
                XMVECTOR OffsetX = vdupq_n_f32(ox);
                XMVECTOR VX = vmlaq_f32( OffsetX, ScaleX, V.val[0] );

                float32x2_t r3 = vget_low_f32( Transform.r[3] );
                float32x2_t r = vget_low_f32( Transform.r[0] );
                XMVECTOR vResult0 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), VX, r, 0 ); // Ax+M
                XMVECTOR vResult1 = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), VX, r, 1 ); // Bx+N

                __prefetch( pInputVector );

                r3 = vget_high_f32( Transform.r[3] );
                r = vget_high_f32( Transform.r[0] );
                XMVECTOR vResult2 = vmlaq_lane_f32( vdupq_lane_f32( r3, 0 ), VX, r, 0 ); // Cx+O
                XMVECTOR W = vmlaq_lane_f32( vdupq_lane_f32( r3, 1 ), VX, r, 1 ); // Dx+P

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                XMVECTOR ScaleY = vdupq_n_f32(sy);
                XMVECTOR OffsetY = vdupq_n_f32(oy);
                XMVECTOR VY = vmlaq_f32( OffsetY, ScaleY, V.val[1] );

                r = vget_low_f32( Transform.r[1] );
                vResult0 = vmlaq_lane_f32( vResult0, VY, r, 0 ); // Ax+Ey+M
                vResult1 = vmlaq_lane_f32( vResult1, VY, r, 1 ); // Bx+Fy+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( Transform.r[1] );
                vResult2 = vmlaq_lane_f32( vResult2, VY, r, 0 ); // Cx+Gy+O
                W = vmlaq_lane_f32( W, VY, r, 1 ); // Dx+Hy+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                XMVECTOR ScaleZ = vdupq_n_f32(sz);
                XMVECTOR OffsetZ = vdupq_n_f32(oz);
                XMVECTOR VZ = vmlaq_f32( OffsetZ, ScaleZ, V.val[2] );

                r = vget_low_f32( Transform.r[2] );
                vResult0 = vmlaq_lane_f32( vResult0, VZ, r, 0 ); // Ax+Ey+Iz+M
                vResult1 = vmlaq_lane_f32( vResult1, VZ, r, 1 ); // Bx+Fy+Jz+N

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*4) );

                r = vget_high_f32( Transform.r[2] );
                vResult2 = vmlaq_lane_f32( vResult2, VZ, r, 0 ); // Cx+Gy+Kz+O
                W = vmlaq_lane_f32( W, VZ, r, 1 ); // Dx+Hy+Lz+P

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*5) );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
                V.val[0] = vdivq_f32( vResult0, W );
                V.val[1] = vdivq_f32( vResult1, W );
                V.val[2] = vdivq_f32( vResult2, W );
#else
                // 2 iterations of Newton-Raphson refinement of reciprocal
                float32x4_t Reciprocal = vrecpeq_f32(W);
                float32x4_t S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );
                S = vrecpsq_f32( Reciprocal, W );
                Reciprocal = vmulq_f32( S, Reciprocal );

                V.val[0] = vmulq_f32( vResult0, Reciprocal );
                V.val[1] = vmulq_f32( vResult1, Reciprocal );
                V.val[2] = vmulq_f32( vResult2, Reciprocal );
#endif

                vst3q_f32( reinterpret_cast<float*>(pOutputVector),V );
                pOutputVector += sizeof(XMFLOAT3)*4;

                i += 4;
            }
        }
    }

    if (i < VectorCount)
    {
        float32x2_t ScaleL = vcreate_f32(((uint64_t)*(const uint32_t* )&sx) | ((uint64_t)(*(const uint32_t* )&sy) << 32));
        float32x2_t ScaleH = vcreate_f32((uint64_t)*(const uint32_t* )&sz);

        float32x2_t OffsetL = vcreate_f32(((uint64_t)*(const uint32_t* )&ox) | ((uint64_t)(*(const uint32_t* )&oy) << 32));
        float32x2_t OffsetH = vcreate_f32((uint64_t)*(const uint32_t* )&oz);

        for (; i < VectorCount; i++)
        {
            float32x2_t VL = vld1_f32( reinterpret_cast<const float*>(pInputVector) );
            float32x2_t zero = vdup_n_f32(0);
            float32x2_t VH = vld1_lane_f32( reinterpret_cast<const float*>(pInputVector)+2, zero, 0 );
            pInputVector += InputStride;

            VL = vmla_f32( OffsetL, VL, ScaleL );
            VH = vmla_f32( OffsetH, VH, ScaleH );

            XMVECTOR vResult = vmlaq_lane_f32( Transform.r[3], Transform.r[0], VL, 0 ); // X
            vResult = vmlaq_lane_f32( vResult, Transform.r[1], VL, 1 ); // Y
            vResult = vmlaq_lane_f32( vResult, Transform.r[2], VH, 0 ); // Z

            VH = vget_high_f32(vResult);
            XMVECTOR W = vdupq_lane_f32( VH, 1 );

#if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
            vResult = vdivq_f32( vResult, W );
#else
            // 2 iterations of Newton-Raphson refinement of reciprocal for W
            float32x4_t Reciprocal = vrecpeq_f32( W );
            float32x4_t S = vrecpsq_f32( Reciprocal, W );
            Reciprocal = vmulq_f32( S, Reciprocal );
            S = vrecpsq_f32( Reciprocal, W );
            Reciprocal = vmulq_f32( S, Reciprocal );

            vResult = vmulq_f32( vResult, Reciprocal );
#endif

            VL = vget_low_f32( vResult );
            vst1_f32( reinterpret_cast<float*>(pOutputVector), VL );
            vst1q_lane_f32( reinterpret_cast<float*>(pOutputVector)+2, vResult, 2 );
            pOutputVector += OutputStride;
        }
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector3UnprojectStream(out pOutputStream: PXMFLOAT3; OutputStride: size_t; constref pInputStream: PXMFLOAT3; InputStride: size_t; VectorCount: size_t; ViewportX: single;
    ViewportY: single; ViewportWidth: single; ViewportHeight: single; ViewportMinZ: single; ViewportMaxZ: single; Projection: TXMMATRIX; View: TXMMATRIX; World: TXMMATRIX): PXMFLOAT3; inline;
const
    D: TXMVECTORF32 = (f: (-1.0, 1.0, 0.0, 0.0));
var
    Scale, _Offset: TXMVECTOR;
    Transform: TXMMATRIX;
    pInputVector, pOutputVector: pointer;
    s: uint32;
    i, j, four: size_t;
    V, V1, V2, V3, V4: TXMVECTOR;
begin
    Scale := XMVectorSet(ViewportWidth * 0.5, -ViewportHeight * 0.5, ViewportMaxZ - ViewportMinZ, 1.0);
    Scale := XMVectorReciprocal(Scale);

    _Offset := XMVectorSet(-ViewportX, -ViewportY, -ViewportMinZ, 0.0);
    asm
               // Offset = _mm_mul_ps(Scale, Offset);
               MOVUPS  XMM0,[Scale]
               MULPS   XMM0, [_Offset]
               // Offset = _mm_add_ps(Offset, D);
               ADDPS   XMM0, [D]
               MOVUPS  [_Offset],XMM0
    end;

    Transform := XMMatrixMultiply(World, View);
    Transform := XMMatrixMultiply(Transform, Projection);
    Transform := XMMatrixInverse(TXMVECTOR(nil^), Transform);

    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    i := 0;
    four := VectorCount shr 2;
    if (four > 0) then
    begin
        if (InputStride = sizeof(TXMFLOAT3)) then
        begin
            if (OutputStride = sizeof(TXMFLOAT3)) then
            begin
                if (not (uintptr(pOutputStream) and $F = $F)) then
                begin
                    // Packed input, aligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for j := 0 to four - 1 do
                    begin
                        asm
                                   // V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]

                                   //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   // pInputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   // XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3],XMM4
                                   // V2 := _mm_shuffle_ps(l2,V1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [V2],XMM2
                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [V4],XMM3

                                   // Result 1
                                   // V1 = _mm_mul_ps( V1, Scale );
                                   MULPS   XMM1, [Scale]
                                   // V1 = _mm_add_ps( V1, Offset );
                                   ADDPS   XMM1, [_Offset]
                                   MOVUPS  [V1],XMM1

                                   // Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,XMM1
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   //  vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   //  vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V1 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V1],XMM2

                                   // Result 2
                                   // V2 = _mm_mul_ps( V2, Scale );
                                   MOVUPS  XMM0,[V2]
                                   MULPS   XMM0, [Scale]
                                   // V2 = _mm_add_ps( V2, Offset );
                                   ADDPS   XMM0, [_Offset]

                                   // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V2 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V2],XMM2

                                   // Result 3
                                   // V3 = _mm_mul_ps( V3, Scale );
                                   MOVUPS  XMM0, [V3]
                                   MULPS   XMM0, [Scale]
                                   // V3 = _mm_add_ps( V3, Offset );
                                   ADDPS   XMM0, [_Offset]

                                   // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V3 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V3],XMM2

                                   // Result 4
                                   // V4 = _mm_mul_ps( V4, Scale );
                                   MOVUPS  XMM0,[V4]
                                   MULPS   XMM0, [Scale]
                                   // V4 = _mm_add_ps( V4, Offset );
                                   ADDPS   XMM0, [_Offset]

                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V4 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V4],XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);
                                   MOVUPS  XMM0, [v2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // vTemp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));\
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));\
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector],XMM1
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector+16],XMM0
                                   // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector+32],XMM3
                                   // pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 4);
                    end;
                end
                else
                begin
                    // Packed input, unaligned & packed output
                    s := sizeof(TXMFLOAT3) * 4;
                    for j := 0 to four - 1 do
                    begin
                        asm
                                   //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                                   MOVUPS  XMM1, [pInputVector]

                                   //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                                   MOVUPS  XMM2, [pInputVector+16]
                                   //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                                   MOVUPS  XMM3, [pInputVector+32]
                                   // pInputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pInputVector
                                   ADD     EDX, [s]
                                   MOV     pInputVector,EDX

                                   // Unpack the 4 vectors (.w components are junk)
                                   // XM3UNPACK3INTO4(V1,L2,L3);
                                   // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                                   MOVUPS  XMM4,XMM2
                                   SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                                   MOVUPS  [V3],XMM4
                                   // V2 := _mm_shuffle_ps(l2,l1,_MM_SHUFFLE(3,3,1,0));
                                   SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                                   // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                                   MOVUPS  [V2],XMM2
                                   // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                                   PSRLDQ  XMM3, 4
                                   MOVUPS  [V4],XMM3
                                   // Result 1
                                   // V1 = _mm_mul_ps( V1, Scale );
                                   MULPS   XMM1, [Scale]
                                   // V1 = _mm_add_ps( V1, Offset );
                                   ADDPS   XMM1, [_Offset]

                                   // Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM0,XMM1
                                   MOVUPS  XMM2,XMM1
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   //  vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   //  vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   //  vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V1 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V1],XMM2

                                   // Result 2
                                   // V2 = _mm_mul_ps( V2, Scale );
                                   MOVUPS  XMM0,[V2]
                                   MULPS   XMM0, [Scale]
                                   // V2 = _mm_add_ps( V2, Offset );
                                   ADDPS   XMM0, [_Offset]
                                   // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V2 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V2],XMM2

                                   // Result 3
                                   // V3 = _mm_mul_ps( V3, Scale );
                                   MOVUPS  XMM0,[V3]
                                   MULPS   XMM0, [Scale]
                                   // V3 = _mm_add_ps( V3, Offset );
                                   ADDPS   XMM0, [_Offset]


                                   // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V3 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V3],XMM2

                                   // Result 4
                                   // V4 = _mm_mul_ps( V4, Scale );
                                   MOVUPS  XMM0,[V4]
                                   MULPS   XMM0, [Scale]
                                   // V4 = _mm_add_ps( V4, Offset );
                                   ADDPS   XMM0, [_Offset]

                                   // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                                   MOVUPS  XMM1,XMM0
                                   MOVUPS  XMM2,XMM0
                                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                                   // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                                   // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                                   // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                                   // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                                   // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                                   ADDPS   XMM2, XMM1
                                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                                   ADDPS   XMM2, XMM0

                                   // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                                   MOVUPS  XMM3,XMM2
                                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                                   // V4 = _mm_div_ps( vTemp, W );
                                   DIVPS   XMM2, XMM3
                                   MOVUPS  [V4],XMM2

                                   // Pack and store the vectors
                                   // XM3PACK4INTO3(vTemp);
                                   MOVUPS  XMM0, [v2]
                                   SHUFPS  XMM0, [v3], _MM_SHUFFLE_1_0_2_1 // vTemp := _mm_shuffle_ps(V2,V3,_MM_SHUFFLE(1,0,2,1));
                                   MOVUPS  XMM2, [v2]
                                   SHUFPS  XMM2, [v1], _MM_SHUFFLE_2_2_0_0 // V2 := _mm_shuffle_ps(V2,V1,_MM_SHUFFLE(2,2,0,0));
                                   MOVUPS  XMM1, [v1]
                                   SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_1_0 // V1 := _mm_shuffle_ps(V1,V2,_MM_SHUFFLE(0,2,1,0));
                                   MOVUPS  XMM3, [v3]
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_0_0_2_2 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(0,0,2,2));\
                                   SHUFPS  XMM3, [v4], _MM_SHUFFLE_2_1_2_0 // V3 := _mm_shuffle_ps(V3,V4,_MM_SHUFFLE(2,1,2,0));\
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                                   MOVUPS  [pOutputVector], XMM1
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                                   MOVUPS  [pOutputVector+16], XMM0
                                   // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                                   MOVUPS  [pOutputVector+32], XMM3
                                   // pOutputVector += sizeof(XMFLOAT3)*4;
                                   MOV     EDX, pOutputVector
                                   ADD     EDX, [s]
                                   MOV     pOutputVector,EDX
                        end;
                        Inc(i, 4);
                    end;
                end;
            end
            else
            begin
                // Packed input, unpacked output
                s := sizeof(TXMFLOAT3) * 4;
                for j := 0 to four - 1 do
                begin
                    asm
                               //  V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                               MOVUPS  XMM1, [pInputVector]
                               //  L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                               MOVUPS  XMM2, [pInputVector+16]
                               //  L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                               MOVUPS  XMM3, [pInputVector+32]
                               // pInputVector += sizeof(XMFLOAT3)*4;
                               MOV     EDX, pInputVector
                               ADD     EDX, [s]
                               MOV     pInputVector,EDX

                               // Unpack the 4 vectors (.w components are junk)
                               // XM3UNPACK3INTO4(V1,L2,L3);
                               // V3 := _mm_shuffle_ps(l2,l3,_MM_SHUFFLE(0,0,3,2));
                               MOVUPS  XMM4,XMM2
                               SHUFPS  XMM4, XMM3, _MM_SHUFFLE_0_0_3_2
                               MOVUPS  [V3],XMM4
                               // V2 := _mm_shuffle_ps(l2,V1,_MM_SHUFFLE(3,3,1,0));
                               SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_3_1_0
                               // V2 := XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,1,0,2));
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_1_0_2
                               MOVUPS  [V2],XMM2
                               // V4 := _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );
                               PSRLDQ  XMM3, 4
                               MOVUPS  [V4],XMM3

                               // Result 1
                               // V1 = _mm_mul_ps( V1, Scale );
                               MULPS   XMM1, [Scale]
                               // V1 = _mm_add_ps( V1, Offset );
                               ADDPS   XMM1, [_Offset]

                               //  Z = XM_PERMUTE_PS( V1, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM0,XMM1
                               MOVUPS  XMM2,XMM1
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               //  Y = XM_PERMUTE_PS( V1, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               //  X = XM_PERMUTE_PS( V1, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               //  vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               //  vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               //  vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3, XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 2
                               // V2 = _mm_mul_ps( V2, Scale );
                               MOVUPS  XMM0,[V2]
                               MULPS   XMM0, [Scale]
                               // V2 = _mm_add_ps( V2, Offset );
                               ADDPS   XMM0, [_Offset]

                               // Z = XM_PERMUTE_PS( V2, _MM_SHUFFLE(2, 2, 2, 2) );

                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V2, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V2, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3, XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 3
                               // V3 = _mm_mul_ps( V3, Scale );
                               MOVUPS  XMM0,[V3]
                               MULPS   XMM0, [Scale]
                               // V3 = _mm_add_ps( V3, Offset );
                               ADDPS   XMM0, [_Offset]

                               // Z = XM_PERMUTE_PS( V3, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V3, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V3, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3, XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX

                               // Result 4
                               // V4 = _mm_mul_ps( V4, Scale );
                               MOVUPS  XMM0,[V4]
                               MULPS   XMM0, [Scale]
                               // V4 = _mm_add_ps( V4, Offset );
                               ADDPS   XMM0, [_Offset]

                               // Z = XM_PERMUTE_PS( V4, _MM_SHUFFLE(2, 2, 2, 2) );
                               MOVUPS  XMM1,XMM0
                               MOVUPS  XMM2,XMM0
                               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                               // Y = XM_PERMUTE_PS( V4, _MM_SHUFFLE(1, 1, 1, 1) );
                               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                               // X = XM_PERMUTE_PS( V4, _MM_SHUFFLE(0, 0, 0, 0) );
                               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0

                               // vTemp = _mm_mul_ps( Z, Transform.r[2] );
                               MULPS   XMM2, TXMMATRIX([Transform]).r2
                               // vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                               MULPS   XMM1, TXMMATRIX([Transform]).r1
                               // vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                               MULPS   XMM0, TXMMATRIX([Transform]).r0
                               // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                               ADDPS   XMM2, TXMMATRIX([Transform]).r3
                               // vTemp = _mm_add_ps( vTemp, vTemp2 );
                               ADDPS   XMM2, XMM1
                               // vTemp = _mm_add_ps( vTemp, vTemp3 );
                               ADDPS   XMM2, XMM0

                               // W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                               MOVUPS  XMM3, XMM2
                               SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                               // vTemp = _mm_div_ps( vTemp, W );
                               DIVPS   XMM2, XMM3

                               // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                               MOVUPS  [pOutputVector], XMM2
                               // pOutputVector += OutputStride;
                               MOV     EDX, pOutputVector
                               ADD     EDX, [OutputStride]
                               MOV     pOutputVector,EDX
                    end;
                    Inc(i, 4);
                end;
            end;
        end;
    end;

    // for (; i < VectorCount; i++)
    while i < VectorCount do
    begin
        V := XMLoadFloat3(pInputVector);
        asm
                   // pInputVector += InputStride;
                   MOV     EDX, pInputVector
                   ADD     EDX, [InputStride]
                   MOV     pInputVector,EDX

                   // V = _mm_mul_ps( V, Scale );
                   MOVUPS  XMM0, [V]
                   MULPS   XMM0, [Scale]
                   // V = _mm_add_ps( V, Offset );
                   ADDPS   XMM0, [_Offset]

                   //  Z = XM_PERMUTE_PS( V, _MM_SHUFFLE(2, 2, 2, 2) );
                   MOVUPS  XMM1,XMM0
                   MOVUPS  XMM2,XMM0
                   SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                   //  Y = XM_PERMUTE_PS( V, _MM_SHUFFLE(1, 1, 1, 1) );
                   SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                   //  X = XM_PERMUTE_PS( V, _MM_SHUFFLE(0, 0, 0, 0) );
                   SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0


                   //  vTemp = _mm_mul_ps( Z, Transform.r[2] );
                   MULPS   XMM2, TXMMATRIX([Transform]).r2
                   //  vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                   MULPS   XMM1, TXMMATRIX([Transform]).r1
                   //  vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                   MULPS   XMM0, TXMMATRIX([Transform]).r0
                   // vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                   ADDPS   XMM2, TXMMATRIX([Transform]).r3
                   // vTemp = _mm_add_ps( vTemp, vTemp2 );
                   ADDPS   XMM2, XMM1
                   // vTemp = _mm_add_ps( vTemp, vTemp3 );
                   ADDPS   XMM2, XMM0

                   //  W = XM_PERMUTE_PS( vTemp, _MM_SHUFFLE(3, 3, 3, 3) );
                   MOVUPS  XMM3,XMM2
                   SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
                   // vTemp = _mm_div_ps( vTemp, W );
                   DIVPS   XMM2, XMM3

                   // XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                   MOVUPS  [pOutputVector], XMM2
                   // pOutputVector += OutputStride;
                   MOV     EDX, pOutputVector
                   ADD     EDX, [OutputStride]
                   MOV     pOutputVector,EDX
        end;
        Inc(i);
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}

{***************************************************************************
 *
 * 4D vector operations
 *
 ***************************************************************************}

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] = V2.f32[0]) and (V1.f32[1] = V2.f32[1]) and (V1.f32[2] = V2.f32[2]) and (V1.f32[3] = V2.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
      uint32x4_t vResult = vceqq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           MOVUPS  XMM0,[v1]
           CMPPS   XMM0, [v2], 0
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4Equal(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllTrue(XMVector4EqualR(V1, V2));
end;

{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;

    if ((V1.f32[0] = V2.f32[0]) and (V1.f32[1] = V2.f32[1]) and (V1.f32[2] = V2.f32[2]) and (V1.f32[3] = V2.f32[3])) then
        Result := XM_CRMASK_CR6TRUE
    else if ((V1.f32[0] <> V2.f32[0]) and (V1.f32[1] <> V2.f32[1]) and (V1.f32[2] <> V2.f32[2]) and (V1.f32[3] <> V2.f32[3])) then
        Result := XM_CRMASK_CR6FALSE;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (*
      uint32x4_t vResult = vceqq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);

    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4EqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           MOVUPS  XMM0,[v1]
           CMPPS   XMM0,[v2], 0
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           MOV     EDX,XM_CRMASK_CR6TRUE
           JE      @Finished
           CMP     EAX, 0
           JE      @Less
           MOV     EDX,0
           JMP     @finished
           @Less:
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.u32[0] = V2.u32[0]) and (V1.u32[1] = V2.u32[1]) and (V1.u32[2] = V2.u32[2]) and (V1.u32[3] = V2.u32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
    uint32x4_t vResult = vceqq_u32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           MOVUPS  XMM0,[v1]
           PCMPEQD  XMM0, [v2]
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4EqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllTrue(XMVector4EqualIntR(V1, V2));
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.u32[0] = V2.u32[0]) and (V1.u32[1] = V2.u32[1]) and (V1.u32[2] = V2.u32[2]) and (V1.u32[3] = V2.u32[3])) then
        Result := XM_CRMASK_CR6TRUE
    else if ((V1.u32[0] <> V2.u32[0]) and (V1.u32[1] <> V2.u32[1]) and (V1.u32[2] <> V2.u32[2]) and (V1.u32[3] <> V2.u32[3])) then
        Result := XM_CRMASK_CR6FALSE;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
     (*
       uint32x4_t vResult = vceqq_u32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);

    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
     *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVector4EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]
           VPCMPEQD  XMM0, XMM1, XMM2
           VMOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           MOV     EDX,XM_CRMASK_CR6TRUE
           JE      @Finished
           CMP     EAX, 0
           JE      @Less
           MOV     EDX,0
           JMP     @finished
           @Less:
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [result],EDX
end;
{$ELSE}
function XMVector4EqualIntR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           // vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           MOVUPS  XMM0, [V1]
           PCMPEQD XMM0, [V2]
    {
    int iTest = _mm_movemask_ps(_mm_castsi128_ps(vTemp));
    movmskps r32, xmm
    uint32_t CR = 0;
    if (iTest==0xf)     // All equal?
        CR = XM_CRMASK_CR6TRUE;
    else if (iTest==0)  // All not equal?
        CR = XM_CRMASK_CR6FALSE;
    return CR;
    }
           MOVMSKPS EAX, XMM0
           MOV     EDX,XM_CRMASK_CR6FALSE
           CMP     EAX, $0
           JE      @Finished

           MOV     EDX,XM_CRMASK_CR6TRUE
           AND     EAX, $F
           CMP     EAX, $F
           JE      @Finished
           MOV     EDX,$0
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
var
    dx, dy, dz, dw: single;
begin
    dx := abs(V1.f32[0] - V2.f32[0]);
    dy := abs(V1.f32[1] - V2.f32[1]);
    dz := abs(V1.f32[2] - V2.f32[2]);
    dw := abs(V1.f32[3] - V2.f32[3]);
    Result := ((dx <= Epsilon.f32[0]) and (dy <= Epsilon.f32[1]) and (dz <= Epsilon.f32[2]) and (dw <= Epsilon.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean;
begin
    (*
     float32x4_t vDelta = vsubq_f32( V1, V2 );
    uint32x4_t vResult = vacleq_f32( vDelta, Epsilon );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVector4NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean; assembler;
asm
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]

           VSUBPS  XMM0, XMM1, XMM2 // difference vDelta in XMM0
           VXORPS  XMM2, XMM2, XMM2 // Zero in XMM2
           VSUBPS  XMM2, XMM2, XMM0  // negativ differenz in XMM2
           VMAXPS  XMM2, XMM2, XMM0   // Get the absolute value of the difference in XMM2
           VMOVUPS XMM1,[Epsilon]
           VCMPPS  XMM0, XMM2, XMM1, 2 // CMPLEPS
           VMOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4NearEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref Epsilon: TXMVECTOR): boolean; assembler;
asm
           // Get the difference
           //  vDelta = _mm_sub_ps(V1,V2);
           MOVUPS  XMM1, [V1]
           SUBPS   XMM1, [V2]
           // Get the absolute value of the difference
           //  vTemp = _mm_setzero_ps();
           XORPS   XMM0, XMM0
           // vTemp = _mm_sub_ps(vTemp,vDelta);
           SUBPS   XMM0, XMM1
           // vTemp = _mm_max_ps(vTemp,vDelta);
           MAXPS   XMM0, XMM1
           // vTemp = _mm_cmple_ps(vTemp,Epsilon);
           CMPPS   XMM0, [Epsilon], 2
           // return ((_mm_movemask_ps(vTemp)==0xf) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ENDIF}


// Returns true if the 4D vectors are not equal and false otherwise.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] <> V2.f32[0]) or (V1.f32[1] <> V2.f32[1]) or (V1.f32[2] <> V2.f32[2]) or (V1.f32[3] <> V2.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
     uint32x4_t vResult = vceqq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) != 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVector4NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]
           VCMPPS  XMM0, XMM1, XMM2, 4 // CMPNEQPS
           VMOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmpneq_ps(V1,V2);
           MOVUPS  XMM0, [V1]
           CMPPS   XMM0, [V2], 4
           // return ((_mm_movemask_ps(vTemp)) != 0);
           MOVMSKPS EAX, XMM0
           CMP     EAX, $0
           SETNE    [result]
end;
{$ELSE}
function XMVector4NotEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAnyFalse(XMVector4EqualR(V1, V2));
end;

{$ENDIF}



// Test whether two 4D vectors are not equal, treating each component as an unsigned integer.
// Returns true if the 4D vectors are not equal and false otherwise.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.u32[0] <> V2.u32[0]) or (V1.u32[1] <> V2.u32[1]) or (V1.u32[2] <> V2.u32[2]) or (V1.u32[3] <> V2.u32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
      uint32x4_t vResult = vceqq_u32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) != 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           //  __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
           //    return ((_mm_movemask_ps(_mm_castsi128_ps(vTemp))!=0xF) != 0);

           MOVUPS  XMM0,[v1]
           MOVUPS  XMM1,[v2]
           PCMPEQD  XMM0, XMM1
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETNE    [result]
end;
{$ELSE}
function XMVector4NotEqualInt(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAnyFalse(XMVector4EqualIntR(V1, V2));
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] > V2.f32[0]) and (V1.f32[1] > V2.f32[1]) and (V1.f32[2] > V2.f32[2]) and (V1.f32[3] > V2.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
     uint32x4_t vResult = vcgtq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}

function XMVector4Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
           MOVUPS  XMM0,[V2]
           CMPPS   XMM0, [V1], 1
           //    return ((_mm_movemask_ps(vTemp)==0x0f) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4Greater(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllTrue(XMVector4GreaterR(V1, V2));
end;

{$ENDIF}

// Tests whether one 4D vector is greater than another 4D vector and returns a comparison value that can
// be examined using functions such as XMComparisonAllTrue.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] > V2.f32[0]) and (V1.f32[1] > V2.f32[1]) and (V1.f32[2] > V2.f32[2]) and (V1.f32[3] > V2.f32[3])) then
        Result := XM_CRMASK_CR6TRUE
    else if ((V1.f32[0] <= V2.f32[0]) and (V1.f32[1] <= V2.f32[1]) and (V1.f32[2] <= V2.f32[2]) and (V1.f32[3] <= V2.f32[3])) then
        Result := XM_CRMASK_CR6FALSE;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (*
     uint32x4_t vResult = vcgtq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);

    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_

function XMVector4GreaterR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
{    uint32_t CR = 0;
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
    cmpps xmm, xmm, 1
    int iTest = _mm_movemask_ps(vTemp);
    movmskps r32, xmm
    if (iTest==0xf) then
        CR = XM_CRMASK_CR6TRUE;

    else if (!iTest) then
        CR = XM_CRMASK_CR6FALSE;
    return CR; }
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]
           VCMPPS  XMM0, XMM1, XMM2, 6 // CMPNLEPS
           VMOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           MOV     EDX,XM_CRMASK_CR6TRUE
           JE      @Finished
           CMP     EAX, 0
           JE      @Less
           MOV     EDX,0
           JMP     @finished
           @Less:
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

// Returns true if V1 is greater-than-or-equal-to V2 and false otherwise
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] >= V2.f32[0]) and (V1.f32[1] >= V2.f32[1]) and (V1.f32[2] >= V2.f32[2]) and (V1.f32[3] >= V2.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
    uint32x4_t vResult = vcgeq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
           MOVUPS  XMM0,[v2]
           CMPPS   XMM0, [v1], 2
           //    return ((_mm_movemask_ps(vTemp)==0x0f) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4GreaterOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllTrue(XMVector4GreaterOrEqualR(V1, V2));
end;

{$ENDIF}

// Tests whether one 4D vector is greater-than-or-equal-to another 4D vector and returns a comparison value that
// can be examined using functions such as XMComparisonAllTrue.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    Result := 0;
    if ((V1.f32[0] >= V2.f32[0]) and (V1.f32[1] >= V2.f32[1]) and (V1.f32[2] >= V2.f32[2]) and (V1.f32[3] >= V2.f32[3])) then
        Result := XM_CRMASK_CR6TRUE
    else if ((V1.f32[0] < V2.f32[0]) and (V1.f32[1] < V2.f32[1]) and (V1.f32[2] < V2.f32[2]) and (V1.f32[3] < V2.f32[3])) then
        Result := XM_CRMASK_CR6FALSE;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32;
begin
    (*
     uint32x4_t vResult = vcgeq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);

    uint32_t CR = 0;
    if ( r == 0xFFFFFFFFU )
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if ( !r )
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
    *)
end;

{$ELSE}// (_XM_SSE_INTRINSICS_)
function XMVector4GreaterOrEqualR(constref V1: TXMVECTOR; constref V2: TXMVECTOR): UINT32; assembler;
asm
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]
           VCMPPS  XMM0, XMM1, XMM2, 5 // CMPNLTPS
           VMOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           MOV     EDX,XM_CRMASK_CR6TRUE
           JE      @Finished
           CMP     EAX, 0
           JE      @Less
           MOV     EDX,0
           JMP     @finished
           @Less:
           MOV     EDX,XM_CRMASK_CR6FALSE
           @Finished:
           MOV     [result],EDX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] < V2.f32[0]) and (V1.f32[1] < V2.f32[1]) and (V1.f32[2] < V2.f32[2]) and (V1.f32[3] < V2.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
     uint32x4_t vResult = vcltq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVector4Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]
           VCMPPS  XMM0, XMM1, XMM2, 1
           VMOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           // vTemp = _mm_cmplt_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 1
           // return ((_mm_movemask_ps(vTemp)==0x0f) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4Less(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllTrue(XMVector4GreaterR(V2, V1));
end;

{$ENDIF}




{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := ((V1.f32[0] <= V2.f32[0]) and (V1.f32[1] <= V2.f32[1]) and (V1.f32[2] <= V2.f32[2]) and (V1.f32[3] <= V2.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    (*
     uint32x4_t vResult = vcleq_f32( V1, V2 );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
    *)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           //  vTemp = _mm_cmple_ps(V1,V2);
           MOVUPS  XMM0,[V1]
           CMPPS   XMM0, [V2], 2
           // return ((_mm_movemask_ps(vTemp)==0x0f) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMVector4LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean; assembler;
asm
           VMOVUPS XMM1,[v1]
           VMOVUPS XMM2,[v2]
           VCMPPS  XMM0, XMM1, XMM2, 2
           VMOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4LessOrEqual(constref V1: TXMVECTOR; constref V2: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllTrue(XMVector4GreaterOrEqualR(V2, V1));
end;

{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    Result :=
        ((V.f32[0] <= Bounds.f32[0]) and (V.f32[0] >= -Bounds.f32[0])) and ((V.f32[1] <= Bounds.f32[1]) and (V.f32[1] >= -Bounds.f32[1])) and ((V.f32[2] <= Bounds.f32[2]) and
        (V.f32[2] >= -Bounds.f32[2])) and ((V.f32[3] <= Bounds.f32[3]) and (V.f32[3] >= -Bounds.f32[3]));
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    (* ToDo
     // Test if less than or equal
    uint32x4_t ivTemp1 = vcleq_f32(V,Bounds);
    // Negate the bounds
    float32x4_t vTemp2 = vnegq_f32(Bounds);
    // Test if greater or equal (Reversed)
    uint32x4_t ivTemp2 = vcleq_f32(vTemp2,V);
    // Blend answers
    ivTemp1 = vandq_u32(ivTemp1,ivTemp2);
    // in bounds?
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(ivTemp1), vget_high_u8(ivTemp1));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU );
*)
end;

{$ELSEIF DEFINED(_XM_SSE_INTRINSICS_)}
function XMVector4InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean; assembler;
asm
           // Test if less than or equal
           // vTemp1 = _mm_cmple_ps(V,Bounds);
           MOVUPS  XMM0, [V]
           CMPPS   XMM0, [Bounds], 2
           // Negate the bounds
           //  vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
           MOVUPS  XMM1, [Bounds]
           MULPS   XMM1, [g_XMNegativeOne]
           // Test if greater or equal (Reversed)
           // vTemp2 = _mm_cmple_ps(vTemp2,V);
           CMPPS   XMM1, [V], 2
           // Blend answers
           // vTemp1 = _mm_and_ps(vTemp1,vTemp2);
           ANDPS   XMM0, XMM1
           // All in bounds?
           // return ((_mm_movemask_ps(vTemp1)==0x0f) != 0);
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]
end;
{$ELSE}
function XMVector4InBounds(constref V: TXMVECTOR; constref Bounds: TXMVECTOR): boolean;
begin
    Result := XMComparisonAllInBounds(XMVector4InBoundsR(V, Bounds));
end;

{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4IsNaN(constref V: TXMVECTOR): boolean;
begin
    Result := XMISNAN(V.u32[0]) or XMISNAN(V.u32[1]) or XMISNAN(V.u32[2]) or XMISNAN(V.u32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4IsNaN(constref V: TXMVECTOR): boolean;
begin
    (* ToDo
   // Test against itself. NaN is always not equal
    uint32x4_t vTempNan = vceqq_f32( V, V );
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vTempNan), vget_high_u8(vTempNan));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    // If any are NaN, the mask is zero
    return ( vget_lane_u32(vTemp.val[1], 1) != 0xFFFFFFFFU );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4IsNaN(constref V: TXMVECTOR): boolean; assembler;
asm
           // Test against itself. NaN is always not equal
           //  vTempNan = _mm_cmpneq_ps(V,V);
           MOVUPS  XMM0, [V]
           CMPPS   XMM0, XMM0, 4
           // If any are NaN, the mask is non-zero
           //return (_mm_movemask_ps(vTempNan)!=0);
           MOVMSKPS EAX, XMM0
           CMP     EAX, $0
           SETE    [result]
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4IsInfinite(constref V: TXMVECTOR): boolean;
begin
    Result := XMISINF(V.u32[0]) or XMISINF(V.u32[1]) or XMISINF(V.u32[2]) or XMISINF(V.u32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4IsInfinite(constref V: TXMVECTOR): boolean;
begin
    (* ToDo
    // Mask off the sign bit
    uint32x4_t vTempInf = vandq_u32( V, g_XMAbsMask );
    // Compare to infinity
    vTempInf = vceqq_f32(vTempInf, g_XMInfinity );
    // If any are infinity, the signs are true.
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vTempInf), vget_high_u8(vTempInf));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    return ( vget_lane_u32(vTemp.val[1], 1) != 0 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4IsInfinite(constref V: TXMVECTOR): boolean; assembler;
asm
           // Mask off the sign bit
           //  vTemp = _mm_and_ps(V,g_XMAbsMask);
           MOVUPS  XMM0, [V]
           ANDPS   XMM0, [g_XMAbsMask]
           // Compare to infinity
           // vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 0
           // If any are infinity, the signs are true.
           // return (_mm_movemask_ps(vTemp) != 0);
           MOVMSKPS EAX, XMM0
           CMP     EAX, $0
           SETE    [result]
end;
{$ENDIF}


//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
var
    f: single;
begin
    f := V1.f32[0] * V2.f32[0] + V1.f32[1] * V2.f32[1] + V1.f32[2] * V2.f32[2] + V1.f32[3] * V2.f32[3];
    Result.f32[0] := f;
    Result.f32[1] := f;
    Result.f32[2] := f;
    Result.f32[3] := f;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     float32x4_t vTemp = vmulq_f32( V1, V2 );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vadd_f32( v1, v2 );
    v1 = vpadd_f32( v1, v1 );
    return vcombine_f32( v1, v1 );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector4Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           //return _mm_dp_ps( V1, V2, 0xff );
           MOVUPS  XMM0,[V1]
           DPPS    XMM0, [V2], $FF
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector4Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_mul_ps(V1, V2);
           MOVUPS  XMM0,[V1]
           MULPS   XMM0, [V2]
           // vTemp = _mm_hadd_ps(vTemp, vTemp);
           HADDPS  XMM0, XMM0
           // return _mm_hadd_ps(vTemp, vTemp);
           HADDPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4Dot(constref V1: TXMVECTOR; constref V2: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vTemp2 = V2;
           MOVUPS  XMM1, [V2]
           //  vTemp = _mm_mul_ps(V1,vTemp2);
           MOVUPS  XMM0,[V1]
           MULPS   XMM0, XMM1
           // vTemp2 = _mm_shuffle_ps(vTemp2,vTemp,_MM_SHUFFLE(1,0,0,0)); // Copy X to the Z position and Y to the W position
           SHUFPS  XMM1, XMM0, _MM_SHUFFLE_1_0_0_0
           // vTemp2 = _mm_add_ps(vTemp2,vTemp);          // Add Z = X+Z; W = Y+W;
           ADDPS   XMM1, XMM0
           // vTemp = _mm_shuffle_ps(vTemp,vTemp2,_MM_SHUFFLE(0,3,0,0));  // Copy W to the Z position
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_0_3_0_0
           // vTemp = _mm_add_ps(vTemp,vTemp2);           // Add Z and W together
           ADDPS   XMM0, XMM1
           // return XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(2,2,2,2));    // Splat Z and return
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           MOVUPS  [Result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
// [ ((v2.z*v3.w-v2.w*v3.z)*v1.y)-((v2.y*v3.w-v2.w*v3.y)*v1.z)+((v2.y*v3.z-v2.z*v3.y)*v1.w),
//   ((v2.w*v3.z-v2.z*v3.w)*v1.x)-((v2.w*v3.x-v2.x*v3.w)*v1.z)+((v2.z*v3.x-v2.x*v3.z)*v1.w),
//   ((v2.y*v3.w-v2.w*v3.y)*v1.x)-((v2.x*v3.w-v2.w*v3.x)*v1.y)+((v2.x*v3.y-v2.y*v3.x)*v1.w),
//   ((v2.z*v3.y-v2.y*v3.z)*v1.x)-((v2.z*v3.x-v2.x*v3.z)*v1.y)+((v2.y*v3.x-v2.x*v3.y)*v1.z) ]
function XMVector4Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := (((V2.f32[2] * V3.f32[3]) - (V2.f32[3] * V3.f32[2])) * V1.f32[1]) - (((V2.f32[1] * V3.f32[3]) - (V2.f32[3] * V3.f32[1])) * V1.f32[2]) + (((V2.f32[1] * V3.f32[2]) - (V2.f32[2] * V3.f32[1])) * V1.f32[3]);
    Result.f32[1] := (((V2.f32[3] * V3.f32[2]) - (V2.f32[2] * V3.f32[3])) * V1.f32[0]) - (((V2.f32[3] * V3.f32[0]) - (V2.f32[0] * V3.f32[3])) * V1.f32[2]) + (((V2.f32[2] * V3.f32[0]) - (V2.f32[0] * V3.f32[2])) * V1.f32[3]);
    Result.f32[2] := (((V2.f32[1] * V3.f32[3]) - (V2.f32[3] * V3.f32[1])) * V1.f32[0]) - (((V2.f32[0] * V3.f32[3]) - (V2.f32[3] * V3.f32[0])) * V1.f32[1]) + (((V2.f32[0] * V3.f32[1]) - (V2.f32[1] * V3.f32[0])) * V1.f32[3]);
    Result.f32[3] := (((V2.f32[2] * V3.f32[1]) - (V2.f32[1] * V3.f32[2])) * V1.f32[0]) - (((V2.f32[2] * V3.f32[0]) - (V2.f32[0] * V3.f32[2])) * V1.f32[1]) + (((V2.f32[1] * V3.f32[0]) - (V2.f32[0] * V3.f32[1])) * V1.f32[2]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Cross(V1: TXMVECTOR; V2: TXMVECTOR; V3: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    const float32x2_t select = vget_low_f32( g_XMMaskX );

    // Term1: V2zwyz * V3wzwy
    const float32x2_t v2xy = vget_low_f32(V2);
    const float32x2_t v2zw = vget_high_f32(V2);
    const float32x2_t v2yx = vrev64_f32(v2xy);
    const float32x2_t v2wz = vrev64_f32(v2zw);
    const float32x2_t v2yz = vbsl_f32( select, v2yx, v2wz );

    const float32x2_t v3zw = vget_high_f32(V3);
    const float32x2_t v3wz = vrev64_f32(v3zw);
    const float32x2_t v3xy = vget_low_f32(V3);
    const float32x2_t v3wy = vbsl_f32( select, v3wz, v3xy );

    float32x4_t vTemp1 = vcombine_f32(v2zw,v2yz);
    float32x4_t vTemp2 = vcombine_f32(v3wz,v3wy);
    XMVECTOR vResult = vmulq_f32( vTemp1, vTemp2 );

    // - V2wzwy * V3zwyz
    const float32x2_t v2wy = vbsl_f32( select, v2wz, v2xy );

    const float32x2_t v3yx = vrev64_f32(v3xy);
    const float32x2_t v3yz = vbsl_f32( select, v3yx, v3wz );

    vTemp1 = vcombine_f32(v2wz,v2wy);
    vTemp2 = vcombine_f32(v3zw,v3yz);
    vResult = vmlsq_f32( vResult, vTemp1, vTemp2 );

    // term1 * V1yxxx
    const float32x2_t v1xy = vget_low_f32(V1);
    const float32x2_t v1yx = vrev64_f32(v1xy);

    vTemp1 = vcombine_f32( v1yx, vdup_lane_f32( v1yx, 1 ) );
    vResult = vmulq_f32( vResult, vTemp1 );

    // Term2: V2ywxz * V3wxwx
    const float32x2_t v2yw = vrev64_f32(v2wy);
    const float32x2_t v2xz = vbsl_f32( select, v2xy, v2wz );

    const float32x2_t v3wx = vbsl_f32( select, v3wz, v3yx );

    vTemp1 = vcombine_f32(v2yw,v2xz);
    vTemp2 = vcombine_f32(v3wx,v3wx);
    float32x4_t vTerm = vmulq_f32( vTemp1, vTemp2 );

    // - V2wxwx * V3ywxz
    const float32x2_t v2wx = vbsl_f32( select, v2wz, v2yx );

    const float32x2_t v3yw = vrev64_f32(v3wy);
    const float32x2_t v3xz = vbsl_f32( select, v3xy, v3wz );

    vTemp1 = vcombine_f32(v2wx,v2wx);
    vTemp2 = vcombine_f32(v3yw,v3xz);
    vTerm = vmlsq_f32( vTerm, vTemp1, vTemp2 );

    // vResult - term2 * V1zzyy
    const float32x2_t v1zw = vget_high_f32(V1);

    vTemp1 = vcombine_f32( vdup_lane_f32(v1zw, 0), vdup_lane_f32(v1yx, 0) );
    vResult = vmlsq_f32( vResult, vTerm, vTemp1 );

    // Term3: V2yzxy * V3zxyx
    const float32x2_t v3zx = vrev64_f32(v3xz);

    vTemp1 = vcombine_f32(v2yz,v2xy);
    vTemp2 = vcombine_f32(v3zx,v3yx);
    vTerm = vmulq_f32( vTemp1, vTemp2 );

    // - V2zxyx * V3yzxy
    const float32x2_t v2zx = vrev64_f32(v2xz);

    vTemp1 = vcombine_f32(v2zx,v2yx);
    vTemp2 = vcombine_f32(v3yz,v3xy);
    vTerm = vmlsq_f32( vTerm, vTemp1, vTemp2 );

    // vResult + term3 * V1wwwz
    const float32x2_t v1wz = vrev64_f32(v1zw);

    vTemp1 = vcombine_f32( vdup_lane_f32( v1wz, 0 ), v1wz );
    return vmlaq_f32( vResult, vTerm, vTemp1 );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4Cross(constref V1: TXMVECTOR; constref V2: TXMVECTOR; constref V3: TXMVECTOR): TXMVECTOR; assembler;
asm
           // V2zwyz * V3wzwy
           //  vResult = XM_PERMUTE_PS(V2,_MM_SHUFFLE(2,1,3,2));
           MOVUPS  XMM0,[V2]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_1_3_2
           //  vTemp3 = XM_PERMUTE_PS(V3,_MM_SHUFFLE(1,3,2,3));
           MOVUPS  XMM3, [V3]
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_1_3_2_3
           // vResult = _mm_mul_ps(vResult,vTemp3);
           MULPS   XMM0, XMM3
           // - V2wzwy * V3zwyz
           //  vTemp2 = XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,3,2,3));
           MOVUPS  XMM2 ,[V2]
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_3_2_3
           // vTemp3 = XM_PERMUTE_PS(vTemp3,_MM_SHUFFLE(1,3,0,1));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_1_3_0_1
           // vTemp2 = _mm_mul_ps(vTemp2,vTemp3);
           MULPS   XMM2, XMM3
           //  vResult = _mm_sub_ps(vResult,vTemp2);
           SUBPS   XMM0, XMM2
           // term1 * V1yxxx
           //  vTemp1 = XM_PERMUTE_PS(V1,_MM_SHUFFLE(0,0,0,1));
           MOVUPS  XMM1, [V1]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_0_0_0_1
           // vResult = _mm_mul_ps(vResult,vTemp1);
           MULPS   XMM0, XMM1

           // V2ywxz * V3wxwx
           // vTemp2 = XM_PERMUTE_PS(V2,_MM_SHUFFLE(2,0,3,1));
           MOVUPS  XMM2, [V2]
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_0_3_1
           // vTemp3 = XM_PERMUTE_PS(V3,_MM_SHUFFLE(0,3,0,3));
           MOVUPS  XMM3, [V3]
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_0_3_0_3
           // vTemp3 = _mm_mul_ps(vTemp3,vTemp2);
           MULPS   XMM3, XMM2
           // - V2wxwx * V3ywxz
           // vTemp2 = XM_PERMUTE_PS(vTemp2,_MM_SHUFFLE(2,1,2,1));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_1_2_1
           // vTemp1 = XM_PERMUTE_PS(V3,_MM_SHUFFLE(2,0,3,1));
           MOVUPS  XMM1, [V3]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_0_3_1
           // vTemp2 = _mm_mul_ps(vTemp2,vTemp1);
           MULPS   XMM2, XMM1
           // vTemp3 = _mm_sub_ps(vTemp3,vTemp2);
           SUBPS   XMM3, XMM2
           // vResult - temp * V1zzyy
           // vTemp1 = XM_PERMUTE_PS(V1,_MM_SHUFFLE(1,1,2,2));
           MOVUPS  XMM1, [V1]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_2_2
           // vTemp1 = _mm_mul_ps(vTemp1,vTemp3);
           MULPS   XMM1, XMM3
           // vResult = _mm_sub_ps(vResult,vTemp1);
           SUBPS   XMM0, XMM1

           // V2yzxy * V3zxyx
           // vTemp2 = XM_PERMUTE_PS(V2,_MM_SHUFFLE(1,0,2,1));
           MOVUPS  XMM2, [V2]
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_0_2_1
           // vTemp3 = XM_PERMUTE_PS(V3,_MM_SHUFFLE(0,1,0,2));
           MOVUPS  XMM3, [V3]
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_0_1_0_2
           // vTemp3 = _mm_mul_ps(vTemp3,vTemp2);
           MULPS   XMM3, XMM2
           // - V2zxyx * V3yzxy
           // vTemp2 = XM_PERMUTE_PS(vTemp2,_MM_SHUFFLE(2,0,2,1));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_0_2_1
           // vTemp1 = XM_PERMUTE_PS(V3,_MM_SHUFFLE(1,0,2,1));
           MOVUPS  XMM1, [V3]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_0_2_1
           // vTemp1 = _mm_mul_ps(vTemp1,vTemp2);
           MULPS   XMM1, XMM2
           // vTemp3 = _mm_sub_ps(vTemp3,vTemp1);
           SUBPS   XMM3, XMM1
           // vResult + term * V1wwwz
           // vTemp1 = XM_PERMUTE_PS(V1,_MM_SHUFFLE(2,3,3,3));
           MOVUPS  XMM1 ,[V1]
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_3_3_3
           // vTemp3 = _mm_mul_ps(vTemp3,vTemp1);
           MULPS   XMM3, XMM1
           // vResult = _mm_add_ps(vResult,vTemp3);
           ADDPS   XMM0, XMM3
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

function XMVector4LengthSq(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4Dot(V, V);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4LengthSq(V);
    Result := XMVectorReciprocalSqrtEst(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot4
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vadd_f32( v1, v2 );
    v1 = vpadd_f32( v1, v1 );
    // Reciprocal sqrt (estimate)
    v2 = vrsqrte_f32( v1 );
    return vcombine_f32(v2, v2);
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector4ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0xff );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $FF
           // return _mm_rsqrt_ps( vTemp );
           RSQRTPS XMM0, XMM0
           MOVUPS  [result] , XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector4ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_rsqrt_ps(vLengthSq);
           RSQRTPS XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result] , XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4ReciprocalLengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y,z and w
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and w
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(3,2,3,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_2_3_2
           // x+z, y+w
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // x+z,x+z,x+z,y+w
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_0_0
           // ??,??,y+w,y+w
           // vTemp = _mm_shuffle_ps(vTemp,vLengthSq,_MM_SHUFFLE(3,3,0,0));
           SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_0_0
           // ??,??,x+z+y+w,??
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // Splat the length
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // Get the reciprocal
           // vLengthSq = _mm_rsqrt_ps(vLengthSq);
           RSQRTPS XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4LengthSq(V);
    Result := XMVectorReciprocalSqrt(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot4
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vadd_f32( v1, v2 );
    v1 = vpadd_f32( v1, v1 );
    // Reciprocal sqrt
    float32x2_t  S0 = vrsqrte_f32(v1);
    float32x2_t  P0 = vmul_f32( v1, S0 );
    float32x2_t  R0 = vrsqrts_f32( P0, S0 );
    float32x2_t  S1 = vmul_f32( S0, R0 );
    float32x2_t  P1 = vmul_f32( v1, S1 );
    float32x2_t  R1 = vrsqrts_f32( P1, S1 );
    float32x2_t Result = vmul_f32( S1, R1 );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector4ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0xff );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $FF
           //  vLengthSq = _mm_sqrt_ps( vTemp );
           SQRTPS  XMM1, XMM0
           // return _mm_div_ps( g_XMOne, vLengthSq );
           MOVUPS  XMM0,[g_XMOne]
           DIVPS   XMM0, XMM1
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector4ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // vLengthSq = _mm_div_ps(g_XMOne, vLengthSq);
           MOVUPS  XMM0,[g_XMOne]
           DIVPS   XMM0, XMM1
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4ReciprocalLength(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y,z and w
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and w
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(3,2,3,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_2_3_2
           // x+z, y+w
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // x+z,x+z,x+z,y+w
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_0_0
           // ??,??,y+w,y+w
           // vTemp = _mm_shuffle_ps(vTemp,vLengthSq,_MM_SHUFFLE(3,3,0,0));
           SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_0_0
           // ??,??,x+z+y+w,??
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // Splat the length
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // Get the reciprocal
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // Accurate!
           // vLengthSq = _mm_div_ps(g_XMOne,vLengthSq);
           MOVUPS  XMM1,[g_XMOne]
           DIVPS   XMM1, XMM0
           // return vLengthSq;
           MOVUPS  [result],XMM1
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4LengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4LengthSq(V);
    Result := XMVectorSqrtEst(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4LengthEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot4
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vadd_f32( v1, v2 );
    v1 = vpadd_f32( v1, v1 );
    const float32x2_t zero = vdup_n_f32(0);
    uint32x2_t VEqualsZero = vceq_f32( v1, zero );
    // Sqrt (estimate)
    float32x2_t Result = vrsqrte_f32( v1 );
    Result = vmul_f32( v1, Result );
    Result = vbsl_f32( VEqualsZero, zero, Result );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector4LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0xff );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $FF
           // return _mm_sqrt_ps( vTemp );
           SQRTPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector4LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4LengthEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y,z and w
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0, [V]
           MULPS   XMM0, XMM0
           // vTemp has z and w
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(3,2,3,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_2_3_2
           // x+z, y+w
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // x+z,x+z,x+z,y+w
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_0_0
           // ??,??,y+w,y+w
           // vTemp = _mm_shuffle_ps(vTemp,vLengthSq,_MM_SHUFFLE(3,3,0,0));
           SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_0_0
           // ??,??,x+z+y+w,??
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // Splat the length
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // Get the length
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Length(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4LengthSq(V);
    Result := XMVectorSqrt(Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Length(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    // Dot4
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vadd_f32( v1, v2 );
    v1 = vpadd_f32( v1, v1 );
    const float32x2_t zero = vdup_n_f32(0);
    uint32x2_t VEqualsZero = vceq_f32( v1, zero );
    // Sqrt
    float32x2_t S0 = vrsqrte_f32( v1 );
    float32x2_t P0 = vmul_f32( v1, S0 );
    float32x2_t R0 = vrsqrts_f32( P0, S0 );
    float32x2_t S1 = vmul_f32( S0, R0 );
    float32x2_t P1 = vmul_f32( v1, S1 );
    float32x2_t R1 = vrsqrts_f32( P1, S1 );
    float32x2_t Result = vmul_f32( S1, R1 );
    Result = vmul_f32( v1, Result );
    Result = vbsl_f32( VEqualsZero, zero, Result );
    return vcombine_f32( Result, Result );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector4Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0xff );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $FF
           // return _mm_sqrt_ps( vTemp );
           SQRTPS  XMM0, XMM0
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector4Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4Length(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y,z and w
           // vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and w
           // vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(3,2,3,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_2_3_2
           // x+z, y+w
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // x+z,x+z,x+z,y+w
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_0_0
           // ??,??,y+w,y+w
           // vTemp = _mm_shuffle_ps(vTemp,vLengthSq,_MM_SHUFFLE(3,3,0,0));
           SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_0_0
           // ??,??,x+z+y+w,??
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // Splat the length
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // Get the length
           // vLengthSq = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM0, XMM0
           // return vLengthSq;
           MOVUPS  [result],XMM0
end;
{$ENDIF}


//------------------------------------------------------------------------------
// XMVector4NormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.
{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4ReciprocalLength(V);
    Result := XMVectorMultiply(V, Result);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4NormalizeEst(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Dot4
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vadd_f32( v1, v2 );
    v1 = vpadd_f32( v1, v1 );
    // Reciprocal sqrt (estimate)
    v2 = vrsqrte_f32( v1 );
    // Normalize
    return vmulq_f32( V, vcombine_f32(v2,v2) );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector4NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vTemp = _mm_dp_ps( V, V, 0xff );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $FF
           //  vResult = _mm_rsqrt_ps( vTemp );
           RSQRTPS XMM0, XMM0
           // return _mm_mul_ps(vResult, V);
           MULPS   XMM0, [V]
           MOVUPS  [result],XMM0
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector4NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vDot = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vDot = _mm_hadd_ps(vDot, vDot);
           HADDPS  XMM0, XMM0
           // vDot = _mm_hadd_ps(vDot, vDot);
           HADDPS  XMM0, XMM0
           // vDot = _mm_rsqrt_ps(vDot);
           RSQRTPS XMM0, XMM0
           // vDot = _mm_mul_ps(vDot, V);
           MULPS   XMM0, [V]
           // return vDot;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4NormalizeEst(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y,z and w
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and w
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(3,2,3,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_2_3_2
           // x+z, y+w
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // x+z,x+z,x+z,y+w
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_0_0
           // ??,??,y+w,y+w
           // vTemp = _mm_shuffle_ps(vTemp,vLengthSq,_MM_SHUFFLE(3,3,0,0));
           SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_0_0
           // ??,??,x+z+y+w,??
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // Splat the length
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // Get the reciprocal
           //  vResult = _mm_rsqrt_ps(vLengthSq);
           RSQRTPS XMM0, XMM0
           // Reciprocal mul to perform the normalization
           // vResult = _mm_mul_ps(vResult,V);
           MULPS   XMM0, [V]
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Normalize(constref V: TXMVECTOR): TXMVECTOR;
var
    fLength: single;
    vResult: TXMVECTOR;
begin
    vResult := XMVector4Length(V);
    fLength := vResult.f32[0];

    // Prevent divide by zero
    if (fLength > 0) then
        fLength := 1.0 / fLength;

    Result.f32[0] := V.f32[0] * fLength;
    Result.f32[1] := V.f32[1] * fLength;
    Result.f32[2] := V.f32[2] * fLength;
    Result.f32[3] := V.f32[3] * fLength;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Normalize(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Dot4
    float32x4_t vTemp = vmulq_f32( V, V );
    float32x2_t v1 = vget_low_f32( vTemp );
    float32x2_t v2 = vget_high_f32( vTemp );
    v1 = vadd_f32( v1, v2 );
    v1 = vpadd_f32( v1, v1 );
    uint32x2_t VEqualsZero = vceq_f32( v1, vdup_n_f32(0) );
    uint32x2_t VEqualsInf = vceq_f32( v1, vget_low_f32(g_XMInfinity) );
    // Reciprocal sqrt (2 iterations of Newton-Raphson)
    float32x2_t S0 = vrsqrte_f32( v1 );
    float32x2_t P0 = vmul_f32( v1, S0 );
    float32x2_t R0 = vrsqrts_f32( P0, S0 );
    float32x2_t S1 = vmul_f32( S0, R0 );
    float32x2_t P1 = vmul_f32( v1, S1 );
    float32x2_t R1 = vrsqrts_f32( P1, S1 );
    v2 = vmul_f32( S1, R1 );
    // Normalize
    XMVECTOR vResult = vmulq_f32( V, vcombine_f32(v2,v2) );
    vResult = vbslq_f32( vcombine_f32(VEqualsZero,VEqualsZero), vdupq_n_f32(0), vResult );
    return vbslq_f32( vcombine_f32(VEqualsInf,VEqualsInf), g_XMQNaN, vResult );
*)
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMVector4Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // vLengthSq = _mm_dp_ps( V, V, 0xff );
           MOVUPS  XMM0,[V]
           DPPS    XMM0, XMM0, $FF
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Create zero with a single instruction
           //  vZeroMask = _mm_setzero_ps();
           XORPS   XMM2, XMM2
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM2, XMM1, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Divide to perform the normalization
           // vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM3,[V]
           DIVPS   XMM3, XMM1
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM3, XMM2
           // Select qnan or result based on infinite length
           //  vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM2, [g_XMQNaN]
           ANDNPS  XMM2, XMM0
           //  vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM1, XMM0
           // vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM1, XMM2
           // return vResult;
           MOVUPS  [result],XMM1
end;
{$ELSEIF DEFINED(_XM_SSE3_INTRINSICS_)}
function XMVector4Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y,z and w
           //  vLengthSq = _mm_mul_ps(V, V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // vLengthSq = _mm_hadd_ps(vLengthSq, vLengthSq);
           HADDPS  XMM0, XMM0
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Create zero with a single instruction
           //  vZeroMask = _mm_setzero_ps();
           XORPS   XMM2, XMM2
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM2, XMM1, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Divide to perform the normalization
           // vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM3,[V]
           DIVPS   XMM3, XMM1
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM1, XMM2
           // Select qnan or result based on infinite length
           //  vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM2, [g_XMQNaN]
           ANDNPS  XMM2, XMM0
           //  vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM0, XMM1
           // vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM0, XMM2
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4Normalize(constref V: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y,z and w
           //  vLengthSq = _mm_mul_ps(V,V);
           MOVUPS  XMM0,[V]
           MULPS   XMM0, XMM0
           // vTemp has z and w
           //  vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(3,2,3,2));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_2_3_2
           // x+z, y+w
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // x+z,x+z,x+z,y+w
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(1,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_0_0
           // ??,??,y+w,y+w
           // vTemp = _mm_shuffle_ps(vTemp,vLengthSq,_MM_SHUFFLE(3,3,0,0));
           SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_3_0_0
           // ??,??,x+z+y+w,??
           // vLengthSq = _mm_add_ps(vLengthSq,vTemp);
           ADDPS   XMM0, XMM1
           // Splat the length
           // vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_2_2_2
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM2, XMM0
           // Create zero with a single instruction
           //  vZeroMask = _mm_setzero_ps();
           XORPS   XMM3, XMM3
           // Test for a divide by zero (Must be FP to detect -0.0)
           // vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
           CMPPS   XMM3, XMM2, 4
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Divide to perform the normalization
           // vResult = _mm_div_ps(V,vResult);
           MOVUPS  XMM4, [V]
           DIVPS   XMM4, XMM2
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vZeroMask);
           ANDPS   XMM4, XMM3
           // Select qnan or result based on infinite length
           //  vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
           MOVUPS  XMM1, [g_XMQNaN]
           ANDNPS  XMM1, XMM0
           //  vTemp2 = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM4, XMM0
           // vResult = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM1, XMM4
           // return vResult;
           MOVUPS  [result],XMM1
end;
{$ENDIF}




function XMVector4ClampLength(V: TXMVECTOR; LengthMin: single; LengthMax: single): TXMVECTOR;
var
    ClampMax, ClampMin: TXMVECTOR;
begin
    ClampMax := XMVectorReplicate(LengthMax);
    ClampMin := XMVectorReplicate(LengthMin);

    Result := XMVector4ClampLengthV(V, ClampMin, ClampMax);
end;


function XMVector4ClampLengthV(V: TXMVECTOR; LengthMin: TXMVECTOR; LengthMax: TXMVECTOR): TXMVECTOR;
var
    LengthSq, Zero: TXMVECTOR;
    RcpLength, InfiniteLength, ZeroLength: TXMVECTOR;
    Normal, Length, Select: TXMVECTOR;
    ControlMax, ControlMin, ClampLength, Control: TXMVECTOR;
begin
    assert((XMVectorGetY(LengthMin) = XMVectorGetX(LengthMin)) and (XMVectorGetZ(LengthMin) = XMVectorGetX(LengthMin)) and (XMVectorGetW(LengthMin) = XMVectorGetX(LengthMin)));
    assert((XMVectorGetY(LengthMax) = XMVectorGetX(LengthMax)) and (XMVectorGetZ(LengthMax) = XMVectorGetX(LengthMax)) and (XMVectorGetW(LengthMax) = XMVectorGetX(LengthMax)));
    assert(XMVector4GreaterOrEqual(LengthMin, XMVectorZero()));
    assert(XMVector4GreaterOrEqual(LengthMax, XMVectorZero()));
    assert(XMVector4GreaterOrEqual(LengthMax, LengthMin));

    LengthSq := XMVector4LengthSq(V);

    Zero := XMVectorZero();

    RcpLength := XMVectorReciprocalSqrt(LengthSq);

    InfiniteLength := XMVectorEqualInt(LengthSq, g_XMInfinity.v);
    ZeroLength := XMVectorEqual(LengthSq, Zero);

    Normal := XMVectorMultiply(V, RcpLength);

    Length := XMVectorMultiply(LengthSq, RcpLength);

    Select := XMVectorEqualInt(InfiniteLength, ZeroLength);
    Length := XMVectorSelect(LengthSq, Length, Select);
    Normal := XMVectorSelect(LengthSq, Normal, Select);


    ControlMax := XMVectorGreater(Length, LengthMax);
    ControlMin := XMVectorLess(Length, LengthMin);

    ClampLength := XMVectorSelect(Length, LengthMax, ControlMax);
    ClampLength := XMVectorSelect(ClampLength, LengthMin, ControlMin);

    Result := XMVectorMultiply(Normal, ClampLength);

    // Preserve the original vector (with no precision loss) if the length falls within the given range
    Control := XMVectorEqualInt(ControlMax, ControlMin);
    Result := XMVectorSelect(Result, V, Control);
end;



function XMVector4Reflect(Incident: TXMVECTOR; Normal: TXMVECTOR): TXMVECTOR;
begin
    // Result  :=  Incident - (2 * dot(Incident, Normal)) * Normal
    Result := XMVector4Dot(Incident, Normal);
    Result := XMVectorAdd(Result, Result);
    Result := XMVectorNegativeMultiplySubtract(Result, Normal, Incident);
end;



function XMVector4Refract(Incident: TXMVECTOR; Normal: TXMVECTOR; RefractionIndex: single): TXMVECTOR;
var
    Index: TXMVECTOR;
begin
    Index := XMVectorReplicate(RefractionIndex);
    Result := XMVector4RefractV(Incident, Normal, Index);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4RefractV(Incident: TXMVECTOR; Normal: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR;
var
    IDotN: TXMVECTOR;
    R: TXMVECTOR;
    Zero: TXMVECTOR;
begin
    Zero := XMVectorZero();

    // Result  :=  RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))

    IDotN := XMVector4Dot(Incident, Normal);

    // R  :=  1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    R := XMVectorNegativeMultiplySubtract(IDotN, IDotN, g_XMOne.v);
    R := XMVectorMultiply(R, RefractionIndex);
    R := XMVectorNegativeMultiplySubtract(R, RefractionIndex, g_XMOne.v);

    if (XMVector4LessOrEqual(R, Zero)) then
    begin
        // Total internal reflection
        Result := Zero;
    end
    else
    begin
        // R  :=  RefractionIndex * IDotN + sqrt(R)
        R := XMVectorSqrt(R);
        R := XMVectorMultiplyAdd(RefractionIndex, IDotN, R);

        // Result  :=  RefractionIndex * Incident - Normal * R
        Result := XMVectorMultiply(RefractionIndex, Incident);
        Result := XMVectorNegativeMultiplySubtract(Normal, R, Result);
    end;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4RefractV(Incident: TXMVECTOR; Normal: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
    XMVECTOR IDotN = XMVector4Dot(Incident,Normal);

    // R = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    float32x4_t R = vmlsq_f32( g_XMOne, IDotN, IDotN);
    R = vmulq_f32(R, RefractionIndex);
    R = vmlsq_f32(g_XMOne, R, RefractionIndex );

    uint32x4_t vResult = vcleq_f32(R,g_XMZero);
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vResult), vget_high_u8(vResult));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    if ( vget_lane_u32(vTemp.val[1], 1) == 0xFFFFFFFFU )
    {
        // Total internal reflection
        vResult = g_XMZero;
    }
    else
    {
        // Sqrt(R)
        float32x4_t S0 = vrsqrteq_f32(R);
        float32x4_t P0 = vmulq_f32( R, S0 );
        float32x4_t R0 = vrsqrtsq_f32( P0, S0 );
        float32x4_t S1 = vmulq_f32( S0, R0 );
        float32x4_t P1 = vmulq_f32( R, S1 );
        float32x4_t R1 = vrsqrtsq_f32( P1, S1 );
        float32x4_t S2 = vmulq_f32( S1, R1 );
        R = vmulq_f32( R, S2 );
        // R = RefractionIndex * IDotN + sqrt(R)
        R = vmlaq_f32( R, RefractionIndex, IDotN );
        // Result = RefractionIndex * Incident - Normal * R
        vResult = vmulq_f32(RefractionIndex, Incident);
        vResult = vmlsq_f32( vResult, R, Normal );
    }
    return vResult;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4RefractV(Incident: TXMVECTOR; Normal: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR; inline;
var
    IDotN: TXMVECTOR;
begin
    IDotN := XMVector4Dot(Incident, Normal);
    asm


               // R = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
               MOVUPS  XMM0,[IDotN]
               // R = _mm_mul_ps(IDotN,IDotN);
               MULPS   XMM0, XMM0
               // R = _mm_sub_ps(g_XMOne,R);
               MOVUPS  XMM1, [g_XMOne]
               SUBPS   XMM1, XMM0
               // R = _mm_mul_ps(R, RefractionIndex);
               MULPS   XMM1, [RefractionIndex]
               // R = _mm_mul_ps(R, RefractionIndex);
               MULPS   XMM1, [RefractionIndex]
               // R = _mm_sub_ps(g_XMOne,R);
               MOVUPS  XMM0, [g_XMOne]
               SUBPS   XMM0, XMM1

               //  vResult = _mm_cmple_ps(R,g_XMZero);
               MOVUPS  XMM1, XMM0  // XMM1 = R
               CMPPS   XMM0, [g_XMZero], 2
               // if (_mm_movemask_ps(vResult)==0x0f)
               MOVMSKPS EAX, XMM0
               AND     EAX, $F
               CMP     EAX, $F
               JE      @TotalInternalReflection

               // R = RefractionIndex * IDotN + sqrt(R)
               // R = _mm_sqrt_ps(R);
               SQRTPS  XMM1, XMM1
               //vResult = _mm_mul_ps(RefractionIndex, IDotN);
               MOVUPS  XMM0,[RefractionIndex]
               MULPS   XMM0, [IDotN]
               //R = _mm_add_ps(R,vResult);
               ADDPS   XMM1, XMM0
               // Result = RefractionIndex * Incident - Normal * R
               //vResult = _mm_mul_ps(RefractionIndex, Incident);
               MOVUPS  XMM0, [RefractionIndex]
               MULPS   XMM0, [Incident]
               //R = _mm_mul_ps(R,Normal);
               MULPS   XMM1, [Normal]
               //vResult = _mm_sub_ps(vResult,R);
               SUBPS   XMM0, XMM1
               JMP     @Finished
               @TotalInternalReflection:
               MOVUPS  XMM0,[g_XMZero]
               @Finished:
               // return vResult;
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Orthogonal(constref V: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := V.f32[2];
    Result.f32[1] := V.f32[3];
    Result.f32[2] := -V.f32[0];
    Result.f32[3] := -V.f32[1];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Orthogonal(constref V: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     static const XMVECTORF32 Negate = { { { 1.f, 1.f, -1.f, -1.f } } };

    float32x4_t Result = vcombine_f32( vget_high_f32( V ), vget_low_f32( V ) );
    return vmulq_f32( Result, Negate );
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4Orthogonal(constref V: TXMVECTOR): TXMVECTOR; assembler;
const
    FlipZW: TXMVECTORF32 = (f: (1.0, 1.0, -1.0, -1.0));
asm
           // vResult = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,0,3,2));
           MOVUPS  XMM0, [V]
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_1_0_3_2
           // vResult = _mm_mul_ps(vResult,FlipZW);
           MULPS   XMM0 ,[FlipZW]
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ENDIF}



function XMVector4AngleBetweenNormalsEst(N1: TXMVECTOR; N2: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4Dot(N1, N2);
    Result := XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result := XMVectorACosEst(Result);
end;



function XMVector4AngleBetweenNormals(N1: TXMVECTOR; N2: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4Dot(N1, N2);
    Result := XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result := XMVectorACos(Result);
end;



function XMVector4AngleBetweenVectors(V1: TXMVECTOR; V2: TXMVECTOR): TXMVECTOR;
var
    L1, L2, Dot, CosAngle: TXMVECTOR;
begin
    L1 := XMVector4ReciprocalLength(V1);
    L2 := XMVector4ReciprocalLength(V2);

    Dot := XMVector4Dot(V1, V2);

    L1 := XMVectorMultiply(L1, L2);

    CosAngle := XMVectorMultiply(Dot, L1);
    CosAngle := XMVectorClamp(CosAngle, g_XMNegativeOne.v, g_XMOne.v);

    Result := XMVectorACos(CosAngle);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
begin
    Result.f32[0] := (M.m[0, 0] * V.f32[0]) + (M.m[1, 0] * V.f32[1]) + (M.m[2, 0] * V.f32[2]) + (M.m[3, 0] * V.f32[3]);
    Result.f32[1] := (M.m[0, 1] * V.f32[0]) + (M.m[1, 1] * V.f32[1]) + (M.m[2, 1] * V.f32[2]) + (M.m[3, 1] * V.f32[3]);
    Result.f32[2] := (M.m[0, 2] * V.f32[0]) + (M.m[1, 2] * V.f32[1]) + (M.m[2, 2] * V.f32[2]) + (M.m[3, 2] * V.f32[3]);
    Result.f32[3] := (M.m[0, 3] * V.f32[0]) + (M.m[1, 3] * V.f32[1]) + (M.m[2, 3] * V.f32[2]) + (M.m[3, 3] * V.f32[3]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR;
begin
    (* ToDo
   float32x2_t VL = vget_low_f32( V );
    XMVECTOR vResult = vmulq_lane_f32( M.r[0], VL, 0 ); // X
    vResult = vmlaq_lane_f32( vResult, M.r[1], VL, 1 ); // Y
    float32x2_t VH = vget_high_f32( V );
    vResult = vmlaq_lane_f32( vResult, M.r[2], VH, 0  ); // Z
    return vmlaq_lane_f32( vResult, M.r[3], VH, 1 ); // W
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4Transform(constref V: TXMVECTOR; constref M: TXMMATRIX): TXMVECTOR; assembler;
asm
           // Splat x,y,z and w
           // vTempX = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0, [V]
           MOVUPS  XMM1, XMM0
           MOVUPS  XMM2, XMM0
           MOVUPS  XMM3, XMM0
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           //  vTempY = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vTempZ = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // vTempW = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // Mul by the matrix
           // vTempX = _mm_mul_ps(vTempX,M.r[0]);
           MULPS   XMM0, TXMMATRIX([M]).r0
           // vTempY = _mm_mul_ps(vTempY,M.r[1]);
           MULPS   XMM1, TXMMATRIX([M]).r1
           // vTempZ = _mm_mul_ps(vTempZ,M.r[2]);
           MULPS   XMM2, TXMMATRIX([M]).r2
           // vTempW = _mm_mul_ps(vTempW,M.r[3]);
           MULPS   XMM3, TXMMATRIX([M]).r3
           // Add them all together
           // vTempX = _mm_add_ps(vTempX,vTempY);
           ADDPS   XMM0, XMM1
           // vTempZ = _mm_add_ps(vTempZ,vTempW);
           ADDPS   XMM2, XMM3
           // vTempX = _mm_add_ps(vTempX,vTempZ);
           ADDPS   XMM0, XMM2
           // return vTempX;
           MOVUPS  [result],XMM0
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVector4TransformStream(out pOutputStream: PXMFLOAT4; OutputStride: size_t; const pInputStream: PXMFLOAT4; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT4;
var
    row0, row1, row2, row3: TXMVECTOR;
    i: size_t;
    vResult, V, W, Z, Y, X: TXMVECTOR;
    pInputVector: array of TXMFloat4 absolute pInputStream;
    pOutputVector: array of TXMFloat4;
begin
    assert(pOutputStream <> nil);
    assert(pInputStream <> nil);
    assert(InputStride >= sizeof(TXMFLOAT4));
    assert(OutputStride >= sizeof(TXMFLOAT4));

    SetLength(pOutputVector, VectorCount);

    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];
    row3 := M.r[3];

    for  i := 0 to VectorCount - 1 do
    begin
        V := XMLoadFloat4(pInputVector[i]);
        W := XMVectorSplatW(V);
        Z := XMVectorSplatZ(V);
        Y := XMVectorSplatY(V);
        X := XMVectorSplatX(V);

        vResult := XMVectorMultiply(W, row3);
        vResult := XMVectorMultiplyAdd(Z, row2, vResult);
        vResult := XMVectorMultiplyAdd(Y, row1, vResult);
        vResult := XMVectorMultiplyAdd(X, row0, vResult);
        XMStoreFloat4(pOutputVector[i], vResult);
    end;
    pOutputStream := @pOutputVector;
    Result := @pOutputVector;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMVector4TransformStream(out pOutputStream: PXMFLOAT4; OutputStride: size_t; const pInputStream: PXMFLOAT4; InputStride: size_t; VectorCount: size_t; M: TXMMATRIX): PXMFLOAT4;
begin
    (* ToDo
     const uint8_t* pInputVector = (const uint8_t* )pInputStream;
    uint8_t* pOutputVector = (uint8_t* )pOutputStream;

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if ((InputStride == sizeof(XMFLOAT4)) && (OutputStride == sizeof(XMFLOAT4)))
        {
            for (size_t j = 0; j < four; ++j)
            {
                float32x4x4_t V = vld4q_f32( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += sizeof(XMFLOAT4)*4;

                float32x2_t r = vget_low_f32( row0 );
                XMVECTOR vResult0 = vmulq_lane_f32( V.val[0], r, 0 ); // Ax
                XMVECTOR vResult1 = vmulq_lane_f32( V.val[0], r, 1 ); // Bx

                __prefetch( pInputVector );

                r = vget_high_f32( row0 );
                XMVECTOR vResult2 = vmulq_lane_f32( V.val[0], r, 0 ); // Cx
                XMVECTOR vResult3 = vmulq_lane_f32( V.val[0], r, 1 ); // Dx

                __prefetch( pInputVector+XM_CACHE_LINE_SIZE );

                r = vget_low_f32( row1 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[1], r, 0 ); // Ax+Ey
                vResult1 = vmlaq_lane_f32( vResult1, V.val[1], r, 1 ); // Bx+Fy

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*2) );

                r = vget_high_f32( row1 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[1], r, 0 ); // Cx+Gy
                vResult3 = vmlaq_lane_f32( vResult3, V.val[1], r, 1 ); // Dx+Hy

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*3) );

                r = vget_low_f32( row2 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[2], r, 0 ); // Ax+Ey+Iz
                vResult1 = vmlaq_lane_f32( vResult1, V.val[2], r, 1 ); // Bx+Fy+Jz

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*4) );

                r = vget_high_f32( row2 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[2], r, 0 ); // Cx+Gy+Kz
                vResult3 = vmlaq_lane_f32( vResult3, V.val[2], r, 1 ); // Dx+Hy+Lz

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*5) );

                r = vget_low_f32( row3 );
                vResult0 = vmlaq_lane_f32( vResult0, V.val[3], r, 0 ); // Ax+Ey+Iz+Mw
                vResult1 = vmlaq_lane_f32( vResult1, V.val[3], r, 1 ); // Bx+Fy+Jz+Nw

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*6) );

                r = vget_high_f32( row3 );
                vResult2 = vmlaq_lane_f32( vResult2, V.val[3], r, 0 ); // Cx+Gy+Kz+Ow
                vResult3 = vmlaq_lane_f32( vResult3, V.val[3], r, 1 ); // Dx+Hy+Lz+Pw

                __prefetch( pInputVector+(XM_CACHE_LINE_SIZE*7) );

                V.val[0] = vResult0;
                V.val[1] = vResult1;
                V.val[2] = vResult2;
                V.val[3] = vResult3;

                vst4q_f32( reinterpret_cast<float*>(pOutputVector), V );
                pOutputVector += sizeof(XMFLOAT4)*4;

                i += 4;
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        XMVECTOR V = vld1q_f32( reinterpret_cast<const float*>(pInputVector) );
        pInputVector += InputStride;

        float32x2_t VL = vget_low_f32( V );
        XMVECTOR vResult = vmulq_lane_f32( row0, VL, 0 ); // X
        vResult = vmlaq_lane_f32( vResult, row1, VL, 1 ); // Y
        float32x2_t VH = vget_high_f32( V );
        vResult = vmlaq_lane_f32( vResult, row2, VH, 0 ); // Z
        vResult = vmlaq_lane_f32( vResult, row3, VH, 1 ); // W

        vst1q_f32( reinterpret_cast<float*>(pOutputVector), vResult );
        pOutputVector += OutputStride;
    }

    return pOutputStream;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMVector4TransformStream(out pOutputStream: PXMFLOAT4; constref OutputStride: size_t; constref pInputStream: PXMFLOAT4; constref InputStride: size_t; constref VectorCount: size_t; constref M: TXMMATRIX): PXMFLOAT4; inline;
var
    pInputVector, pOutputVector: pointer;
    row0, row1, row2, row3: TXMVECTOR;
    V: TXMVECTOR;
    i: size_t;
begin
    pInputVector := pInputStream;
    pOutputVector := pOutputStream;

    row0 := M.r[0];
    row1 := M.r[1];
    row2 := M.r[2];
    row3 := M.r[3];

    if (not (uintptr(pOutputStream) and $F = $F) and not (OutputStride and $F = $F)) then
    begin
        if (not (uintptr(pInputStream) and $F = $F) and not (InputStride and $F = $F)) then
        begin
            // Aligned input, aligned output
            for  i := 0 to VectorCount - 1 do
            begin
                asm
                           //  V = _mm_load_ps( reinterpret_cast<const float*>(pInputVector) );
                           MOVAPS  XMM0, [pInputVector]
                           // pInputVector += InputStride;
                           MOV     EDX, pInputVector
                           ADD     EDX, [InputStride]
                           MOV     pInputVector,EDX

                           //  vTempX = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));

                           MOVUPS  XMM1,XMM0
                           MOVUPS  XMM2,XMM0
                           MOVUPS  XMM3,XMM0
                           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
                           //  vTempY = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                           //  vTempZ = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
                           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                           //  vTempW = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
                           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                           // vTempX = _mm_mul_ps(vTempX,row0);
                           MULPS   XMM0, [row0]
                           // vTempY = _mm_mul_ps(vTempY,row1);
                           MULPS   XMM1, [row1]
                           // vTempZ = _mm_mul_ps(vTempZ,row2);
                           MULPS   XMM2, [row2]
                           // vTempW = _mm_mul_ps(vTempW,row3);
                           MULPS   XMM3, [row3]

                           // vTempX = _mm_add_ps(vTempX,vTempY);
                           ADDPS   XMM0, XMM1
                           // vTempZ = _mm_add_ps(vTempZ,vTempW);
                           ADDPS   XMM2, XMM3
                           // vTempX = _mm_add_ps(vTempX,vTempZ);
                           ADDPS   XMM0, XMM2

                           // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTempX );
                           MOVUPS  [pOutputVector], XMM0
                           // pOutputVector += OutputStride;
                           MOV     EDX, pOutputVector
                           ADD     EDX, [OutputStride]
                           MOV     pOutputVector,EDX
                end;
            end;
        end
        else
        begin
            // Unaligned input, aligned output
            for i := 0 to VectorCount - 1 do
            begin
                asm
                           // V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                           MOVUPS  XMM0, [pInputVector]
                           // pInputVector += InputStride;
                           MOV     EDX, pInputVector
                           ADD     EDX, [InputStride]
                           MOV     pInputVector,EDX

                           //  vTempX = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
                           MOVUPS  XMM1,XMM0
                           MOVUPS  XMM2,XMM0
                           MOVUPS  XMM3,XMM0
                           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
                           //  vTempY = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                           //  vTempZ = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
                           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                           //  vTempW = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
                           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                           // vTempX = _mm_mul_ps(vTempX,row0);
                           MULPS   XMM0, [row0]
                           // vTempY = _mm_mul_ps(vTempY,row1);
                           MULPS   XMM1, [row1]
                           // vTempZ = _mm_mul_ps(vTempZ,row2);
                           MULPS   XMM2, [row2]
                           // vTempW = _mm_mul_ps(vTempW,row3);
                           MULPS   XMM3, [row3]

                           // vTempX = _mm_add_ps(vTempX,vTempY);
                           ADDPS   XMM0, XMM1
                           // vTempZ = _mm_add_ps(vTempZ,vTempW);
                           ADDPS   XMM2, XMM3
                           // vTempX = _mm_add_ps(vTempX,vTempZ);
                           ADDPS   XMM0, XMM2

                           // XM_STREAM_PS( reinterpret_cast<float*>(pOutputVector), vTempX );
                           MOVUPS  [pOutputVector], XMM0
                           // pOutputVector += OutputStride;
                           MOV     EDX, pOutputVector
                           ADD     EDX, [OutputStride]
                           MOV     pOutputVector,EDX
                end;
            end;
        end;
    end
    else
    begin
        if (not (uintptr(pInputStream) and $F = $F) and not (InputStride and $F = $F)) then
        begin
            // Aligned input, unaligned output
            for i := 0 to VectorCount - 1 do
            begin
                asm
                           //  V = _mm_load_ps( reinterpret_cast<const float*>(pInputVector) );
                           MOVAPS  XMM0, [pInputVector]
                           // pInputVector += InputStride;
                           MOV     EDX, pInputVector
                           ADD     EDX, [InputStride]
                           MOV     pInputVector,EDX

                           //  vTempX = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
                           MOVUPS  XMM1,XMM0
                           MOVUPS  XMM2,XMM0
                           MOVUPS  XMM3,XMM0
                           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
                           //  vTempY = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                           //  vTempZ = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
                           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                           //  vTempW = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
                           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                           // vTempX = _mm_mul_ps(vTempX,row0);
                           MULPS   XMM0, [row0]
                           // vTempY = _mm_mul_ps(vTempY,row1);
                           MULPS   XMM1, [row1]
                           // vTempZ = _mm_mul_ps(vTempZ,row2);
                           MULPS   XMM2, [row2]
                           // vTempW = _mm_mul_ps(vTempW,row3);
                           MULPS   XMM3, [row3]

                           // vTempX = _mm_add_ps(vTempX,vTempY);
                           ADDPS   XMM0, XMM1
                           // vTempZ = _mm_add_ps(vTempZ,vTempW);
                           ADDPS   XMM2, XMM3
                           // vTempX = _mm_add_ps(vTempX,vTempZ);
                           ADDPS   XMM0, XMM2

                           // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTempX );
                           MOVUPS  [pOutputVector], XMM0
                           //  pOutputVector += OutputStride;
                           MOV     EDX, pOutputVector
                           ADD     EDX, [OutputStride]
                           MOV     pOutputVector,EDX
                end;
            end;
        end
        else
        begin
            // Unaligned input, unaligned output
            for i := 0 to VectorCount - 1 do
            begin
                asm
                           // V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                           MOVUPS  XMM0, [pInputVector]
                           // pInputVector += InputStride;
                           MOV     EDX, pInputVector
                           ADD     EDX, [InputStride]
                           MOV     pInputVector,EDX


                           //  vTempX = XM_PERMUTE_PS(V,_MM_SHUFFLE(0,0,0,0));
                           MOVUPS  XMM1,XMM0
                           MOVUPS  XMM2,XMM0
                           MOVUPS  XMM3,XMM0
                           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
                           //  vTempY = XM_PERMUTE_PS(V,_MM_SHUFFLE(1,1,1,1));
                           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
                           //  vTempZ = XM_PERMUTE_PS(V,_MM_SHUFFLE(2,2,2,2));
                           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
                           //  vTempW = XM_PERMUTE_PS(V,_MM_SHUFFLE(3,3,3,3));
                           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3

                           // vTempX = _mm_mul_ps(vTempX,row0);
                           MULPS   XMM0, [row0]
                           // vTempY = _mm_mul_ps(vTempY,row1);
                           MULPS   XMM1, [row1]
                           // vTempZ = _mm_mul_ps(vTempZ,row2);
                           MULPS   XMM2, [row2]
                           // vTempW = _mm_mul_ps(vTempW,row3);
                           MULPS   XMM3, [row3]

                           // vTempX = _mm_add_ps(vTempX,vTempY);
                           ADDPS   XMM0, XMM1
                           // vTempZ = _mm_add_ps(vTempZ,vTempW);
                           ADDPS   XMM2, XMM3
                           // vTempX = _mm_add_ps(vTempX,vTempZ);
                           ADDPS   XMM0, XMM2

                           // _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTempX );
                           MOVUPS  [pOutputVector], XMM0
                           // pOutputVector += OutputStride;
                           MOV     EDX, pOutputVector
                           ADD     EDX, [OutputStride]
                           MOV     pOutputVector,EDX
                end;
            end;
        end;
    end;

    // XM_SFENCE();

    Result := pOutputStream;
end;

{$ENDIF}

{***************************************************************************
 *
 * Matrix
 *
 ***************************************************************************}

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

// Return true if any entry in the matrix is NaN


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixIsNaN(constref M: TXMMATRIX): boolean;
var
    i: size_t = 16;
    pWork: uint32;
    uTest: uint32;
begin
    while i > 0 do
    begin
        // Fetch value into integer unit
        uTest := M.u[i - 1];
        // Remove sign
        uTest := uTest and $7FFFFFFF;
        // NaN is 0x7F800001 through 0x7FFFFFFF inclusive
        uTest := uTest - $7F800001;
        if (uTest < $007FFFFF) then
            break;      // NaN found
        Dec(i); // Next entry
    end;
    Result := (i <> 0);      // i == 0 if nothing matched
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixIsNaN(constref M: TXMMATRIX): boolean;
begin
    (* ToDo
      // Load in registers
    XMVECTOR vX = M.r[0];
    XMVECTOR vY = M.r[1];
    XMVECTOR vZ = M.r[2];
    XMVECTOR vW = M.r[3];
    // Test themselves to check for NaN
    vX = vmvnq_u32(vceqq_f32(vX, vX));
    vY = vmvnq_u32(vceqq_f32(vY, vY));
    vZ = vmvnq_u32(vceqq_f32(vZ, vZ));
    vW = vmvnq_u32(vceqq_f32(vW, vW));
    // Or all the results
    vX = vorrq_u32(vX,vZ);
    vY = vorrq_u32(vY,vW);
    vX = vorrq_u32(vX,vY);
    // If any tested true, return true
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vX), vget_high_u8(vX));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    return (r != 0);
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixIsNaN(constref M: TXMMATRIX): boolean; assembler;
asm

           // Load in registers
           MOVUPS  XMM0, TXMVECTOR(TXMMATRIX([M]).r[0]) // vX
           MOVUPS  XMM1, TXMVECTOR(TXMMATRIX([M]).r[1]) // vY
           MOVUPS  XMM2, TXMVECTOR(TXMMATRIX([M]).r[2]) // vZ
           MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([M]).r[3]) // vW
           // Test themselves to check for NaN
           CMPPS   XMM0, XMM0, $4
           CMPPS   XMM1, XMM1, $4
           CMPPS   XMM2, XMM2, $4
           CMPPS   XMM3, XMM3, $4
           // Or all the results
           ORPS    XMM0, XMM2 // _mm_or_ps(vX,vZ);
           ORPS    XMM1, XMM3// vY = _mm_or_ps(vY,vW);
           ORPS    XMM0, XMM1// vX = _mm_or_ps(vX,vY);
           // If any tested true, return true
           // return (_mm_movemask_ps(vX)!=0);
           MOVMSKPS EAX, XMM0
           TEST    EAX,EAX  // set ZF to 1 if eax == 0
           JE      @zero  // jump if ZF == 1 (eax==0)
           mov [result],$1  // return TRUE
           @zero:
           MOV     [result],$0  // return FALSE
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
//------------------------------------------------------------------------------

// Return true if any entry in the matrix is +/-INF
function XMMatrixIsInfinite(M: TXMMATRIX): boolean;
var
    i: size_t = 16;
    uTest: UINT32;
begin
    while i > 0 do
    begin
        // Fetch value into integer unit
        uTest := M.u[i];
        // Remove sign
        uTest := uTest and $7FFFFFFF;
        // INF is $7F800000
        if (uTest = $7F800000) then
            break;      // INF found
        Dec(i);// Next entry
    end;
    Result := (i <> 0);      // i == 0 if nothing matched
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixIsInfinite(M: TXMMATRIX): boolean;
begin
    (* ToDo
    // Mask off the sign bits
    XMVECTOR vTemp1 = vandq_u32(M.r[0],g_XMAbsMask);
    XMVECTOR vTemp2 = vandq_u32(M.r[1],g_XMAbsMask);
    XMVECTOR vTemp3 = vandq_u32(M.r[2],g_XMAbsMask);
    XMVECTOR vTemp4 = vandq_u32(M.r[3],g_XMAbsMask);
    // Compare to infinity
    vTemp1 = vceqq_f32(vTemp1,g_XMInfinity);
    vTemp2 = vceqq_f32(vTemp2,g_XMInfinity);
    vTemp3 = vceqq_f32(vTemp3,g_XMInfinity);
    vTemp4 = vceqq_f32(vTemp4,g_XMInfinity);
    // Or the answers together
    vTemp1 = vorrq_u32(vTemp1,vTemp2);
    vTemp3 = vorrq_u32(vTemp3,vTemp4);
    vTemp1 = vorrq_u32(vTemp1,vTemp3);
    // If any are infinity, the signs are true.
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vTemp1), vget_high_u8(vTemp1));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    return (r != 0);
    *)
end;

{$ELSE}
function XMMatrixIsInfinite(M: TXMMATRIX): boolean; assembler;
asm

           // Mask off the sign bits
           //  vTemp1 = _mm_and_ps(M.r[0],g_XMAbsMask);
           MOVUPS  XMM1, TXMMATRIX([M]).r0
           ANDPS   XMM1, [g_XMAbsMask]
           //  vTemp2 = _mm_and_ps(M.r[1],g_XMAbsMask);
           MOVUPS  XMM2, TXMMATRIX([M]).r1
           ANDPS   XMM2, [g_XMAbsMask]
           //  vTemp3 = _mm_and_ps(M.r[2],g_XMAbsMask);
           MOVUPS  XMM3, TXMMATRIX([M]).r2
           ANDPS   XMM3, [g_XMAbsMask]
           //  vTemp4 = _mm_and_ps(M.r[3],g_XMAbsMask);
           MOVUPS  XMM4, TXMMATRIX([M]).r3
           ANDPS   XMM4, [g_XMAbsMask]
           // Compare to infinity
           // vTemp1 = _mm_cmpeq_ps(vTemp1,g_XMInfinity);
           CMPPS   XMM1, [g_XMInfinity], 0
           // vTemp2 = _mm_cmpeq_ps(vTemp2,g_XMInfinity);
           CMPPS   XMM2, [g_XMInfinity], 0
           // vTemp3 = _mm_cmpeq_ps(vTemp3,g_XMInfinity);
           CMPPS   XMM3, [g_XMInfinity], 0
           // vTemp4 = _mm_cmpeq_ps(vTemp4,g_XMInfinity);
           CMPPS   XMM4, [g_XMInfinity], 0
           // Or the answers together
           // vTemp1 = _mm_or_ps(vTemp1,vTemp2);
           ORPS    XMM1, XMM2
           // vTemp3 = _mm_or_ps(vTemp3,vTemp4);
           ORPS    XMM3, XMM4
           // vTemp1 = _mm_or_ps(vTemp1,vTemp3);
           ORPS    XMM1, XMM3
           // If any are infinity, the signs are true.
           // return (_mm_movemask_ps(vTemp1)!=0);
           MOVMSKPS EAX, XMM1
           CMP     EAX, $0
           SETNE    [result]

end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
//------------------------------------------------------------------------------

// Return true if the XMMatrix is equal to identity
function XMMatrixIsIdentity(M: TXMMATRIX): boolean;
var
    uOne, uZero: UINT32;
begin
    // Use the integer pipeline to reduce branching to a minimum
    // Convert 1.0f to zero and or them together
    uOne := M.u[0] xor $3F800000;
    // Or all the 0.0f entries together
    uZero := M.u[1];
    uZero := uZero or M.u[2];
    uZero := uZero or M.u[3];
    // 2nd row
    uZero := uZero or M.u[4];
    uOne := uOne or (M.u[5] xor $3F800000);
    uZero := uZero or M.u[6];
    uZero := uZero or M.u[7];
    // 3rd row
    uZero := uZero or M.u[8];
    uZero := uZero or M.u[9];
    uOne := uOne or (M.u[10] xor $3F800000);
    uZero := uZero or M.u[11];
    // 4th row
    uZero := uZero or M.u[12];
    uZero := uZero or M.u[13];
    uZero := uZero or M.u[14];
    uOne := uOne or (M.u[15] xor $3F800000);
    // If all zero entries are zero, the uZero==0
    uZero := uZero and $7FFFFFFF;    // Allow -0.0f
    // If all 1.0f entries are 1.0f, then uOne==0
    uOne := uOne or uZero;
    Result := (uOne = 0);
end;

{$elseIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixIsIdentity(M: TXMMATRIX): boolean;
begin
    (* ToDo
    XMVECTOR vTemp1 = vceqq_f32(M.r[0],g_XMIdentityR0);
    XMVECTOR vTemp2 = vceqq_f32(M.r[1],g_XMIdentityR1);
    XMVECTOR vTemp3 = vceqq_f32(M.r[2],g_XMIdentityR2);
    XMVECTOR vTemp4 = vceqq_f32(M.r[3],g_XMIdentityR3);
    vTemp1 = vandq_u32(vTemp1,vTemp2);
    vTemp3 = vandq_u32(vTemp3,vTemp4);
    vTemp1 = vandq_u32(vTemp1,vTemp3);
    int8x8x2_t vTemp = vzip_u8(vget_low_u8(vTemp1), vget_high_u8(vTemp1));
    vTemp = vzip_u16(vTemp.val[0], vTemp.val[1]);
    uint32_t r = vget_lane_u32(vTemp.val[1], 1);
    return ( r == 0xFFFFFFFFU );
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixIsIdentity(M: TXMMATRIX): boolean; assembler;
asm
           // vTemp1 = _mm_cmpeq_ps(M.r[0],g_XMIdentityR0);
           MOVUPS  XMM1,  TXMMATRIX([M]).r0
           CMPPS   XMM1, [g_XMIdentityR0], 0
           //  vTemp2 = _mm_cmpeq_ps(M.r[1],g_XMIdentityR1);
           MOVUPS  XMM2,  TXMMATRIX([M]).r1
           CMPPS   XMM2, [g_XMIdentityR1], 0
           //  vTemp3 = _mm_cmpeq_ps(M.r[2],g_XMIdentityR2);
           MOVUPS  XMM3,  TXMMATRIX([M]).r2
           CMPPS   XMM3, [g_XMIdentityR2], 0
           //  vTemp4 = _mm_cmpeq_ps(M.r[3],g_XMIdentityR3);
           MOVUPS  XMM4,  TXMMATRIX([M]).r3
           CMPPS   XMM4, [g_XMIdentityR3], 0
           // vTemp1 = _mm_and_ps(vTemp1,vTemp2);
           ANDPS   XMM1, XMM2
           // vTemp3 = _mm_and_ps(vTemp3,vTemp4);
           ANDPS   XMM3, XMM4
           // vTemp1 = _mm_and_ps(vTemp1,vTemp3);
           ANDPS   XMM1, XMM3
           // return (_mm_movemask_ps(vTemp1)==0x0f);
           MOVMSKPS EAX, XMM0
           AND     EAX, $F
           CMP     EAX, $F
           SETE    [result]

end;

{$ENDIF}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------
// Perform a 4x4 matrix multiply by a 4x4 matrix

{$IF DEFINED(_XM_NO_INTRINSICS_)}

function XMMatrixMultiply(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX;
var
    x, y, z, w: single;
begin
    // Cache the invariants in registers
    x := M1.m[0, 0];
    y := M1.m[0, 1];
    z := M1.m[0, 2];
    w := M1.m[0, 3];
    // Perform the operation on the first row
    Result.m[0, 0] := (M2.m[0, 0] * x) + (M2.m[1, 0] * y) + (M2.m[2, 0] * z) + (M2.m[3, 0] * w);
    Result.m[0, 1] := (M2.m[0, 1] * x) + (M2.m[1, 1] * y) + (M2.m[2, 1] * z) + (M2.m[3, 1] * w);
    Result.m[0, 2] := (M2.m[0, 2] * x) + (M2.m[1, 2] * y) + (M2.m[2, 2] * z) + (M2.m[3, 2] * w);
    Result.m[0, 3] := (M2.m[0, 3] * x) + (M2.m[1, 3] * y) + (M2.m[2, 3] * z) + (M2.m[3, 3] * w);
    // Repeat for all the other rows
    x := M1.m[1, 0];
    y := M1.m[1, 1];
    z := M1.m[1, 2];
    w := M1.m[1, 3];
    Result.m[1, 0] := (M2.m[0, 0] * x) + (M2.m[1, 0] * y) + (M2.m[2, 0] * z) + (M2.m[3, 0] * w);
    Result.m[1, 1] := (M2.m[0, 1] * x) + (M2.m[1, 1] * y) + (M2.m[2, 1] * z) + (M2.m[3, 1] * w);
    Result.m[1, 2] := (M2.m[0, 2] * x) + (M2.m[1, 2] * y) + (M2.m[2, 2] * z) + (M2.m[3, 2] * w);
    Result.m[1, 3] := (M2.m[0, 3] * x) + (M2.m[1, 3] * y) + (M2.m[2, 3] * z) + (M2.m[3, 3] * w);
    x := M1.m[2, 0];
    y := M1.m[2, 1];
    z := M1.m[2, 2];
    w := M1.m[2, 3];
    Result.m[2, 0] := (M2.m[0, 0] * x) + (M2.m[1, 0] * y) + (M2.m[2, 0] * z) + (M2.m[3, 0] * w);
    Result.m[2, 1] := (M2.m[0, 1] * x) + (M2.m[1, 1] * y) + (M2.m[2, 1] * z) + (M2.m[3, 1] * w);
    Result.m[2, 2] := (M2.m[0, 2] * x) + (M2.m[1, 2] * y) + (M2.m[2, 2] * z) + (M2.m[3, 2] * w);
    Result.m[2, 3] := (M2.m[0, 3] * x) + (M2.m[1, 3] * y) + (M2.m[2, 3] * z) + (M2.m[3, 3] * w);
    x := M1.m[3, 0];
    y := M1.m[3, 1];
    z := M1.m[3, 2];
    w := M1.m[3, 3];
    Result.m[3, 0] := (M2.m[0, 0] * x) + (M2.m[1, 0] * y) + (M2.m[2, 0] * z) + (M2.m[3, 0] * w);
    Result.m[3, 1] := (M2.m[0, 1] * x) + (M2.m[1, 1] * y) + (M2.m[2, 1] * z) + (M2.m[3, 1] * w);
    Result.m[3, 2] := (M2.m[0, 2] * x) + (M2.m[1, 2] * y) + (M2.m[2, 2] * z) + (M2.m[3, 2] * w);
    Result.m[3, 3] := (M2.m[0, 3] * x) + (M2.m[1, 3] * y) + (M2.m[2, 3] * z) + (M2.m[3, 3] * w);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixMultiply(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX;
begin
    (* ToDo
     XMMATRIX mResult;
    float32x2_t VL = vget_low_f32( M1.r[0] );
    float32x2_t VH = vget_high_f32( M1.r[0] );
    // Perform the operation on the first row
    XMVECTOR vX = vmulq_lane_f32(M2.r[0], VL, 0);
    XMVECTOR vY = vmulq_lane_f32(M2.r[1], VL, 1);
    XMVECTOR vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    XMVECTOR vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    mResult.r[0] = vaddq_f32( vZ, vW );
    // Repeat for the other 3 rows
    VL = vget_low_f32( M1.r[1] );
    VH = vget_high_f32( M1.r[1] );
    vX = vmulq_lane_f32(M2.r[0], VL, 0);
    vY = vmulq_lane_f32(M2.r[1], VL, 1);
    vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    mResult.r[1] = vaddq_f32( vZ, vW );
    VL = vget_low_f32( M1.r[2] );
    VH = vget_high_f32( M1.r[2] );
    vX = vmulq_lane_f32(M2.r[0], VL, 0);
    vY = vmulq_lane_f32(M2.r[1], VL, 1);
    vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    mResult.r[2] = vaddq_f32( vZ, vW );
    VL = vget_low_f32( M1.r[3] );
    VH = vget_high_f32( M1.r[3] );
    vX = vmulq_lane_f32(M2.r[0], VL, 0);
    vY = vmulq_lane_f32(M2.r[1], VL, 1);
    vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    mResult.r[3] = vaddq_f32( vZ, vW );
    return mResult;
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMMatrixMultiply(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX; assembler;
asm
           // Load the M2 to XMM4 to XMM7, a littel bit different than on MSDN, but more efficiency
           MOVUPS  XMM4, TXMVECTOR(TXMMATRIX([M2]).r[0]) // vX
           MOVUPS  XMM5, TXMVECTOR(TXMMATRIX([M2]).r[1]) // vY
           MOVUPS  XMM6, TXMVECTOR(TXMMATRIX([M2]).r[2]) // vZ
           MOVUPS  XMM7, TXMVECTOR(TXMMATRIX([M2]).r[3]) // vW

           // Splat the component X,Y,Z then W
           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[3] // vW
           // Perform the operation on the first row
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           // Perform a binary add to reduce cumulative errors
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMMATRIX([Result]).r[0], XMM0   //  mResult.r[0] = vX;
           // Repeat for the other 3 rows
           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[3] // vW
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMMATRIX([Result]).r[1], XMM0   //  mResult.r[1] = vX;

           // Repeat for the other 2 rows
           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[3] // vW
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMMATRIX([Result]).r[2], XMM0   //  mResult.r[2] = vX;
           // Repeat for the last row
           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[3] // vW
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMMATRIX([Result]).r[3], XMM0   //  mResult.r[2] = vX;
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixMultiply(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX; assembler;
asm
           // Splat the component X,Y,Z then W
           // Use vW to hold the original row
           // vW = M1.r[0];
           MOVUPS  XMM3, TXMMATRIX([M1]).r0
           //  vX = XM_PERMUTE_PS(vW,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0,XMM3
           MOVUPS  XMM1,XMM3
           MOVUPS  XMM2,XMM3

           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           //  vY = XM_PERMUTE_PS(vW,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           //  vZ = XM_PERMUTE_PS(vW,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // vW = XM_PERMUTE_PS(vW,_MM_SHUFFLE(3,3,3,3));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // Perform the operation on the first row
           // vX = _mm_mul_ps(vX,M2.r[0]);
           MULPS   XMM0, TXMMATRIX([M2]).r0
           // vY = _mm_mul_ps(vY,M2.r[1]);
           MULPS   XMM1, TXMMATRIX([M2]).r1
           // vZ = _mm_mul_ps(vZ,M2.r[2]);
           MULPS   XMM2, TXMMATRIX([M2]).r2
           // vW = _mm_mul_ps(vW,M2.r[3]);
           MULPS   XMM3, TXMMATRIX([M2]).r3
           // Perform a binary add to reduce cumulative errors
           // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM0, XMM2
           // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM1, XMM3
           // vX = _mm_add_ps(vX,vY);
           ADDPS   XMM0, XMM1
           // mResult.r[0] = vX;
           MOVUPS  TXMMATRIX([result]).r0, XMM0
           // Repeat for the other 3 rows
           //  vW = M1.r[1];
           MOVUPS  XMM3, TXMMATRIX([M1]).r1
           // vX = XM_PERMUTE_PS(vW,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0,XMM3
           MOVUPS  XMM1,XMM3
           MOVUPS  XMM2,XMM3
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vY = XM_PERMUTE_PS(vW,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vZ = XM_PERMUTE_PS(vW,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // vW = XM_PERMUTE_PS(vW,_MM_SHUFFLE(3,3,3,3));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // vX = _mm_mul_ps(vX,M2.r[0]);
           MULPS   XMM0, TXMMATRIX([M2]).r0
           // vY = _mm_mul_ps(vY,M2.r[1]);
           MULPS   XMM1, TXMMATRIX([M2]).r1
           // vZ = _mm_mul_ps(vZ,M2.r[2]);
           MULPS   XMM2, TXMMATRIX([M2]).r2
           // vW = _mm_mul_ps(vW,M2.r[3]);
           MULPS   XMM3, TXMMATRIX([M2]).r3
           // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM0, XMM2
           // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM1, XMM3
           // vX = _mm_add_ps(vX,vY);
           ADDPS   XMM0, XMM1
           // mResult.r[1] = vX;
           MOVUPS  TXMMATRIX([result]).r1, XMM0
           // vW = M1.r[2];
           MOVUPS  XMM3, TXMMATRIX([M1]).r2
           // vX = XM_PERMUTE_PS(vW,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0,XMM3
           MOVUPS  XMM1,XMM3
           MOVUPS  XMM2,XMM3
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vY = XM_PERMUTE_PS(vW,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vZ = XM_PERMUTE_PS(vW,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // vW = XM_PERMUTE_PS(vW,_MM_SHUFFLE(3,3,3,3));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // vX = _mm_mul_ps(vX,M2.r[0]);
           MULPS   XMM0, TXMMATRIX([M2]).r0
           // vY = _mm_mul_ps(vY,M2.r[1]);
           MULPS   XMM1, TXMMATRIX([M2]).r1
           // vZ = _mm_mul_ps(vZ,M2.r[2]);
           MULPS   XMM2, TXMMATRIX([M2]).r2
           // vW = _mm_mul_ps(vW,M2.r[3]);
           MULPS   XMM3, TXMMATRIX([M2]).r3
           // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM0, XMM2
           // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM1, XMM3
           // vX = _mm_add_ps(vX,vY);
           ADDPS   XMM0, XMM1
           // mResult.r[2] = vX;
           MOVUPS  TXMMATRIX([result]).r2, XMM0
           // vW = M1.r[3];
           MOVUPS  XMM3, TXMMATRIX([M1]).r3
           // vX = XM_PERMUTE_PS(vW,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM0,XMM3
           MOVUPS  XMM1,XMM3
           MOVUPS  XMM2,XMM3
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // vY = XM_PERMUTE_PS(vW,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           // vZ = XM_PERMUTE_PS(vW,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           // vW = XM_PERMUTE_PS(vW,_MM_SHUFFLE(3,3,3,3));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           // vX = _mm_mul_ps(vX,M2.r[0]);
           MULPS   XMM0, TXMMATRIX([M2]).r0
           // vY = _mm_mul_ps(vY,M2.r[1]);
           MULPS   XMM1, TXMMATRIX([M2]).r1
           // vZ = _mm_mul_ps(vZ,M2.r[2]);
           MULPS   XMM2, TXMMATRIX([M2]).r2
           // vW = _mm_mul_ps(vW,M2.r[3]);
           MULPS   XMM3, TXMMATRIX([M2]).r3
           // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM0, XMM2
           // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM1, XMM3
           // vX = _mm_add_ps(vX,vY);
           ADDPS   XMM0, XMM1
           // mResult.r[3] = vX;
           MOVUPS  TXMMATRIX([result]).r3, XMM0
           // return mResult;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}

//------------------------------------------------------------------------------
function XMMatrixMultiplyTranspose(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX;
var
    x, y, z, w: single;
begin
    // Cache the invariants in registers
    x := M2.m[0, 0];
    y := M2.m[1, 0];
    z := M2.m[2, 0];
    w := M2.m[3, 0];
    // Perform the operation on the first row
    Result.m[0, 0] := (M1.m[0, 0] * x) + (M1.m[0, 1] * y) + (M1.m[0, 2] * z) + (M1.m[0, 3] * w);
    Result.m[0, 1] := (M1.m[1, 0] * x) + (M1.m[1, 1] * y) + (M1.m[1, 2] * z) + (M1.m[1, 3] * w);
    Result.m[0, 2] := (M1.m[2, 0] * x) + (M1.m[2, 1] * y) + (M1.m[2, 2] * z) + (M1.m[2, 3] * w);
    Result.m[0, 3] := (M1.m[3, 0] * x) + (M1.m[3, 1] * y) + (M1.m[3, 2] * z) + (M1.m[3, 3] * w);
    // Repeat for all the other rows
    x := M2.m[0, 1];
    y := M2.m[1, 1];
    z := M2.m[2, 1];
    w := M2.m[3, 1];
    Result.m[1, 0] := (M1.m[0, 0] * x) + (M1.m[0, 1] * y) + (M1.m[0, 2] * z) + (M1.m[0, 3] * w);
    Result.m[1, 1] := (M1.m[1, 0] * x) + (M1.m[1, 1] * y) + (M1.m[1, 2] * z) + (M1.m[1, 3] * w);
    Result.m[1, 2] := (M1.m[2, 0] * x) + (M1.m[2, 1] * y) + (M1.m[2, 2] * z) + (M1.m[2, 3] * w);
    Result.m[1, 3] := (M1.m[3, 0] * x) + (M1.m[3, 1] * y) + (M1.m[3, 2] * z) + (M1.m[3, 3] * w);
    x := M2.m[0, 2];
    y := M2.m[1, 2];
    z := M2.m[2, 2];
    w := M2.m[3, 2];
    Result.m[2, 0] := (M1.m[0, 0] * x) + (M1.m[0, 1] * y) + (M1.m[0, 2] * z) + (M1.m[0, 3] * w);
    Result.m[2, 1] := (M1.m[1, 0] * x) + (M1.m[1, 1] * y) + (M1.m[1, 2] * z) + (M1.m[1, 3] * w);
    Result.m[2, 2] := (M1.m[2, 0] * x) + (M1.m[2, 1] * y) + (M1.m[2, 2] * z) + (M1.m[2, 3] * w);
    Result.m[2, 3] := (M1.m[3, 0] * x) + (M1.m[3, 1] * y) + (M1.m[3, 2] * z) + (M1.m[3, 3] * w);
    x := M2.m[0, 3];
    y := M2.m[1, 3];
    z := M2.m[2, 3];
    w := M2.m[3, 3];
    Result.m[3, 0] := (M1.m[0, 0] * x) + (M1.m[0, 1] * y) + (M1.m[0, 2] * z) + (M1.m[0, 3] * w);
    Result.m[3, 1] := (M1.m[1, 0] * x) + (M1.m[1, 1] * y) + (M1.m[1, 2] * z) + (M1.m[1, 3] * w);
    Result.m[3, 2] := (M1.m[2, 0] * x) + (M1.m[2, 1] * y) + (M1.m[2, 2] * z) + (M1.m[2, 3] * w);
    Result.m[3, 3] := (M1.m[3, 0] * x) + (M1.m[3, 1] * y) + (M1.m[3, 2] * z) + (M1.m[3, 3] * w);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixMultiplyTranspose(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX;
begin
    (* ToDo
      float32x2_t VL = vget_low_f32( M1.r[0] );
    float32x2_t VH = vget_high_f32( M1.r[0] );
    // Perform the operation on the first row
    XMVECTOR vX = vmulq_lane_f32(M2.r[0], VL, 0);
    XMVECTOR vY = vmulq_lane_f32(M2.r[1], VL, 1);
    XMVECTOR vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    XMVECTOR vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    float32x4_t r0 = vaddq_f32( vZ, vW );
    // Repeat for the other 3 rows
    VL = vget_low_f32( M1.r[1] );
    VH = vget_high_f32( M1.r[1] );
    vX = vmulq_lane_f32(M2.r[0], VL, 0);
    vY = vmulq_lane_f32(M2.r[1], VL, 1);
    vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    float32x4_t r1 = vaddq_f32( vZ, vW );
    VL = vget_low_f32( M1.r[2] );
    VH = vget_high_f32( M1.r[2] );
    vX = vmulq_lane_f32(M2.r[0], VL, 0);
    vY = vmulq_lane_f32(M2.r[1], VL, 1);
    vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    float32x4_t r2 = vaddq_f32( vZ, vW );
    VL = vget_low_f32( M1.r[3] );
    VH = vget_high_f32( M1.r[3] );
    vX = vmulq_lane_f32(M2.r[0], VL, 0);
    vY = vmulq_lane_f32(M2.r[1], VL, 1);
    vZ = vmlaq_lane_f32(vX, M2.r[2], VH, 0);
    vW = vmlaq_lane_f32(vY, M2.r[3], VH, 1);
    float32x4_t r3 = vaddq_f32( vZ, vW );

    // Transpose result
    float32x4x2_t P0 = vzipq_f32( r0, r2 );
    float32x4x2_t P1 = vzipq_f32( r1, r3 );

    float32x4x2_t T0 = vzipq_f32( P0.val[0], P1.val[0] );
    float32x4x2_t T1 = vzipq_f32( P0.val[1], P1.val[1] );

    XMMATRIX mResult;
    mResult.r[0] = T0.val[0];
    mResult.r[1] = T0.val[1];
    mResult.r[2] = T1.val[0];
    mResult.r[3] = T1.val[1];
    return mResult;
    *)
end;

{$ELSEIF DEFINED(_XM_AVX_INTRINSICS_)}
function XMMatrixMultiplyTranspose(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX; assembler;
var
    r0, r1, r2, r3: TXMVECTOR;
asm
           // Load the M2 to XMM4 to XMM7, a littel bit different than on MSDN, but more efficiency
           MOVUPS  XMM4, TXMVECTOR(TXMMATRIX([M2]).r[0]) // vX
           MOVUPS  XMM5, TXMVECTOR(TXMMATRIX([M2]).r[1]) // vY
           MOVUPS  XMM6, TXMVECTOR(TXMMATRIX([M2]).r[2]) // vZ
           MOVUPS  XMM7, TXMVECTOR(TXMMATRIX([M2]).r[3]) // vW


           // Splat the component X,Y,Z then W
           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[0]).f32[3] // vW
           // Perform the operation on the first row
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           // Perform a binary add to reduce cumulative errors
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMMATRIX([r0]), XMM0   //  r0 = vX;
           // Repeat for the other 3 rows
           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[1]).f32[3] // vW
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMVECTOR([r1]), XMM0   //  r1 = vX;

           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[2]).f32[3] // vW
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMVECTOR([r2]), XMM0   //  r1 = vX;

           VBROADCASTSS XMM0, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[0] // vX
           VBROADCASTSS XMM1, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[1] // vY
           VBROADCASTSS XMM2, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[2] // vZ
           VBROADCASTSS XMM3, TXMVECTOR(TXMMATRIX([M1]).r[3]).f32[3] // vW
           MULPS   XMM0, XMM4 // vX
           MULPS   XMM1, XMM5 // vY
           MULPS   XMM2, XMM6 // vZ
           MULPS   XMM3, XMM7 // vW
           ADDPS   XMM0, XMM2  // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3  // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1  // vX = _mm_add_ps(vX,vY);
           MOVUPS  TXMVECTOR([r3]), XMM0   //  r1 = vX;

           MOVUPS  XMM0,TXMVECTOR([r0])
           MOVUPS  XMM1,TXMVECTOR([r1])
           MOVUPS  XMM2,TXMVECTOR([r2])
           MOVUPS  XMM3,TXMVECTOR([r3])

           // x.x,x.y,y.x,y.y
           VSHUFPS XMM4, XMM0, XMM1, _MM_SHUFFLE_1_0_1_0 // vTemp1 = _mm_shuffle_ps(r0,r1,_MM_SHUFFLE(1,0,1,0));
           // x.z,x.w,y.z,y.w
           VSHUFPS XMM5, XMM0, XMM1, _MM_SHUFFLE_3_2_3_2 // vTemp3 = _mm_shuffle_ps(r0,r1,_MM_SHUFFLE(3,2,3,2));
           // z.x,z.y,w.x,w.y
           VSHUFPS XMM6, XMM2, XMM3, _MM_SHUFFLE_1_0_1_0 // vTemp2 = _mm_shuffle_ps(r2,r3,_MM_SHUFFLE(1,0,1,0));
           // z.z,z.w,w.z,w.w
           VSHUFPS XMM7, XMM2, XMM3, _MM_SHUFFLE_3_2_3_2 // vTemp4 = _mm_shuffle_ps(r2,r3,_MM_SHUFFLE(3,2,3,2));


           // x.x,y.x,z.x,w.x
           VSHUFPS XMM0, XMM4, XMM5, _MM_SHUFFLE_2_0_2_0 // mResult.r[0] = _mm_shuffle_ps(vTemp1, vTemp2,_MM_SHUFFLE(2,0,2,0));
           // x.y,y.y,z.y,w.y
           VSHUFPS XMM1, XMM4, XMM5, _MM_SHUFFLE_3_1_3_1 // mResult.r[1] = _mm_shuffle_ps(vTemp1, vTemp2,_MM_SHUFFLE(3,1,3,1));
           // x.z,y.z,z.z,w.z
           VSHUFPS XMM2, XMM6, XMM7, _MM_SHUFFLE_2_0_2_0 // mResult.r[2] = _mm_shuffle_ps(vTemp3, vTemp4,_MM_SHUFFLE(2,0,2,0));
           // x.w,y.w,z.w,w.w
           VSHUFPS XMM3, XMM6, XMM7, _MM_SHUFFLE_3_1_3_1 // mResult.r[3] = _mm_shuffle_ps(vTemp3, vTemp4,_MM_SHUFFLE(3,1,3,1));

           // return Result;
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]) ,XMM0
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]) ,XMM1
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]) ,XMM2
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]) ,XMM3
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixMultiplyTranspose(M1: TXMMATRIX; M2: TXMMATRIX): TXMMATRIX; assembler;
var
    r0, r1, r2, r3: TXMVECTOR;
asm
           // Splat the component X,Y,Z then W
           // Use vW to hold the original row
           MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([M1]).r[0]) // vW = M1.r[0];
           MOVUPS  XMM0, XMM3
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0 // vX = XM_PERMUTE_PS(vW,_MM_SHUFFLE(0,0,0,0));
           MOVUPS  XMM1, XMM3
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1 //  vY = XM_PERMUTE_PS(vW,_MM_SHUFFLE(1,1,1,1));
           MOVUPS  XMM2, XMM3
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2 //  vZ = XM_PERMUTE_PS(vW,_MM_SHUFFLE(2,2,2,2));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3 // vW = XM_PERMUTE_PS(vW,_MM_SHUFFLE(3,3,3,3));
           // Perform the operation on the first row
           MULPS   XMM0, TXMVECTOR(TXMMATRIX([M2]).r[0]) // vX = _mm_mul_ps(vX,M2.r[0]);
           MULPS   XMM1, TXMVECTOR(TXMMATRIX([M2]).r[1]) // vY = _mm_mul_ps(vY,M2.r[1]);
           MULPS   XMM2, TXMVECTOR(TXMMATRIX([M2]).r[2]) // vZ = _mm_mul_ps(vZ,M2.r[2]);
           MULPS   XMM3, TXMVECTOR(TXMMATRIX([M2]).r[3]) // vW = _mm_mul_ps(vW,M2.r[3]);
           // Perform a binary add to reduce cumulative errors
           ADDPS   XMM0, XMM2 // vX = _mm_add_ps(vX,vZ);
           ADDPS   XMM1, XMM3 // vY = _mm_add_ps(vY,vW);
           ADDPS   XMM0, XMM1 // vX = _mm_add_ps(vX,vY);
           MOVUPS  [r0], XMM0 // r0 = vX;
           // Repeat for the other 3 rows
           MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([M1]).r[1]) // vW = M1.r[1];
           MOVUPS  XMM0, XMM3
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           MOVUPS  XMM1, XMM3
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           MOVUPS  XMM2, XMM3
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           MULPS   XMM0, TXMVECTOR(TXMMATRIX([M2]).r[0])
           MULPS   XMM1, TXMVECTOR(TXMMATRIX([M2]).r[1])
           MULPS   XMM2, TXMVECTOR(TXMMATRIX([M2]).r[2])
           MULPS   XMM3, TXMVECTOR(TXMMATRIX([M2]).r[3])
           ADDPS   XMM0, XMM2
           ADDPS   XMM1, XMM3
           ADDPS   XMM0, XMM1
           MOVUPS  [r1], XMM0 // r1 = vX;

           MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([M1]).r[2]) // vW = M1.r[2];
           MOVUPS  XMM0, XMM3
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           MOVUPS  XMM1, XMM3
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           MOVUPS  XMM2, XMM3
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           MULPS   XMM0, TXMVECTOR(TXMMATRIX([M2]).r[0])
           MULPS   XMM1, TXMVECTOR(TXMMATRIX([M2]).r[1])
           MULPS   XMM2, TXMVECTOR(TXMMATRIX([M2]).r[2])
           MULPS   XMM3, TXMVECTOR(TXMMATRIX([M2]).r[3])
           ADDPS   XMM0, XMM2
           ADDPS   XMM1, XMM3
           ADDPS   XMM0, XMM1
           MOVUPS  [r2], XMM0 // r1 = vX;

           MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([M1]).r[2]) // vW = M1.r[3];
           MOVUPS  XMM0, XMM3
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           MOVUPS  XMM1, XMM3
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           MOVUPS  XMM2, XMM3
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_3_3_3_3
           MULPS   XMM0, TXMVECTOR(TXMMATRIX([M2]).r[0])
           MULPS   XMM1, TXMVECTOR(TXMMATRIX([M2]).r[1])
           MULPS   XMM2, TXMVECTOR(TXMMATRIX([M2]).r[2])
           MULPS   XMM3, TXMVECTOR(TXMMATRIX([M2]).r[3])
           ADDPS   XMM0, XMM2
           ADDPS   XMM1, XMM3
           ADDPS   XMM0, XMM1
           MOVUPS  [r3], XMM0 // r1 = vX;



           // x.x,x.y,y.x,y.y
           MOVUPS  XMM0 ,[r0]
           SHUFPS  XMM0, [r1], _MM_SHUFFLE_1_0_1_0 // vTemp1 = _mm_shuffle_ps(r0,r1,_MM_SHUFFLE(1,0,1,0));
           // x.z,x.w,y.z,y.w
           MOVUPS  XMM2 ,[r0]
           SHUFPS  XMM2, [r1], _MM_SHUFFLE_3_2_3_2 // vTemp3 = _mm_shuffle_ps(r0,r1,_MM_SHUFFLE(3,2,3,2));
           // z.x,z.y,w.x,w.y
           MOVUPS  XMM1 ,[r2]
           SHUFPS  XMM1, [r3], _MM_SHUFFLE_1_0_1_0 // vTemp2 = _mm_shuffle_ps(r2,r3,_MM_SHUFFLE(1,0,1,0));
           // z.z,z.w,w.z,w.w
           MOVUPS  XMM3 ,[r2]
           SHUFPS  XMM3, [r3], _MM_SHUFFLE_3_2_3_2 // vTemp4 = _mm_shuffle_ps(r2,r3,_MM_SHUFFLE(3,2,3,2));

           // x.x,y.x,z.x,w.x
           MOVUPS  XMM4, XMM0
           SHUFPS  XMM4, XMM1, _MM_SHUFFLE_2_0_2_0 // mResult.r[0] = _mm_shuffle_ps(vTemp1, vTemp2,_MM_SHUFFLE(2,0,2,0));
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]), XMM4
           // x.y,y.y,z.y,w.y
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_1_3_1 // mResult.r[1] = _mm_shuffle_ps(vTemp1, vTemp2,_MM_SHUFFLE(3,1,3,1));
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]), XMM0
           // x.z,y.z,z.z,w.z
           MOVUPS  XMM4, XMM2
           SHUFPS  XMM4, XMM3, _MM_SHUFFLE_2_0_2_0 // mResult.r[2] = _mm_shuffle_ps(vTemp3, vTemp4,_MM_SHUFFLE(2,0,2,0));
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]), XMM4
           // x.w,y.w,z.w,w.w
           SHUFPS  XMM2, XMM3, _MM_SHUFFLE_3_1_3_1 // mResult.r[3] = _mm_shuffle_ps(vTemp3, vTemp4,_MM_SHUFFLE(3,1,3,1));
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]), XMM2
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}

function XMMatrixTranspose(M: TXMMATRIX): TXMMATRIX;
var
    P: TXMMATRIX;
begin
    // Original matrix:

    //     m00m01m02m03
    //     m10m11m12m13
    //     m20m21m22m23
    //     m30m31m32m33


    P.r[0] := XMVectorMergeXY(M.r[0], M.r[2]); // m00m20m01m21
    P.r[1] := XMVectorMergeXY(M.r[1], M.r[3]); // m10m30m11m31
    P.r[2] := XMVectorMergeZW(M.r[0], M.r[2]); // m02m22m03m23
    P.r[3] := XMVectorMergeZW(M.r[1], M.r[3]); // m12m32m13m33

    Result.r[0] := XMVectorMergeXY(P.r[0], P.r[1]); // m00m10m20m30
    Result.r[1] := XMVectorMergeZW(P.r[0], P.r[1]); // m01m11m21m31
    Result.r[2] := XMVectorMergeXY(P.r[2], P.r[3]); // m02m12m22m32
    Result.r[3] := XMVectorMergeZW(P.r[2], P.r[3]); // m03m13m23m33
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixTranspose(M: TXMMATRIX): TXMMATRIX;
begin
    (* ToDo
    float32x4x2_t P0 = vzipq_f32( M.r[0], M.r[2] );
    float32x4x2_t P1 = vzipq_f32( M.r[1], M.r[3] );

    float32x4x2_t T0 = vzipq_f32( P0.val[0], P1.val[0] );
    float32x4x2_t T1 = vzipq_f32( P0.val[1], P1.val[1] );

    XMMATRIX mResult;
    mResult.r[0] = T0.val[0];
    mResult.r[1] = T0.val[1];
    mResult.r[2] = T1.val[0];
    mResult.r[3] = T1.val[1];
    return mResult;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixTranspose(M: TXMMATRIX): TXMMATRIX; assembler;
asm
           MOVUPS  XMM0, TXMVECTOR(TXMMATRIX([M]).r[0])
           MOVUPS  XMM1, TXMVECTOR(TXMMATRIX([M]).r[1])
           MOVAPS  XMM2,XMM0
           MOVAPS  XMM3,XMM1

           MOVUPS  XMM4, TXMVECTOR(TXMMATRIX([M]).r[2])
           MOVUPS  XMM5, TXMVECTOR(TXMMATRIX([M]).r[3])
           MOVAPS  XMM6,XMM4
           MOVAPS  XMM7,XMM5

           // x.x,x.y,y.x,y.y
           SHUFPS  XMM0, XMM1, _MM_SHUFFLE_1_0_1_0 // vTemp1 = XMM0 = _mm_shuffle_ps(M.r[0],M.r[1],_MM_SHUFFLE(1,0,1,0));
           // x.z,x.w,y.z,y.w
           SHUFPS  XMM2, XMM3, _MM_SHUFFLE_3_2_3_2 // vTemp3 = XMM2 = _mm_shuffle_ps(M.r[0],M.r[1],_MM_SHUFFLE(3,2,3,2));
           // z.x,z.y,w.x,w.y
           SHUFPS  XMM4, XMM5, _MM_SHUFFLE_3_2_3_2 // vTemp2 = XMM4 = _mm_shuffle_ps(M.r[2],M.r[3],_MM_SHUFFLE(1,0,1,0));
           // z.z,z.w,w.z,w.w
           SHUFPS  XMM6, XMM7, _MM_SHUFFLE_3_2_3_2 // vTemp4 = XMM6 = _mm_shuffle_ps(M.r[2],M.r[3],_MM_SHUFFLE(3,2,3,2));

           MOVAPS  XMM1,XMM0  // vTemp1 = XMM0 = XMM1
           MOVAPS  XMM5,XMM4  // vTemp2 = XMM4 = XMM5

           MOVAPS  XMM3,XMM2  // vTemp3 = XMM2 = XMM3
           MOVAPS  XMM7,XMM6  // vTemp4 = XMM6 = XMM7

           // x.x,y.x,z.x,w.x
           SHUFPS  XMM0, XMM4, _MM_SHUFFLE_2_0_2_0 // mResult.r[0] = _mm_shuffle_ps(vTemp1, vTemp2,_MM_SHUFFLE(2,0,2,0));
           // x.y,y.y,z.y,w.y
           SHUFPS  XMM1, XMM5, _MM_SHUFFLE_3_1_3_1 // mResult.r[1] = _mm_shuffle_ps(vTemp1, vTemp2,_MM_SHUFFLE(3,1,3,1));
           // x.z,y.z,z.z,w.z
           SHUFPS  XMM2, XMM6, _MM_SHUFFLE_2_0_2_0 // mResult.r[2] = _mm_shuffle_ps(vTemp3, vTemp4,_MM_SHUFFLE(2,0,2,0));
           // x.w,y.w,z.w,w.w
           SHUFPS  XMM3, XMM7, _MM_SHUFFLE_3_1_3_1 // mResult.r[3] = _mm_shuffle_ps(vTemp3, vTemp4,_MM_SHUFFLE(3,1,3,1));

           // return mResult;
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]) ,XMM0
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]) ,XMM1
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]) ,XMM2
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]) ,XMM3
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_) OR DEFINED(_XM_ARM_NEON_INTRINSICS_)}

//------------------------------------------------------------------------------
// Return the inverse and the determinant of a 4x4 matrix
function XMMatrixInverse(out pDeterminant: TXMVECTOR; M: TXMMATRIX): TXMMATRIX;
var
    MT: TXMMATRIX;
    V0, V1: array [0..3] of TXMVECTOR;
    D0, D1, D2: TXMVECTOR;
    C0, C2, C4, C6: TXMVECTOR;
    C1, C3, C5, C7: TXMVECTOR;
    R: TXMMATRIX;
    Reciprocal: TXMVECTOR;
begin

    MT := XMMatrixTranspose(M);
    V0[0] := XMVectorSwizzle(MT.r[2], XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V1[0] := XMVectorSwizzle(MT.r[3], XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Z, XM_SWIZZLE_W);
    V0[1] := XMVectorSwizzle(MT.r[0], XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V1[1] := XMVectorSwizzle(MT.r[1], XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Z, XM_SWIZZLE_W);
    V0[2] := XMVectorPermute(MT.r[2], MT.r[0], XM_PERMUTE_0X, XM_PERMUTE_0Z, XM_PERMUTE_1X, XM_PERMUTE_1Z);
    V1[2] := XMVectorPermute(MT.r[3], MT.r[1], XM_PERMUTE_0Y, XM_PERMUTE_0W, XM_PERMUTE_1Y, XM_PERMUTE_1W);

    D0 := XMVectorMultiply(V0[0], V1[0]);
    D1 := XMVectorMultiply(V0[1], V1[1]);
    D2 := XMVectorMultiply(V0[2], V1[2]);

    V0[0] := XMVectorSwizzle(MT.r[2], XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Z, XM_SWIZZLE_W);
    V1[0] := XMVectorSwizzle(MT.r[3], XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V0[1] := XMVectorSwizzle(MT.r[0], XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Z, XM_SWIZZLE_W);
    V1[1] := XMVectorSwizzle(MT.r[1], XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V0[2] := XMVectorPermute(MT.r[2], MT.r[0], XM_PERMUTE_0Y, XM_PERMUTE_0W, XM_PERMUTE_1Y, XM_PERMUTE_1W);
    V1[2] := XMVectorPermute(MT.r[3], MT.r[1], XM_PERMUTE_0X, XM_PERMUTE_0Z, XM_PERMUTE_1X, XM_PERMUTE_1Z);

    D0 := XMVectorNegativeMultiplySubtract(V0[0], V1[0], D0);
    D1 := XMVectorNegativeMultiplySubtract(V0[1], V1[1], D1);
    D2 := XMVectorNegativeMultiplySubtract(V0[2], V1[2], D2);

    V0[0] := XMVectorSwizzle(MT.r[1], XM_SWIZZLE_Y, XM_SWIZZLE_Z, XM_SWIZZLE_X, XM_SWIZZLE_Y);
    V1[0] := XMVectorPermute(D0, D2, XM_PERMUTE_1Y, XM_PERMUTE_0Y, XM_PERMUTE_0W, XM_PERMUTE_0X);
    V0[1] := XMVectorSwizzle(MT.r[0], XM_SWIZZLE_Z, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_X);
    V1[1] := XMVectorPermute(D0, D2, XM_PERMUTE_0W, XM_PERMUTE_1Y, XM_PERMUTE_0Y, XM_PERMUTE_0Z);
    V0[2] := XMVectorSwizzle(MT.r[3], XM_SWIZZLE_Y, XM_SWIZZLE_Z, XM_SWIZZLE_X, XM_SWIZZLE_Y);
    V1[2] := XMVectorPermute(D1, D2, XM_PERMUTE_1W, XM_PERMUTE_0Y, XM_PERMUTE_0W, XM_PERMUTE_0X);
    V0[3] := XMVectorSwizzle(MT.r[2], XM_SWIZZLE_Z, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_X);
    V1[3] := XMVectorPermute(D1, D2, XM_PERMUTE_0W, XM_PERMUTE_1W, XM_PERMUTE_0Y, XM_PERMUTE_0Z);


    C0 := XMVectorMultiply(V0[0], V1[0]);
    C2 := XMVectorMultiply(V0[1], V1[1]);
    C4 := XMVectorMultiply(V0[2], V1[2]);
    C6 := XMVectorMultiply(V0[3], V1[3]);

    V0[0] := XMVectorSwizzle(MT.r[1], XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Y, XM_SWIZZLE_Z);
    V1[0] := XMVectorPermute(D0, D2, XM_PERMUTE_0W, XM_PERMUTE_0X, XM_PERMUTE_0Y, XM_PERMUTE_1X);
    V0[1] := XMVectorSwizzle(MT.r[0], XM_SWIZZLE_W, XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Y);
    V1[1] := XMVectorPermute(D0, D2, XM_PERMUTE_0Z, XM_PERMUTE_0Y, XM_PERMUTE_1X, XM_PERMUTE_0X);
    V0[2] := XMVectorSwizzle(MT.r[3], XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Y, XM_SWIZZLE_Z);
    V1[2] := XMVectorPermute(D1, D2, XM_PERMUTE_0W, XM_PERMUTE_0X, XM_PERMUTE_0Y, XM_PERMUTE_1Z);
    V0[3] := XMVectorSwizzle(MT.r[2], XM_SWIZZLE_W, XM_SWIZZLE_Z, XM_SWIZZLE_W, XM_SWIZZLE_Y);
    V1[3] := XMVectorPermute(D1, D2, XM_PERMUTE_0Z, XM_PERMUTE_0Y, XM_PERMUTE_1Z, XM_PERMUTE_0X);

    C0 := XMVectorNegativeMultiplySubtract(V0[0], V1[0], C0);
    C2 := XMVectorNegativeMultiplySubtract(V0[1], V1[1], C2);
    C4 := XMVectorNegativeMultiplySubtract(V0[2], V1[2], C4);
    C6 := XMVectorNegativeMultiplySubtract(V0[3], V1[3], C6);

    V0[0] := XMVectorSwizzle(MT.r[1], XM_SWIZZLE_W, XM_SWIZZLE_X, XM_SWIZZLE_W, XM_SWIZZLE_X);
    V1[0] := XMVectorPermute(D0, D2, XM_PERMUTE_0Z, XM_PERMUTE_1Y, XM_PERMUTE_1X, XM_PERMUTE_0Z);
    V0[1] := XMVectorSwizzle(MT.r[0], XM_SWIZZLE_Y, XM_SWIZZLE_W, XM_SWIZZLE_X, XM_SWIZZLE_Z);
    V1[1] := XMVectorPermute(D0, D2, XM_PERMUTE_1Y, XM_PERMUTE_0X, XM_PERMUTE_0W, XM_PERMUTE_1X);
    V0[2] := XMVectorSwizzle(MT.r[3], XM_SWIZZLE_W, XM_SWIZZLE_X, XM_SWIZZLE_W, XM_SWIZZLE_X);
    V1[2] := XMVectorPermute(D1, D2, XM_PERMUTE_0Z, XM_PERMUTE_1W, XM_PERMUTE_1Z, XM_PERMUTE_0Z);
    V0[3] := XMVectorSwizzle(MT.r[2], XM_SWIZZLE_Y, XM_SWIZZLE_W, XM_SWIZZLE_X, XM_SWIZZLE_Z);
    V1[3] := XMVectorPermute(D1, D2, XM_PERMUTE_1W, XM_PERMUTE_0X, XM_PERMUTE_0W, XM_PERMUTE_1Z);

    C1 := XMVectorNegativeMultiplySubtract(V0[0], V1[0], C0);
    C0 := XMVectorMultiplyAdd(V0[0], V1[0], C0);
    C3 := XMVectorMultiplyAdd(V0[1], V1[1], C2);
    C2 := XMVectorNegativeMultiplySubtract(V0[1], V1[1], C2);
    C5 := XMVectorNegativeMultiplySubtract(V0[2], V1[2], C4);
    C4 := XMVectorMultiplyAdd(V0[2], V1[2], C4);
    C7 := XMVectorMultiplyAdd(V0[3], V1[3], C6);
    C6 := XMVectorNegativeMultiplySubtract(V0[3], V1[3], C6);


    R.r[0] := XMVectorSelect(C0, C1, g_XMSelect0101.v);
    R.r[1] := XMVectorSelect(C2, C3, g_XMSelect0101.v);
    R.r[2] := XMVectorSelect(C4, C5, g_XMSelect0101.v);
    R.r[3] := XMVectorSelect(C6, C7, g_XMSelect0101.v);

    pDeterminant := XMVector4Dot(R.r[0], MT.r[0]);

    Reciprocal := XMVectorReciprocal(pDeterminant);

    Result.r[0] := XMVectorMultiply(R.r[0], Reciprocal);
    Result.r[1] := XMVectorMultiply(R.r[1], Reciprocal);
    Result.r[2] := XMVectorMultiply(R.r[2], Reciprocal);
    Result.r[3] := XMVectorMultiply(R.r[3], Reciprocal);

end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixInverse(out pDeterminant: TXMVECTOR; M: TXMMATRIX): TXMMATRIX; inline;
var
    MT: TXMMATRIX;
    D0, D1, D2: TXMVECTOR;
    V10, V11, V12, V13: TXMVECTOR;
    C0, C2, C4, C6: TXMVECTOR;
    C1, C3, C5, C7: TXMVECTOR;
    vTemp: TXMVECTOR;
begin
    MT := XMMatrixTranspose(M);
    asm
               // XMVECTOR V00 = XM_PERMUTE_PS(MT.r[2],_MM_SHUFFLE(1,1,0,0));
               MOVUPS  XMM0, TXMVECTOR(TXMMATRIX([MT]).r[2])
               MOVAPS  XMM1,XMM0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_1_1_0_0  // XMVECTOR V00 = XMM0

               // XMVECTOR V10 = XM_PERMUTE_PS(MT.r[3],_MM_SHUFFLE(3,2,3,2));
               MOVUPS  XMM2, TXMVECTOR(TXMMATRIX([MT]).r[3])
               MOVAPS  XMM3,XMM2
               SHUFPS  XMM2, XMM3, _MM_SHUFFLE_3_2_3_2  // XMVECTOR V10 = XMM2

               // XMVECTOR V01 = XM_PERMUTE_PS(MT.r[0],_MM_SHUFFLE(1,1,0,0));
               MOVUPS  XMM4, TXMVECTOR(TXMMATRIX([MT]).r[0])
               MOVAPS  XMM5,XMM4
               SHUFPS  XMM4, XMM5, _MM_SHUFFLE_1_1_0_0  // XMVECTOR V01 = XMM4

               // XMVECTOR V11 = XM_PERMUTE_PS(MT.r[1],_MM_SHUFFLE(3,2,3,2));
               MOVUPS  XMM6, TXMVECTOR(TXMMATRIX([MT]).r[1])
               MOVAPS  XMM7,XMM6
               SHUFPS  XMM6, XMM7, _MM_SHUFFLE_3_2_3_2  // XMVECTOR V11 = XMM6


               // XMVECTOR V02 = _mm_shuffle_ps(MT.r[2], MT.r[0],_MM_SHUFFLE(2,0,2,0));
               MOVUPS  XMM1, TXMVECTOR(TXMMATRIX([MT]).r[2])
               MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([MT]).r[0])
               SHUFPS  XMM1, XMM3, _MM_SHUFFLE_2_0_2_0  // XMVECTOR V02 = XMM1

               // XMVECTOR V12 = _mm_shuffle_ps(MT.r[3], MT.r[1],_MM_SHUFFLE(3,1,3,1));
               MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([MT]).r[3])
               MOVUPS  XMM5, TXMVECTOR(TXMMATRIX([MT]).r[1])
               SHUFPS  XMM3, XMM5, _MM_SHUFFLE_3_1_3_1  // XMVECTOR V12 = XMM3


               // XMVECTOR D0 = _mm_mul_ps(V00,V10);
               MULPS   XMM0, XMM2
               MOVUPS  [D0], XMM0

               // XMVECTOR D1 = _mm_mul_ps(V01,V11);
               MULPS   XMM4, XMM6
               MOVUPS  [D1], XMM4

               // XMVECTOR D2 = _mm_mul_ps(V02,V12);
               MULPS   XMM1, XMM3
               MOVUPS  [D2], XMM1

               // V00 = XM_PERMUTE_PS(MT.r[2],_MM_SHUFFLE(3,2,3,2));
               MOVUPS  XMM0, TXMVECTOR(TXMMATRIX([MT]).r[2])
               MOVAPS  XMM1,XMM0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_2_3_2 // V00 = XMM0

               // V10 = XM_PERMUTE_PS(MT.r[3],_MM_SHUFFLE(1,1,0,0));
               MOVUPS  XMM2, TXMVECTOR(TXMMATRIX([MT]).r[3])
               MOVAPS  XMM3,XMM2
               SHUFPS  XMM2, XMM3, _MM_SHUFFLE_1_1_0_0 // V10 = XMM2

               // V01 = XM_PERMUTE_PS(MT.r[0],_MM_SHUFFLE(3,2,3,2));
               MOVUPS  XMM4, TXMVECTOR(TXMMATRIX([MT]).r[0])
               MOVAPS  XMM5,XMM4
               SHUFPS  XMM4, XMM5, _MM_SHUFFLE_3_2_3_2 // V01 = XMM4

               // V11 = XM_PERMUTE_PS(MT.r[1],_MM_SHUFFLE(1,1,0,0));
               MOVUPS  XMM6, TXMVECTOR(TXMMATRIX([MT]).r[1])
               MOVAPS  XMM7,XMM6
               SHUFPS  XMM6, XMM7, _MM_SHUFFLE_1_1_0_0 // V11 = XMM6

               // V02 = _mm_shuffle_ps(MT.r[2],MT.r[0],_MM_SHUFFLE(3,1,3,1));
               MOVUPS  XMM1, TXMVECTOR(TXMMATRIX([MT]).r[2])
               MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([MT]).r[0])
               SHUFPS  XMM1, XMM3, _MM_SHUFFLE_3_1_3_1 // V02 = XMM1

               // V12 = _mm_shuffle_ps(MT.r[3],MT.r[1],_MM_SHUFFLE(2,0,2,0));
               MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([MT]).r[3])
               MOVUPS  XMM5, TXMVECTOR(TXMMATRIX([MT]).r[1])
               SHUFPS  XMM3, XMM5, _MM_SHUFFLE_2_0_2_0 // V12 = XMM3

               // V00 = _mm_mul_ps(V00,V10);
               MULPS   XMM0, XMM2 // V00 = XMM0

               // V01 = _mm_mul_ps(V01,V11);
               MULPS   XMM4, XMM6  // V01 = XMM4

               // V02 = _mm_mul_ps(V02,V12);
               MULPS   XMM1, XMM3  // V02 = XMM1

               // D0 = _mm_sub_ps(D0,V00);
               MOVUPS  XMM2, [D0]
               SUBPS   XMM2, XMM0 // D0 = XMM2
               MOVUPS  [D0], XMM2

               // D1 = _mm_sub_ps(D1,V01);
               MOVUPS  XMM3, [D1]
               SUBPS   XMM3, XMM4  // D1 = XMM3
               MOVUPS  [D1], XMM3

               // D2 = _mm_sub_ps(D2,V02);
               MOVUPS  XMM5, [D2]
               SUBPS   XMM5, XMM1 // D2 = XMM5
               MOVUPS  [D2], XMM5

               // V11 = D0Y,D0W,D2Y,D2Y
               // V11 = _mm_shuffle_ps(D0,D2,_MM_SHUFFLE(1,1,3,1));
               SHUFPS  XMM2, XMM5, _MM_SHUFFLE_1_1_3_1 // Temp V11 = XMM2

               // V00 = XM_PERMUTE_PS(MT.r[1], _MM_SHUFFLE(1,0,2,1));
               MOVUPS  XMM0, TXMVECTOR(TXMMATRIX([MT]).r[1])
               MOVAPS  XMM1,XMM0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_1_0_2_1  // V00 = XMM0

               // V10 = _mm_shuffle_ps(V11,D0,_MM_SHUFFLE(0,3,0,2));
               MOVUPS  XMM3, TXMVECTOR([D0])
               MOVAPS  XMM1,XMM2
               SHUFPS  XMM1, XMM3, _MM_SHUFFLE_0_3_0_2
               MOVUPS  TXMVECTOR([V10]), XMM1

               // V01 = XM_PERMUTE_PS(MT.r[0], _MM_SHUFFLE(0,1,0,2));
               MOVUPS  XMM4, TXMVECTOR(TXMMATRIX([MT]).r[0])
               MOVAPS  XMM5,XMM4
               SHUFPS  XMM4, XMM5, _MM_SHUFFLE_0_1_0_2  // V01 = XMM4

               // V11 = _mm_shuffle_ps(V11,D0,_MM_SHUFFLE(2,1,2,1));
               MOVUPS  XMM5, TXMVECTOR([D0])
               SHUFPS  XMM2, XMM5, _MM_SHUFFLE_2_1_2_1
               MOVUPS  TXMVECTOR([V11]), XMM2

               // V13 = D1Y,D1W,D2W,D2W
               // XMVECTOR V13 = _mm_shuffle_ps(D1,D2,_MM_SHUFFLE(3,3,3,1));
               MOVUPS  XMM6, TXMVECTOR([D1])
               MOVUPS  XMM7, TXMVECTOR([D2])
               SHUFPS  XMM6, XMM7, _MM_SHUFFLE_3_3_3_1
               MOVUPS  TXMVECTOR([V13]), XMM6           // temp V13 = XMM6

               // V02 = XM_PERMUTE_PS(MT.r[3], _MM_SHUFFLE(1,0,2,1));
               MOVUPS  XMM1, TXMVECTOR(TXMMATRIX([MT]).r[3])
               MOVAPS  XMM2,XMM1
               SHUFPS  XMM1, XMM2, _MM_SHUFFLE_1_0_2_1   // V02 = XMM1

               // V12 = _mm_shuffle_ps(V13,D1,_MM_SHUFFLE(0,3,0,2));
               MOVAPS  XMM2,XMM6
               MOVUPS  XMM3, TXMVECTOR([D1])
               SHUFPS  XMM2, XMM3, _MM_SHUFFLE_0_3_0_2
               MOVUPS  TXMVECTOR([V12]), XMM2

               //  V03 = XM_PERMUTE_PS(MT.r[2],_MM_SHUFFLE(0,1,0,2));
               MOVUPS  XMM2, TXMVECTOR(TXMMATRIX([MT]).r[2])
               MOVAPS  XMM3,XMM2
               SHUFPS  XMM2, XMM3, _MM_SHUFFLE_0_1_0_2   // V03 = XMM2

               // V13 = _mm_shuffle_ps(V13,D1,_MM_SHUFFLE(2,1,2,1));
               MOVUPS  XMM7, TXMVECTOR([D1])
               SHUFPS  XMM6, XMM7, _MM_SHUFFLE_2_1_2_1  // V13 = XMM6
               MOVUPS  TXMVECTOR([V13]), XMM6

               // XMVECTOR C0 = _mm_mul_ps(V00,V10);
               MULPS   XMM0, TXMVECTOR([V10])
               MOVUPS  TXMVECTOR([C0]), XMM0

               // XMVECTOR C2 = _mm_mul_ps(V01,V11);
               MULPS   XMM4, TXMVECTOR([V11])
               MOVUPS  TXMVECTOR([C2]), XMM4

               // XMVECTOR C4 = _mm_mul_ps(V02,V12);
               MULPS   XMM1, TXMVECTOR([V12])
               MOVUPS  TXMVECTOR([C4]), XMM1

               // XMVECTOR C6 = _mm_mul_ps(V03,V13);
               MULPS   XMM2, TXMVECTOR([V13])
               MOVUPS  TXMVECTOR([C6]), XMM2

               // V11 = D0X,D0Y,D2X,D2X
               // V11 = _mm_shuffle_ps(D0,D2,_MM_SHUFFLE(0,0,1,0));
               MOVUPS  XMM0, TXMVECTOR([D0])
               MOVUPS  XMM1, TXMVECTOR([D2])
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_0_0_1_0  // V11 = XMM0

               // V00 = XM_PERMUTE_PS(MT.r[1], _MM_SHUFFLE(2,1,3,2));
               MOVUPS  XMM1, TXMVECTOR(TXMMATRIX([MT]).r[1])
               MOVAPS  XMM2,XMM1
               SHUFPS  XMM1, XMM2, _MM_SHUFFLE_2_1_3_2  // V00 = XMM1

               // V10 = _mm_shuffle_ps(D0,V11,_MM_SHUFFLE(2,1,0,3));
               MOVUPS  XMM2, TXMVECTOR([D0])
               SHUFPS  XMM2, XMM0, _MM_SHUFFLE_2_1_0_3
               MOVUPS  TXMVECTOR([V10]), XMM2

               // V01 = XM_PERMUTE_PS(MT.r[0], _MM_SHUFFLE(1,3,2,3));
               MOVUPS  XMM3, TXMVECTOR(TXMMATRIX([MT]).r[0])
               MOVAPS  XMM4,XMM3
               SHUFPS  XMM3, XMM4, _MM_SHUFFLE_1_3_2_3 // V01 = XMM3

               // V11 = _mm_shuffle_ps(D0,V11,_MM_SHUFFLE(0,2,1,2));
               MOVUPS  XMM4, TXMVECTOR([D0])
               SHUFPS  XMM4, XMM0, _MM_SHUFFLE_0_2_1_2
               MOVUPS  TXMVECTOR([V11]), XMM4        // XMM0 is free

               // V13 = D1X,D1Y,D2Z,D2Z
               // V13 = _mm_shuffle_ps(D1,D2,_MM_SHUFFLE(2,2,1,0));
               MOVUPS  XMM6, TXMVECTOR([D1])
               MOVUPS  XMM7, TXMVECTOR([D2])
               SHUFPS  XMM6, XMM7, _MM_SHUFFLE_2_2_1_0
               MOVUPS  TXMVECTOR([V13]), XMM6

               // V02 = XM_PERMUTE_PS(MT.r[3], _MM_SHUFFLE(2,1,3,2));
               MOVUPS  XMM2, TXMVECTOR(TXMMATRIX([MT]).r[3])
               MOVAPS  XMM0,XMM2
               SHUFPS  XMM0, XMM2, _MM_SHUFFLE_2_1_3_2   // V02 = XMM0

               // V12 = _mm_shuffle_ps(D1,V13,_MM_SHUFFLE(2,1,0,3));
               MOVUPS  XMM6, TXMVECTOR([D1])
               SHUFPS  XMM6, TXMVECTOR([V13]), _MM_SHUFFLE_2_1_0_3 // V12 = XMM6

               // V03 = XM_PERMUTE_PS(MT.r[2],_MM_SHUFFLE(1,3,2,3));
               MOVUPS  XMM5, TXMVECTOR(TXMMATRIX([MT]).r[2])
               SHUFPS  XMM5, TXMVECTOR(TXMMATRIX([MT]).r[2]), _MM_SHUFFLE_1_3_2_3 // V03 = XMM5

               // V13 = _mm_shuffle_ps(D1,V13,_MM_SHUFFLE(0,2,1,2));
               MOVUPS  XMM7, TXMVECTOR([D1])
               SHUFPS  XMM7, TXMVECTOR([V13]), _MM_SHUFFLE_0_2_1_2  // V13 = XMM7

               // V00 = _mm_mul_ps(V00,V10);
               MULPS   XMM1, TXMVECTOR([V10])

               // V01 = _mm_mul_ps(V01,V11);
               MULPS   XMM3, TXMVECTOR([V11])

               // V02 = _mm_mul_ps(V02,V12);
               MULPS   XMM0, XMM6

               // V03 = _mm_mul_ps(V03,V13);
               MULPS   XMM5, XMM7

               // C0 = _mm_sub_ps(C0,V00);
               MOVUPS  XMM7, TXMVECTOR([C0])
               SUBPS   XMM7, XMM1
               MOVUPS  TXMVECTOR([C0]),XMM7

               // C2 = _mm_sub_ps(C2,V01);
               MOVUPS  XMM7, TXMVECTOR([C2])
               SUBPS   XMM7, XMM3
               MOVUPS  TXMVECTOR([C2]),XMM7

               // C4 = _mm_sub_ps(C4,V02);
               MOVUPS  XMM7, TXMVECTOR([C4])
               SUBPS   XMM7, XMM0
               MOVUPS  TXMVECTOR([C4]),XMM7

               // C6 = _mm_sub_ps(C6,V03);
               MOVUPS  XMM7, TXMVECTOR([C6])
               SUBPS   XMM7, XMM5
               MOVUPS  TXMVECTOR([C6]),XMM7

               // V00 = XM_PERMUTE_PS(MT.r[1],_MM_SHUFFLE(0,3,0,3));
               MOVUPS  XMM0, TXMVECTOR(TXMMATRIX([MT]).r[1])
               SHUFPS  XMM0, TXMVECTOR(TXMMATRIX([MT]).r[1]), _MM_SHUFFLE_0_3_0_3 // V00 = XMM0

               // V10 = D0Z,D0Z,D2X,D2Y
               // V10 = _mm_shuffle_ps(D0,D2,_MM_SHUFFLE(1,0,2,2));
               MOVUPS  XMM1, TXMVECTOR([D0])
               SHUFPS  XMM1, TXMVECTOR([D2]), _MM_SHUFFLE_1_0_2_2 // V10 = XMM1

               // V10 = XM_PERMUTE_PS(V10,_MM_SHUFFLE(0,2,3,0));
               MOVAPS  XMM2,XMM1
               SHUFPS  XMM1, XMM2, _MM_SHUFFLE_0_2_3_0 // V10 = XMM1

               // V01 = XM_PERMUTE_PS(MT.r[0],_MM_SHUFFLE(2,0,3,1));
               MOVUPS  XMM2, TXMVECTOR(TXMMATRIX([MT]).r[0])
               SHUFPS  XMM2, TXMVECTOR(TXMMATRIX([MT]).r[0]), _MM_SHUFFLE_2_0_3_1 // V01 = XMM2

               // V11 = D0X,D0W,D2X,D2Y
               // V11 = _mm_shuffle_ps(D0,D2,_MM_SHUFFLE(1,0,3,0));
               MOVUPS  XMM3, TXMVECTOR([D0])
               SHUFPS  XMM3, TXMVECTOR([D2]), _MM_SHUFFLE_1_0_3_0 // V11 = XMM3

               // V11 = XM_PERMUTE_PS(V11,_MM_SHUFFLE(2,1,0,3));
               MOVAPS  XMM4,XMM3
               SHUFPS  XMM3, XMM4, _MM_SHUFFLE_2_1_0_3 // V11 = XMM3

               // V02 = XM_PERMUTE_PS(MT.r[3],_MM_SHUFFLE(0,3,0,3));
               MOVUPS  XMM4, TXMVECTOR(TXMMATRIX([MT]).r[3])
               SHUFPS  XMM4, TXMVECTOR(TXMMATRIX([MT]).r[3]), _MM_SHUFFLE_0_3_0_3 // V02 = XMM4

               // V12 = D1Z,D1Z,D2Z,D2W
               // V12 = _mm_shuffle_ps(D1,D2,_MM_SHUFFLE(3,2,2,2));
               MOVUPS  XMM5, TXMVECTOR([D1])
               SHUFPS  XMM5, TXMVECTOR([D2]), _MM_SHUFFLE_3_2_2_2 // V12 = XMM5

               // V12 = XM_PERMUTE_PS(V12,_MM_SHUFFLE(0,2,3,0));
               MOVAPS  XMM6,XMM5
               SHUFPS  XMM5, XMM6, _MM_SHUFFLE_0_2_3_0  // V12 = XMM5

               // V13 = D1X,D1W,D2Z,D2W
               // V13 = _mm_shuffle_ps(D1,D2,_MM_SHUFFLE(3,2,3,0));
               MOVUPS  XMM6, TXMVECTOR([D1])
               SHUFPS  XMM6, TXMVECTOR([D2]), _MM_SHUFFLE_3_2_3_0

               // V13 = XM_PERMUTE_PS(V13,_MM_SHUFFLE(2,1,0,3));
               MOVAPS  XMM7,XMM6
               SHUFPS  XMM6, XMM7, _MM_SHUFFLE_2_1_0_3  // V13 = XMM6
               // V03 = XM_PERMUTE_PS(MT.r[2],_MM_SHUFFLE(2,0,3,1));
               MOVUPS  XMM7, TXMVECTOR(TXMMATRIX([MT]).r[2])
               SHUFPS  XMM7, TXMVECTOR(TXMMATRIX([MT]).r[2]), _MM_SHUFFLE_2_0_3_1 // V03 = XMM7

               // V00 = _mm_mul_ps(V00,V10);
               MULPS   XMM0, XMM1 // V00 = XMM0
               // V01 = _mm_mul_ps(V01,V11);
               MULPS   XMM2, XMM3  // V01 = XMM2
               // V02 = _mm_mul_ps(V02,V12);
               MULPS   XMM4, XMM5  // V02 = XMM4
               // V03 = _mm_mul_ps(V03,V13);
               MULPS   XMM7, XMM6  // V03 = XMM7

               // C1 = _mm_sub_ps(C0,V00);
               MOVUPS  XMM1, TXMVECTOR([C0])
               SUBPS   XMM1, XMM0;
               MOVUPS  TXMVECTOR([C1]),XMM1

               // C0 = _mm_add_ps(C0,V00);
               ADDPS   XMM0, TXMVECTOR([C0]) // a+b = b+a
               MOVUPS  TXMVECTOR([C0]),XMM0

               // C3 = _mm_add_ps(C2,V01);
               MOVUPS  XMM0, TXMVECTOR([C2])
               ADDPS   XMM0, XMM2
               MOVUPS  TXMVECTOR([C3]),XMM0

               // C2 = _mm_sub_ps(C2,V01);
               MOVUPS  XMM3, TXMVECTOR([C0])
               SUBPS   XMM3, XMM2
               MOVUPS  TXMVECTOR([C2]),XMM3

               // C5 = _mm_sub_ps(C4,V02);
               MOVUPS  XMM1, TXMVECTOR([C4])
               SUBPS   XMM1, XMM4;
               MOVUPS  TXMVECTOR([C5]),XMM1

               // C4 = _mm_add_ps(C4,V02);
               ADDPS   XMM4, TXMVECTOR([C4]); // a+b = b+a
               MOVUPS  TXMVECTOR([C4]),XMM4

               // C7 = _mm_add_ps(C6,V03);
               MOVUPS  XMM0, TXMVECTOR([C6])
               ADDPS   XMM0, XMM7
               MOVUPS  TXMVECTOR([C7]),XMM0

               // C6 = _mm_sub_ps(C6,V03);
               MOVUPS  XMM3, TXMVECTOR([C6])
               SUBPS   XMM3, XMM7
               MOVUPS  TXMVECTOR([C6]),XMM3

               // C0 = _mm_shuffle_ps(C0,C1,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C0])
               SHUFPS  XMM0, TXMVECTOR([C1]), _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C0]),XMM0

               // C2 = _mm_shuffle_ps(C2,C3,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C2])
               SHUFPS  XMM0, TXMVECTOR([C3]), _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C2]),XMM0

               // C4 = _mm_shuffle_ps(C4,C5,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C4])
               SHUFPS  XMM0, TXMVECTOR([C5]), _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C4]),XMM0

               // C6 = _mm_shuffle_ps(C6,C7,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C6])
               SHUFPS  XMM0, TXMVECTOR([C7]), _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C6]),XMM0

               // C0 = XM_PERMUTE_PS(C0,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C0])
               MOVAPS  XMM1,XMM0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C0]),XMM0

               // C2 = XM_PERMUTE_PS(C2,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C2])
               MOVAPS  XMM1,XMM0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C2]),XMM0

               // C4 = XM_PERMUTE_PS(C4,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C4])
               MOVAPS  XMM1,XMM0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C4]),XMM0

               // C6 = XM_PERMUTE_PS(C6,_MM_SHUFFLE(3,1,2,0));
               MOVUPS  XMM0, TXMVECTOR([C6])
               MOVAPS  XMM1,XMM0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_1_2_0
               MOVUPS   TXMVECTOR([C6]),XMM0
    end;
    // Get the determinate
    vTemp := XMVector4Dot(C0, MT.r[0]);
    pDeterminant := vTemp;
    asm
               // vTemp = _mm_div_ps(g_XMOne,vTemp);
               MOVUPS  XMM0, TXMVECTOR([g_XMOne])
               MOVUPS  XMM1, TXMVECTOR([vTemp])
               DIVPS   XMM0, XMM1
               MOVUPS   TXMVECTOR([vTemp]),XMM0  // vTemp = XMM0
               // mResult.r[0] = _mm_mul_ps(C0,vTemp);
               MOVUPS  XMM1, TXMVECTOR([C0])
               MULPS   XMM1, XMM0
               MOVUPS   TXMVECTOR(TXMMATRIX([result]).r[0]) ,XMM1
               // mResult.r[1] = _mm_mul_ps(C2,vTemp);
               MOVUPS  XMM1, TXMVECTOR([C2])
               MULPS   XMM1, XMM0
               MOVUPS   TXMVECTOR(TXMMATRIX([result]).r[1]) ,XMM1
               // mResult.r[2] = _mm_mul_ps(C4,vTemp);
               MOVUPS  XMM1, TXMVECTOR([C4])
               MULPS   XMM1, XMM0
               MOVUPS   TXMVECTOR(TXMMATRIX([result]).r[2]) ,XMM1
               // mResult.r[3] = _mm_mul_ps(C6,vTemp);
               MOVUPS  XMM1, TXMVECTOR([C6])
               MULPS   XMM1, XMM0
               MOVUPS   TXMVECTOR(TXMMATRIX([result]).r[3]) ,XMM1
    end;
end;

{$ENDIF}

function XMMatrixDeterminant(M: TXMMATRIX): TXMVECTOR;
const
    Sign: TXMVECTORF32 = (f: (1.0, -1.0, 1.0, -1.0));
var
    V0, V1, V2, V3, V4, V5, P0, P1, P2, S, R: TXMVECTOR;
begin

    V0 := XMVectorSwizzle(M.r[2], XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X);
    V1 := XMVectorSwizzle(M.r[3], XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V2 := XMVectorSwizzle(M.r[2], XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X);
    V3 := XMVectorSwizzle(M.r[3], XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z);
    V4 := XMVectorSwizzle(M.r[2], XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V5 := XMVectorSwizzle(M.r[3], XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z);

    P0 := XMVectorMultiply(V0, V1);
    P1 := XMVectorMultiply(V2, V3);
    P2 := XMVectorMultiply(V4, V5);

    V0 := XMVectorSwizzle(M.r[2], XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V1 := XMVectorSwizzle(M.r[3], XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X);
    V2 := XMVectorSwizzle(M.r[2], XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z);
    V3 := XMVectorSwizzle(M.r[3], XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X);
    V4 := XMVectorSwizzle(M.r[2], XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z);
    V5 := XMVectorSwizzle(M.r[3], XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y);

    P0 := XMVectorNegativeMultiplySubtract(V0, V1, P0);
    P1 := XMVectorNegativeMultiplySubtract(V2, V3, P1);
    P2 := XMVectorNegativeMultiplySubtract(V4, V5, P2);

    V0 := XMVectorSwizzle(M.r[1], XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z);
    V1 := XMVectorSwizzle(M.r[1], XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y);
    V2 := XMVectorSwizzle(M.r[1], XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X);

    S := XMVectorMultiply(M.r[0], Sign.v);
    R := XMVectorMultiply(V0, P0);
    R := XMVectorNegativeMultiplySubtract(V1, P1, R);
    R := XMVectorMultiplyAdd(V2, P2, R);

    Result := XMVector4Dot(S, R);
end;



procedure XM3RANKDECOMPOSE(out a, b, c: size_t; x, y, z: single); inline;
begin
    if ((x) < (y)) then
    begin
        if ((y) < (z)) then
        begin
            (a) := 2;
            (b) := 1;
            (c) := 0;
        end
        else
        begin
            (a) := 1;

            if ((x) < (z)) then
            begin
                (b) := 2;
                (c) := 0;
            end
            else
            begin
                (b) := 0;
                (c) := 2;
            end;
        end;
    end
    else
    begin
        if ((x) < (z)) then
        begin
            (a) := 2;
            (b) := 0;
            (c) := 1;
        end
        else
        begin
            (a) := 0;

            if ((y) < (z)) then
            begin
                (b) := 2;
                (c) := 1;
            end
            else
            begin
                (b) := 1;
                (c) := 2;
            end;
        end;
    end;
end;


function XMMatrixDecompose(out outScale: TXMVECTOR; out outRotQuat: TXMVECTOR; out outTrans: TXMVECTOR; M: TXMMATRIX): boolean;
var
    pvCanonicalBasis: array [0..2] of TXMVECTOR;
    ppvBasis: array [0..2] of TXMVECTOR;
    matTemp: TXMMATRIX;
    pfScales: TXMVECTOR;
    a, b, c: size_t;
    fDet: single;
    aa, bb, cc: size_t;
    fAbsX, fAbsY, fAbsZ: single;
begin
    pvCanonicalBasis[1] := g_XMIdentityR0.v;
    pvCanonicalBasis[1] := g_XMIdentityR1.v;
    pvCanonicalBasis[1] := g_XMIdentityR2.v;

    // Get the translation
    outTrans := M.r[3];


    ppvBasis[0] := matTemp.r[0];
    ppvBasis[1] := matTemp.r[1];
    ppvBasis[2] := matTemp.r[2];

    matTemp.r[0] := M.r[0];
    matTemp.r[1] := M.r[1];
    matTemp.r[2] := M.r[2];
    matTemp.r[3] := g_XMIdentityR3.v;

    //    pfScales := (float *)outScale;


    XMVectorGetXPtr(pfScales.f32[0], XMVector3Length(ppvBasis[0]));
    XMVectorGetXPtr(pfScales.f32[1], XMVector3Length(ppvBasis[1]));
    XMVectorGetXPtr(pfScales.f32[2], XMVector3Length(ppvBasis[2]));
    pfScales.f32[3] := 0.0;

    XM3RANKDECOMPOSE(a, b, c, pfScales.f32[0], pfScales.f32[1], pfScales.f32[2]);

    if (pfScales.f32[a] < XM3_DECOMP_EPSILON) then
    begin
        ppvBasis[a] := pvCanonicalBasis[a];
    end;
    ppvBasis[a] := XMVector3Normalize(ppvBasis[a]);

    if (pfScales.f32[b] < XM3_DECOMP_EPSILON) then
    begin

        fAbsX := abs(XMVectorGetX(ppvBasis[a]));
        fAbsY := abs(XMVectorGetY(ppvBasis[a]));
        fAbsZ := abs(XMVectorGetZ(ppvBasis[a]));

        XM3RANKDECOMPOSE(aa, bb, cc, fAbsX, fAbsY, fAbsZ);

        ppvBasis[b] := XMVector3Cross(ppvBasis[a], pvCanonicalBasis[cc]);
    end;

    ppvBasis[b] := XMVector3Normalize(ppvBasis[b]);

    if (pfScales.f32[c] < XM3_DECOMP_EPSILON) then
    begin
        ppvBasis[c] := XMVector3Cross(ppvBasis[a], ppvBasis[b]);
    end;

    ppvBasis[c] := XMVector3Normalize(ppvBasis[c]);

    fDet := XMVectorGetX(XMMatrixDeterminant(matTemp));

    // use Kramer's rule to check for handedness of coordinate system
    if (fDet < 0.0) then
    begin
        // switch coordinate system by negating the scale and inverting the basis vector on the x-axis
        pfScales.f32[a] := -pfScales.f32[a];
        ppvBasis[a] := XMVectorNegate(ppvBasis[a]);

        fDet := -fDet;
    end;

    fDet := fDet - 1.0;
    fDet := fDet * fDet;

    if (XM3_DECOMP_EPSILON < fDet) then
    begin
        // Non-SRT matrix encountered
        Result := False;
        Exit;
    end;

    // generate the quaternion from the matrix
    outRotQuat := XMQuaternionRotationMatrix(matTemp);

    outScale := pfScales;

    Result := True;
end;


//------------------------------------------------------------------------------
// Transformation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------
function XMMatrixIdentity: TXMMATRIX;
begin
    Result.r[0] := g_XMIdentityR0.v;
    Result.r[1] := g_XMIdentityR1.v;
    Result.r[2] := g_XMIdentityR2.v;
    Result.r[3] := g_XMIdentityR3.v;
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixSet(m00: single; m01: single; m02: single; m03: single; m10: single; m11: single; m12: single; m13: single; m20: single; m21: single; m22: single; m23: single;
    m30: single; m31: single; m32: single; m33: single): TXMMATRIX;
begin
    Result.m[0, 0] := m00;
    Result.m[0, 1] := m01;
    Result.m[0, 2] := m02;
    Result.m[0, 3] := m03;
    Result.m[1, 0] := m10;
    Result.m[1, 1] := m11;
    Result.m[1, 2] := m12;
    Result.m[1, 3] := m13;
    Result.m[2, 0] := m20;
    Result.m[2, 1] := m21;
    Result.m[2, 2] := m22;
    Result.m[2, 3] := m23;
    Result.m[3, 0] := m30;
    Result.m[3, 1] := m31;
    Result.m[3, 2] := m32;
    Result.m[3, 3] := m33;
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixSet(m00: single; m01: single; m02: single; m03: single; m10: single; m11: single; m12: single; m13: single; m20: single; m21: single; m22: single; m23: single;
    m30: single; m31: single; m32: single; m33: single): TXMMATRIX;
begin
    Result.r[0] := XMVectorSet(m00, m01, m02, m03);
    Result.r[1] := XMVectorSet(m10, m11, m12, m13);
    Result.r[2] := XMVectorSet(m20, m21, m22, m23);
    Result.r[3] := XMVectorSet(m30, m31, m32, m33);
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixTranslation(OffsetX: single; OffsetY: single; OffsetZ: single): TXMMATRIX;
begin

    Result.m[0, 0] := 1.0;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := 1.0;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := 1.0;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := OffsetX;
    Result.m[3, 1] := OffsetY;
    Result.m[3, 2] := OffsetZ;
    Result.m[3, 3] := 1.0;

end;

{$ELSE}// (_XM_SSE_INTRINSICS_) OR defined(_XM_ARM_NEON_INTRINSICS_)
function XMMatrixTranslation(OffsetX: single; OffsetY: single; OffsetZ: single): TXMMATRIX;
begin
    Result.r[0] := g_XMIdentityR0.v;
    Result.r[1] := g_XMIdentityR1.v;
    Result.r[2] := g_XMIdentityR2.v;
    Result.r[3] := XMVectorSet(OffsetX, OffsetY, OffsetZ, 1.0);
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixTranslationFromVector(Offset: TXMVECTOR): TXMMATRIX;
begin

    Result.m[0, 0] := 1.0;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := 1.0;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := 1.0;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := Offset.f32[0];
    Result.m[3, 1] := Offset.f32[1];
    Result.m[3, 2] := Offset.f32[2];
    Result.m[3, 3] := 1.0;
end;

{$ELSE}// (_XM_SSE_INTRINSICS_) OR defined(_XM_ARM_NEON_INTRINSICS_)
function XMMatrixTranslationFromVector(Offset: TXMVECTOR): TXMMATRIX;
begin
    Result.r[0] := g_XMIdentityR0.v;
    Result.r[1] := g_XMIdentityR1.v;
    Result.r[2] := g_XMIdentityR2.v;
    Result.r[3] := XMVectorSelect(g_XMIdentityR3.v, Offset, g_XMSelect1110.v);
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}

function XMMatrixScaling(ScaleX: single; ScaleY: single; ScaleZ: single): TXMMATRIX;
begin
    Result.m[0, 0] := ScaleX;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := ScaleY;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := ScaleZ;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := 0.0;
    Result.m[3, 3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixScaling(ScaleX: single; ScaleY: single; ScaleZ: single): TXMMATRIX;
begin
    (* ToDo
    const XMVECTOR Zero = vdupq_n_f32(0);
    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( ScaleX, Zero, 0 );
    M.r[1] = vsetq_lane_f32( ScaleY, Zero, 1 );
    M.r[2] = vsetq_lane_f32( ScaleZ, Zero, 2 );
    M.r[3] = g_XMIdentityR3.v;
    return M;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixScaling(ScaleX: single; ScaleY: single; ScaleZ: single): TXMMATRIX; assembler;
asm
           XORPS    XMM0, XMM0 // set to zero
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]),XMM0
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]),XMM0
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]),XMM0
           MOVUPS  XMM1,TXMVECTOR([g_XMIdentityR3.v]);
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]), XMM1
           // result.r[0] := _mm_set_ps( 0, 0, 0, ScaleX );
           MOV     EAX, [ScaleX]
           MOV     TXMVECTOR(TXMMATRIX([result]).r[0]).f32[3],EAX

           // result.r[1] := _mm_set_ps( 0, 0, ScaleY, 0 );
           MOV     EAX, [ScaleY]
           MOV     TXMVECTOR(TXMMATRIX([result]).r[1]).f32[2],EAX

           // result.r[2] := _mm_set_ps( 0, ScaleZ, 0, 0 );
           MOV     EAX, [ScaleZ]
           MOV     TXMVECTOR(TXMMATRIX([result]).r[2]).f32[1],EAX
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}

function XMMatrixScalingFromVector(constref Scale: TXMVECTOR): TXMMATRIX;
begin

    Result.m[0, 0] := Scale.f32[0];
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := Scale.f32[1];
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := Scale.f32[2];
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := 0.0;
    Result.m[3, 3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixScalingFromVector(constref Scale: TXMVECTOR): TXMMATRIX;
begin
    (* ToDo
    XMMATRIX M;
    M.r[0] = vandq_u32(Scale,g_XMMaskX);
    M.r[1] = vandq_u32(Scale,g_XMMaskY);
    M.r[2] = vandq_u32(Scale,g_XMMaskZ);
    M.r[3] = g_XMIdentityR3.v;
    return M;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixScalingFromVector(constref Scale: TXMVECTOR): TXMMATRIX; assembler;
asm
           MOVUPS  XMM0,[g_XMMaskX]
           MOVUPS  XMM1,[g_XMMaskY]
           MOVUPS  XMM2,[g_XMMaskZ]
           MOVUPS  XMM3,[Scale]
           ANDPS   XMM0, XMM3 // _mm_and_ps(Scale,g_XMMaskX);
           ANDPS   XMM1, XMM3 // _mm_and_ps(Scale,g_XMMaskX);
           ANDPS   XMM2, XMM3 // _mm_and_ps(Scale,g_XMMaskZ);
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]),XMM0
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]),XMM1
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]),XMM2
           MOVUPS  XMM3,[g_XMIdentityR3.v]
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]),XMM3
end;
{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixRotationX(Angle: single): TXMMATRIX;
var
    fSinAngle, fCosAngle: single;
begin

    XMScalarSinCos(fSinAngle, fCosAngle, Angle);

    Result.m[0, 0] := 1.0;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := fCosAngle;
    Result.m[1, 2] := fSinAngle;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := -fSinAngle;
    Result.m[2, 2] := fCosAngle;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := 0.0;
    Result.m[3, 3] := 1.0;

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixRotationX(Angle: single): TXMMATRIX;
begin
    (* ToDo
    float    fSinAngle;
    float    fCosAngle;
    XMScalarSinCos(&fSinAngle, &fCosAngle, Angle);

    const XMVECTOR Zero = vdupq_n_f32(0);

    XMVECTOR T1 = vsetq_lane_f32( fCosAngle, Zero, 1 );
    T1 = vsetq_lane_f32( fSinAngle, T1, 2 );

    XMVECTOR T2 = vsetq_lane_f32( -fSinAngle, Zero, 1 );
    T2 = vsetq_lane_f32( fCosAngle, T2, 2 );

    XMMATRIX M;
    M.r[0] = g_XMIdentityR0.v;
    M.r[1] = T1;
    M.r[2] = T2;
    M.r[3] = g_XMIdentityR3.v;
    return M;
*)
end;

{$ELSE}//  _XM_SSE_INTRINSICS_
function XMMatrixRotationX(Angle: single): TXMMATRIX;
var
    SinAngle, CosAngle: single;
    vSin, vCos: TXMVECTOR;
begin
    XMScalarSinCos(SinAngle, CosAngle, Angle);
    asm
               MOVSS   XMM0, [SinAngle] // vSin
               MOVSS   XMM1, [CosAngle] // vCos
               // x = 0,y = cos,z = sin, w = 0
               SHUFPS  XMM1, XMM0, _MM_SHUFFLE_3_0_0_3 // vCos = _mm_shuffle_ps(vCos,vSin,_MM_SHUFFLE(3,0,0,3));

               MOVUPS  XMM2,[g_XMIdentityR0]
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]) ,XMM2
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]),XMM1 // M.r[1] := vCos;
               // x = 0,y = sin,z = cos, w = 0
               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_1_2_0 // vCos = XM_PERMUTE_PS(vCos,_MM_SHUFFLE(3,1,2,0));
               // x = 0,y = -sin,z = cos, w = 0
               MOVSS   XMM2, [g_XMNegateY]
               MULPS   XMM1, XMM2 // vCos = _mm_mul_ps(vCos,g_XMNegateY);
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]),XMM1 // M.r[2] := vCos;
               MOVUPS  XMM2,[g_XMIdentityR3]
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]),XMM2
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixRotationY(Angle: single): TXMMATRIX;
var
    fSinAngle: single;
    fCosAngle: single;
begin

    XMScalarSinCos(fSinAngle, fCosAngle, Angle);


    Result.m[0, 0] := fCosAngle;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := -fSinAngle;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := 1.0;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := fSinAngle;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fCosAngle;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := 0.0;
    Result.m[3, 3] := 1.0;

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixRotationY(Angle: single): TXMMATRIX;
begin
    (* ToDo
    float    fSinAngle;
    float    fCosAngle;
    XMScalarSinCos(&fSinAngle, &fCosAngle, Angle);

    const XMVECTOR Zero = vdupq_n_f32(0);

    XMVECTOR T0 = vsetq_lane_f32( fCosAngle, Zero, 0 );
    T0 = vsetq_lane_f32( -fSinAngle, T0, 2 );

    XMVECTOR T2 = vsetq_lane_f32( fSinAngle, Zero, 0 );
    T2 = vsetq_lane_f32( fCosAngle, T2, 2 );

    XMMATRIX M;
    M.r[0] = T0;
    M.r[1] = g_XMIdentityR1.v;
    M.r[2] = T2;
    M.r[3] = g_XMIdentityR3.v;
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixRotationY(Angle: single): TXMMATRIX;
var
    SinAngle: single;
    CosAngle: single;
begin
    XMScalarSinCos(SinAngle, CosAngle, Angle);
    asm
               MOVSS   XMM0, [SinAngle] // vSin
               MOVSS   XMM1, [CosAngle] // vCos
               // x = sin,y = 0,z = cos, w = 0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_0_3_0 // vSin = _mm_shuffle_ps(vSin,vCos,_MM_SHUFFLE(3,0,3,0));

               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]) ,XMM0 // M.r[2] = vSin;
               MOVUPS  XMM2,[g_XMIdentityR1]
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]) ,XMM2 // M.r[1] = g_XMIdentityR1;
               // x = cos,y = 0,z = sin, w = 0
               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_1_2 // vSin = XM_PERMUTE_PS(vSin,_MM_SHUFFLE(3,0,1,2));
               // x = cos,y = 0,z = -sin, w = 0
               MOVSS   XMM2, [g_XMNegateZ]
               MULPS   XMM0, XMM2 // vSin = _mm_mul_ps(vSin,g_XMNegateZ);
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]) ,XMM0 // M.r[0] = vSin;

               MOVSS   XMM2, [g_XMIdentityR3]
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]) ,XMM2 // M.r[3] = g_XMIdentityR3;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixRotationZ(Angle: single): TXMMATRIX;
var
    fSinAngle: single;
    fCosAngle: single;
begin

    XMScalarSinCos(fSinAngle, fCosAngle, Angle);


    Result.m[0, 0] := fCosAngle;
    Result.m[0, 1] := fSinAngle;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := -fSinAngle;
    Result.m[1, 1] := fCosAngle;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := 1.0;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := 0.0;
    Result.m[3, 3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixRotationZ(Angle: single): TXMMATRIX;
begin
    (* ToDo
    float    fSinAngle;
    float    fCosAngle;
    XMScalarSinCos(&fSinAngle, &fCosAngle, Angle);

    const XMVECTOR Zero = vdupq_n_f32(0);

    XMVECTOR T0 = vsetq_lane_f32( fCosAngle, Zero, 0 );
    T0 = vsetq_lane_f32( fSinAngle, T0, 1 );

    XMVECTOR T1 = vsetq_lane_f32( -fSinAngle, Zero, 0 );
    T1 = vsetq_lane_f32( fCosAngle, T1, 1 );

    XMMATRIX M;
    M.r[0] = T0;
    M.r[1] = T1;
    M.r[2] = g_XMIdentityR2.v;
    M.r[3] = g_XMIdentityR3.v;
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixRotationZ(Angle: single): TXMMATRIX;
var
    SinAngle: single;
    CosAngle: single;
begin

    XMScalarSinCos(SinAngle, CosAngle, Angle);
    asm
               MOVSS   XMM0, [SinAngle] // vSin
               MOVSS   XMM1, [CosAngle] // vCos

               // x = cos,y = sin,z = 0, w = 0
               UNPCKLPS XMM1, XMM0 // vCos = _mm_unpacklo_ps(vCos,vSin);

               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]),XMM1 // M.r[0] = vCos;
               // x = sin,y = cos,z = 0, w = 0
               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_2_0_1 // vCos = XM_PERMUTE_PS(vCos,_MM_SHUFFLE(3,2,0,1));
               // x = cos,y = -sin,z = 0, w = 0
               MOVSS   XMM2, [g_XMNegateX]
               MULPS   XMM1, XMM2 // vCos = _mm_mul_ps(vCos,g_XMNegateX);
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]),XMM1 // M.r[1] = vCos;
               MOVSS   XMM2, [g_XMIdentityR2]
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]),XMM2 // M.r[2] = g_XMIdentityR2;
               MOVSS   XMM2, [g_XMIdentityR3]
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]),XMM2 // M.r[3] = g_XMIdentityR3;
    end;
end;

{$ENDIF}

function XMMatrixRotationRollPitchYaw(Pitch: single; Yaw: single; Roll: single): TXMMATRIX;
var
    Angles: TXMVECTOR;
begin
    Angles := XMVectorSet(Pitch, Yaw, Roll, 0.0);
    Result := XMMatrixRotationRollPitchYawFromVector(Angles);
end;


function XMMatrixRotationRollPitchYawFromVector(Angles: TXMVECTOR): TXMMATRIX;
var
    Q: TXMVECTOR;
begin
    Q := XMQuaternionRotationRollPitchYawFromVector(Angles);
    Result := XMMatrixRotationQuaternion(Q);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_) OR DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixRotationNormal(constref NormalAxis: TXMVECTOR; constref Angle: single): TXMMATRIX;
var
    fSinAngle: single;
    fCosAngle: single;
    A, C0, C1, C2, N0, N1, V0, V1, V2, R0, R1, R2: TXMVECTOR;
begin

    XMScalarSinCos(&fSinAngle, &fCosAngle, Angle);

    A := XMVectorSet(fSinAngle, fCosAngle, 1.0 - fCosAngle, 0.0);

    C2 := XMVectorSplatZ(A);
    C1 := XMVectorSplatY(A);
    C0 := XMVectorSplatX(A);

    N0 := XMVectorSwizzle(NormalAxis, XM_SWIZZLE_Y, XM_SWIZZLE_Z, XM_SWIZZLE_X, XM_SWIZZLE_W);
    N1 := XMVectorSwizzle(NormalAxis, XM_SWIZZLE_Z, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_W);

    V0 := XMVectorMultiply(C2, N0);
    V0 := XMVectorMultiply(V0, N1);

    R0 := XMVectorMultiply(C2, NormalAxis);
    R0 := XMVectorMultiplyAdd(R0, NormalAxis, C1);

    R1 := XMVectorMultiplyAdd(C0, NormalAxis, V0);
    R2 := XMVectorNegativeMultiplySubtract(C0, NormalAxis, V0);

    V0 := XMVectorSelect(A, R0, g_XMSelect1110.v);
    V1 := XMVectorPermute(R1, R2, XM_PERMUTE_0Z, XM_PERMUTE_1Y, XM_PERMUTE_1Z, XM_PERMUTE_0X);
    V2 := XMVectorPermute(R1, R2, XM_PERMUTE_0Y, XM_PERMUTE_1X, XM_PERMUTE_0Y, XM_PERMUTE_1X);


    Result.r[0] := XMVectorPermute(V0, V1, XM_PERMUTE_0X, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_0W);
    Result.r[1] := XMVectorPermute(V0, V1, XM_PERMUTE_1Z, XM_PERMUTE_0Y, XM_PERMUTE_1W, XM_PERMUTE_0W);
    Result.r[2] := XMVectorPermute(V0, V2, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_0Z, XM_PERMUTE_0W);
    Result.r[3] := g_XMIdentityR3.v;
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixRotationNormal(constref NormalAxis: TXMVECTOR; constref Angle: single): TXMMATRIX;
const
    v: single = 1.0;
var
    fSinAngle: single;
    fCosAngle: single;
    C0, C1, C2, N0, N1: TXMVECTOR;

begin

    XMScalarSinCos(fSinAngle, fCosAngle, Angle);
    asm
               // C2 = _mm_set_ps1(1.0f - fCosAngle);
               MOVSS       XMM0,[v]
               SUBSS       XMM0,[fCosAngle]
               SHUFPS      XMM0,XMM0,0
               MOVAPS      [C2],XMM0
               // C1 = _mm_set_ps1(fCosAngle);
               MOVSS       XMM0,[fCosAngle]
               MOVAPS      [C1],XMM0
               // C0 = _mm_set_ps1(fSinAngle);
               MOVSS       XMM0,[fSinAngle]
               MOVAPS      [C0],XMM0

               //  N0 = XM_PERMUTE_PS(NormalAxis,_MM_SHUFFLE(3,0,2,1));
               MOVSS       XMM0,[NormalAxis]
               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_3_0_2_1 // N0 = XMM0

               // N1 = XM_PERMUTE_PS(NormalAxis,_MM_SHUFFLE(3,1,0,2));
               MOVSS       XMM1,[NormalAxis]
               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_3_1_0_2

               MULPS   XMM0, [C2] // V0 = _mm_mul_ps(C2, N0); // a*b = b*a
               MULPS   XMM0, XMM1 // V0 = _mm_mul_ps(V0, N1); // V0 = xmm0

               // R0 = _mm_mul_ps(C2, NormalAxis);
               MOVUPS  XMM1,[C2]
               MULPS   XMM1, [NormalAxis]   // R0 = XMM1

               MULPS   XMM1, [NormalAxis]  // R0 = _mm_mul_ps(R0, NormalAxis);
               ADDPS   XMM1, [C1] // R0 = _mm_add_ps(R0, C1);

               // R1 = _mm_mul_ps(C0, NormalAxis);
               MOVUPS  XMM2,[C0]
               MULPS   XMM2, [NormalAxis]   // R1 = XMM2
               ADDPS   XMM2, XMM0 // R1 = _mm_add_ps(R1, V0);

               // R2 = _mm_mul_ps(C0, NormalAxis);
               MOVUPS  XMM3,[C0]
               MULPS   XMM3, [NormalAxis]   // R2 = XMM3
               // R2 = _mm_sub_ps(V0,R2);
               MOVUPS  XMM4,XMM0
               SUBPS   XMM4, XMM3  // R2 = XMM4

               // V0 = _mm_and_ps(R0,g_XMMask3);
               ADDPS   XMM1, [g_XMMask3] // V0 =XMM1

               // V1 = _mm_shuffle_ps(R1,R2,_MM_SHUFFLE(2,1,2,0));
               MOVUPS  XMM5,XMM2 // save copy R1
               SHUFPS  XMM2, XMM4, _MM_SHUFFLE_2_1_2_0  // XMM2 = V1

               // V1 = XM_PERMUTE_PS(V1,_MM_SHUFFLE(0,3,2,1));
               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_0_3_2_1

               // V2 = _mm_shuffle_ps(R1,R2,_MM_SHUFFLE(0,0,1,1));
               SHUFPS  XMM5, XMM4, _MM_SHUFFLE_0_0_1_1  // XMM5 = V2
               // V2 = XM_PERMUTE_PS(V2,_MM_SHUFFLE(2,0,2,0));
               SHUFPS  XMM5, XMM5, _MM_SHUFFLE_2_0_2_0

               // R2 = _mm_shuffle_ps(V0,V1,_MM_SHUFFLE(1,0,3,0));
               MOVUPS  XMM4, XMM1
               SHUFPS  XMM4, XMM2, _MM_SHUFFLE_1_0_3_0  // R2 = XMM4

               // R2 = XM_PERMUTE_PS(R2,_MM_SHUFFLE(1,3,2,0));
               SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_3_2_0


               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]), XMM4 // M.r[0] = R2;

               // R2 = _mm_shuffle_ps(V0,V1,_MM_SHUFFLE(3,2,3,1));
               MOVUPS  XMM4, XMM1
               SHUFPS  XMM4, XMM2, _MM_SHUFFLE_3_2_3_1  // R2 = XMM4
               // R2 = XM_PERMUTE_PS(R2,_MM_SHUFFLE(1,3,0,2));
               SHUFPS  XMM4, XMM4, _MM_SHUFFLE_1_3_0_2
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]), XMM4 // M.r[1] = R2;

               // V2 = _mm_shuffle_ps(V2,V0,_MM_SHUFFLE(3,2,1,0));
               SHUFPS  XMM5, XMM1, _MM_SHUFFLE_3_2_1_0

               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]), XMM5 // M.r[2] = V2;
               // M.r[3] = g_XMIdentityR3.v;
               MOVUPS  XMM5, [g_XMIdentityR3];
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]), XMM5

    end;
end;

{$ENDIF}


function XMMatrixRotationAxis(Axis: TXMVECTOR; Angle: single): TXMMATRIX;
var
    Normal: TXMVECTOR;
begin
    assert(not XMVector3Equal(Axis, XMVectorZero()));
    assert(not XMVector3IsInfinite(Axis));

    Normal := XMVector3Normalize(Axis);
    Result := XMMatrixRotationNormal(Normal, Angle);
end;

{$IF DEFINED(_XM_NO_INTRINSICS_) OR DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixRotationQuaternion(Quaternion: TXMVECTOR): TXMMATRIX;
const
    Constant1110: TXMVECTOR = (f32: (1.0, 1.0, 1.0, 0.0));
var
    Q0, Q1, V0, V1, V2, R0, R1, R2: TXMVECTOR;
begin

    Q0 := XMVectorAdd(Quaternion, Quaternion);
    Q1 := XMVectorMultiply(Quaternion, Q0);

    V0 := XMVectorPermute(Q1, Constant1110, XM_PERMUTE_0Y, XM_PERMUTE_0X, XM_PERMUTE_0X, XM_PERMUTE_1W);
    V1 := XMVectorPermute(Q1, Constant1110, XM_PERMUTE_0Z, XM_PERMUTE_0Z, XM_PERMUTE_0Y, XM_PERMUTE_1W);
    R0 := XMVectorSubtract(Constant1110, V0);
    R0 := XMVectorSubtract(R0, V1);

    V0 := XMVectorSwizzle(Quaternion, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_Y, XM_SWIZZLE_W);
    V1 := XMVectorSwizzle(Q0, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Z, XM_SWIZZLE_W);
    V0 := XMVectorMultiply(V0, V1);

    V1 := XMVectorSplatW(Quaternion);
    V2 := XMVectorSwizzle(Q0, XM_SWIZZLE_Y, XM_SWIZZLE_Z, XM_SWIZZLE_X, XM_SWIZZLE_W);
    V1 := XMVectorMultiply(V1, V2);

    R1 := XMVectorAdd(V0, V1);
    R2 := XMVectorSubtract(V0, V1);

    V0 := XMVectorPermute(R1, R2, XM_PERMUTE_0Y, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_0Z);
    V1 := XMVectorPermute(R1, R2, XM_PERMUTE_0X, XM_PERMUTE_1Z, XM_PERMUTE_0X, XM_PERMUTE_1Z);


    Result.r[0] := XMVectorPermute(R0, V0, XM_PERMUTE_0X, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_0W);
    Result.r[1] := XMVectorPermute(R0, V0, XM_PERMUTE_1Z, XM_PERMUTE_0Y, XM_PERMUTE_1W, XM_PERMUTE_0W);
    Result.r[2] := XMVectorPermute(R0, V1, XM_PERMUTE_1X, XM_PERMUTE_1Y, XM_PERMUTE_0Z, XM_PERMUTE_0W);
    Result.r[3] := g_XMIdentityR3.v;
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixRotationQuaternion(Quaternion: TXMVECTOR): TXMMATRIX; assembler;
const
    Constant1110: TXMVECTORF32 = (f: (1.0, 1.0, 1.0, 0.0));
asm
           // Q0 = _mm_add_ps(Quaternion,Quaternion);
           MOVUPS  XMM0, [Quaternion]
           ADDPS   XMM0, XMM0  // Q0 = XMM0

           // Q1 = _mm_mul_ps(Quaternion,Q0);
           MOVUPS  XMM1, [Quaternion]
           MULPS   XMM1, XMM0  // Q1 = XMM1

           // V0 = XM_PERMUTE_PS(Q1,_MM_SHUFFLE(3,0,0,1));
           MOVUPS  XMM2, XMM1
           SHUFPS  XMM2, XMM1, _MM_SHUFFLE_3_0_0_1

           // V0 = _mm_and_ps(V0,g_XMMask3);
           ADDPS   XMM2, [g_XMMask3]  // V0 = XMM2

           // V1 = XM_PERMUTE_PS(Q1,_MM_SHUFFLE(3,1,2,2));
           MOVUPS  XMM3, XMM1
           SHUFPS  XMM3, XMM1, _MM_SHUFFLE_3_1_2_2

           // V1 = _mm_and_ps(V1,g_XMMask3);
           ADDPS   XMM3, [g_XMMask3]  // V1 = XMM3

           // R0 = _mm_sub_ps(Constant1110,V0);
           MOVUPS  XMM4, [Constant1110]
           SUBPS   XMM4, XMM2 // R0 = XMM4

           // R0 = _mm_sub_ps(R0, V1);
           SUBPS   XMM4, XMM3

           // V0 = XM_PERMUTE_PS(Quaternion,_MM_SHUFFLE(3,1,0,0));
           MOVUPS  XMM2, [Quaternion]
           SHUFPS  XMM2, [Quaternion] , _MM_SHUFFLE_3_1_0_0

           // V1 = XM_PERMUTE_PS(Q0,_MM_SHUFFLE(3,2,1,2));
           MOVUPS  XMM3, XMM1
           SHUFPS  XMM3, XMM1 , _MM_SHUFFLE_3_2_1_2

           // V0 = _mm_mul_ps(V0, V1);
           MULPS   XMM2, XMM3 // V0 = XMM2

           // V1 = XM_PERMUTE_PS(Quaternion,_MM_SHUFFLE(3,3,3,3));
           MOVUPS  XMM3, [Quaternion]
           SHUFPS  XMM3, [Quaternion] , _MM_SHUFFLE_3_3_3_3

           // V2 = XM_PERMUTE_PS(Q0,_MM_SHUFFLE(3,0,2,1));
           MOVUPS  XMM5, XMM0
           SHUFPS  XMM5, XMM0 , _MM_SHUFFLE_3_0_2_1

           // V1 = _mm_mul_ps(V1, V2);
           MULPS   XMM3, XMM5

           // R1 = _mm_add_ps(V0, V1);
           MOVUPS  XMM6, XMM2
           ADDPS   XMM6, XMM3 // R1 = XMM6

           // R2 = _mm_sub_ps(V0, V1);
           MOVUPS  XMM7, XMM2
           SUBPS   XMM7, XMM2 // R2 = XMM7

           // V0 = _mm_shuffle_ps(R1,R2,_MM_SHUFFLE(1,0,2,1));
           MOVUPS  XMM2, XMM6
           SHUFPS  XMM2, XMM7, _MM_SHUFFLE_1_0_2_1 // V0 = XMM2

           // V0 = XM_PERMUTE_PS(V0,_MM_SHUFFLE(1,3,2,0));
           SHUFPS  XMM2, XMM2, _MM_SHUFFLE_1_3_2_0

           // V1 = _mm_shuffle_ps(R1,R2,_MM_SHUFFLE(2,2,0,0));
           MOVUPS  XMM3, XMM6
           SHUFPS  XMM3, XMM7, _MM_SHUFFLE_2_2_0_0 // V1 = XMM3

           // V1 = XM_PERMUTE_PS(V1,_MM_SHUFFLE(2,0,2,0));
           SHUFPS  XMM3, XMM3, _MM_SHUFFLE_2_0_2_0

           // Q1 = _mm_shuffle_ps(R0,V0,_MM_SHUFFLE(1,0,3,0));
           MOVUPS  XMM1,XMM4
           SHUFPS  XMM1, XMM2, _MM_SHUFFLE_1_0_3_0 // Q1 = XMM1

           // Q1 = XM_PERMUTE_PS(Q1,_MM_SHUFFLE(1,3,2,0));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_3_2_0

           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[0]),XMM1 // M.r[0] = Q1;

           // Q1 = _mm_shuffle_ps(R0,V0,_MM_SHUFFLE(3,2,3,1));
           MOVUPS  XMM1,XMM4
           SHUFPS  XMM1, XMM2, _MM_SHUFFLE_3_2_3_1 // Q1 = XMM1

           // Q1 = XM_PERMUTE_PS(Q1,_MM_SHUFFLE(1,3,0,2));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_3_0_2

           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[1]),XMM1 // M.r[1] = Q1;

           // Q1 = _mm_shuffle_ps(V1,R0,_MM_SHUFFLE(3,2,1,0));
           MOVUPS  XMM1,XMM3
           SHUFPS  XMM1, XMM4, _MM_SHUFFLE_3_2_1_0

           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]),XMM1 // M.r[2] = Q1;

           MOVUPS  XMM1,[g_XMIdentityR3]
           MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[3]),XMM1 // M.r[3] = g_XMIdentityR3;

end;
{$ENDIF}


function XMMatrixTransformation2D(ScalingOrigin: TXMVECTOR; ScalingOrientation: single; Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; Rotation: single; Translation: TXMVECTOR): TXMMATRIX;
var
    VScalingOrigin, NegScalingOrigin: TXMVECTOR;
    VScaling, VRotationOrigin, VTranslation: TXMVECTOR;
    MScalingOriginI, MScalingOrientation, MScalingOrientationT, MScaling, MRotation: TXMMATRIX;
begin
    // M := Inverse(MScalingOrigin) * Transpose(MScalingOrientation) * MScaling * MScalingOrientation *
    //         MScalingOrigin * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    VScalingOrigin := XMVectorSelect(g_XMSelect1100.v, ScalingOrigin, g_XMSelect1100.v);
    NegScalingOrigin := XMVectorNegate(VScalingOrigin);

    MScalingOriginI := XMMatrixTranslationFromVector(NegScalingOrigin);
    MScalingOrientation := XMMatrixRotationZ(ScalingOrientation);
    MScalingOrientationT := XMMatrixTranspose(MScalingOrientation);
    VScaling := XMVectorSelect(g_XMOne.v, Scaling, g_XMSelect1100.v);
    MScaling := XMMatrixScalingFromVector(VScaling);
    VRotationOrigin := XMVectorSelect(g_XMSelect1100.v, RotationOrigin, g_XMSelect1100.v);
    MRotation := XMMatrixRotationZ(Rotation);
    VTranslation := XMVectorSelect(g_XMSelect1100.v, Translation, g_XMSelect1100.v);

    Result := XMMatrixMultiply(MScalingOriginI, MScalingOrientationT);
    Result := XMMatrixMultiply(Result, MScaling);
    Result := XMMatrixMultiply(Result, MScalingOrientation);
    Result.r[3] := XMVectorAdd(Result.r[3], VScalingOrigin);
    Result.r[3] := XMVectorSubtract(Result.r[3], VRotationOrigin);
    Result := XMMatrixMultiply(Result, MRotation);
    Result.r[3] := XMVectorAdd(Result.r[3], VRotationOrigin);
    Result.r[3] := XMVectorAdd(Result.r[3], VTranslation);
end;



function XMMatrixTransformation(ScalingOrigin: TXMVECTOR; ScalingOrientationQuaternion: TXMVECTOR; Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; RotationQuaternion: TXMVECTOR; Translation: TXMVECTOR): TXMMATRIX;
var
    VScalingOrigin, NegScalingOrigin, VRotationOrigin, VTranslation: TXMVECTOR;
    MScalingOriginI, MScalingOrientation, MScalingOrientationT, MScaling, MRotation: TXMMATRIX;
begin
    // M := Inverse(MScalingOrigin) * Transpose(MScalingOrientation) * MScaling * MScalingOrientation *
    //         MScalingOrigin * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    VScalingOrigin := XMVectorSelect(g_XMSelect1110.v, ScalingOrigin, g_XMSelect1110.v);
    NegScalingOrigin := XMVectorNegate(ScalingOrigin);

    MScalingOriginI := XMMatrixTranslationFromVector(NegScalingOrigin);
    MScalingOrientation := XMMatrixRotationQuaternion(ScalingOrientationQuaternion);
    MScalingOrientationT := XMMatrixTranspose(MScalingOrientation);
    MScaling := XMMatrixScalingFromVector(Scaling);
    VRotationOrigin := XMVectorSelect(g_XMSelect1110.v, RotationOrigin, g_XMSelect1110.v);
    MRotation := XMMatrixRotationQuaternion(RotationQuaternion);
    VTranslation := XMVectorSelect(g_XMSelect1110.v, Translation, g_XMSelect1110.v);

    Result := XMMatrixMultiply(MScalingOriginI, MScalingOrientationT);
    Result := XMMatrixMultiply(Result, MScaling);
    Result := XMMatrixMultiply(Result, MScalingOrientation);
    Result.r[3] := XMVectorAdd(Result.r[3], VScalingOrigin);
    Result.r[3] := XMVectorSubtract(Result.r[3], VRotationOrigin);
    Result := XMMatrixMultiply(Result, MRotation);
    Result.r[3] := XMVectorAdd(Result.r[3], VRotationOrigin);
    Result.r[3] := XMVectorAdd(Result.r[3], VTranslation);
end;



function XMMatrixAffineTransformation2D(Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; Rotation: single; Translation: TXMVECTOR): TXMMATRIX;
var
    VScaling, VRotationOrigin, VTranslation: TXMVECTOR;
    MScaling, MRotation: TXMMATRIX;
begin
    // M := MScaling * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    VScaling := XMVectorSelect(g_XMOne.v, Scaling, g_XMSelect1100.v);
    MScaling := XMMatrixScalingFromVector(VScaling);
    VRotationOrigin := XMVectorSelect(g_XMSelect1100.v, RotationOrigin, g_XMSelect1100.v);
    MRotation := XMMatrixRotationZ(Rotation);
    VTranslation := XMVectorSelect(g_XMSelect1100.v, Translation, g_XMSelect1100.v);

    Result := MScaling;
    Result.r[3] := XMVectorSubtract(Result.r[3], VRotationOrigin);
    Result := XMMatrixMultiply(Result, MRotation);
    Result.r[3] := XMVectorAdd(Result.r[3], VRotationOrigin);
    Result.r[3] := XMVectorAdd(Result.r[3], VTranslation);
end;



function XMMatrixAffineTransformation(Scaling: TXMVECTOR; RotationOrigin: TXMVECTOR; RotationQuaternion: TXMVECTOR; Translation: TXMVECTOR): TXMMATRIX;
var
    MScaling, MRotation: TXMMATRIX;
    VRotationOrigin, VTranslation: TXMVECTOR;
begin
    // M := MScaling * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    MScaling := XMMatrixScalingFromVector(Scaling);
    VRotationOrigin := XMVectorSelect(g_XMSelect1110.v, RotationOrigin, g_XMSelect1110.v);
    MRotation := XMMatrixRotationQuaternion(RotationQuaternion);
    VTranslation := XMVectorSelect(g_XMSelect1110.v, Translation, g_XMSelect1110.v);

    Result := MScaling;
    Result.r[3] := XMVectorSubtract(Result.r[3], VRotationOrigin);
    Result := XMMatrixMultiply(Result, MRotation);
    Result.r[3] := XMVectorAdd(Result.r[3], VRotationOrigin);
    Result.r[3] := XMVectorAdd(Result.r[3], VTranslation);
end;



function XMMatrixReflect(ReflectionPlane: TXMVECTOR): TXMMATRIX;
const
    NegativeTwo: TXMVECTOR = (f32: (-2.0, -2.0, -2.0, 0.0));
var
    P, S, A, B, C, D: TXMVECTOR;

begin
    assert(not XMVector3Equal(ReflectionPlane, XMVectorZero()));
    assert(not XMPlaneIsInfinite(ReflectionPlane));



    P := XMPlaneNormalize(ReflectionPlane);
    S := XMVectorMultiply(P, NegativeTwo);

    A := XMVectorSplatX(P);
    B := XMVectorSplatY(P);
    C := XMVectorSplatZ(P);
    D := XMVectorSplatW(P);

    Result.r[0] := XMVectorMultiplyAdd(A, S, g_XMIdentityR0.v);
    Result.r[1] := XMVectorMultiplyAdd(B, S, g_XMIdentityR1.v);
    Result.r[2] := XMVectorMultiplyAdd(C, S, g_XMIdentityR2.v);
    Result.r[3] := XMVectorMultiplyAdd(D, S, g_XMIdentityR3.v);
end;



function XMMatrixShadow(ShadowPlane: TXMVECTOR; LightPosition: TXMVECTOR): TXMMATRIX;

var
    P, Dot, A, B, C, D, Select0001: TXMVECTOR;
begin

    Select0001.u32[0] := XM_SELECT_0;
    Select0001.u32[1] := XM_SELECT_0;
    Select0001.u32[2] := XM_SELECT_0;
    Select0001.u32[3] := XM_SELECT_1;


    assert(not XMVector3Equal(ShadowPlane, XMVectorZero()));
    assert(not XMPlaneIsInfinite(ShadowPlane));

    P := XMPlaneNormalize(ShadowPlane);
    Dot := XMPlaneDot(P, LightPosition);
    P := XMVectorNegate(P);
    D := XMVectorSplatW(P);
    C := XMVectorSplatZ(P);
    B := XMVectorSplatY(P);
    A := XMVectorSplatX(P);
    Dot := XMVectorSelect(Select0001, Dot, Select0001);

    Result.r[3] := XMVectorMultiplyAdd(D, LightPosition, Dot);
    Dot := XMVectorRotateLeft(Dot, 1);
    Result.r[2] := XMVectorMultiplyAdd(C, LightPosition, Dot);
    Dot := XMVectorRotateLeft(Dot, 1);
    Result.r[1] := XMVectorMultiplyAdd(B, LightPosition, Dot);
    Dot := XMVectorRotateLeft(Dot, 1);
    Result.r[0] := XMVectorMultiplyAdd(A, LightPosition, Dot);
end;

//------------------------------------------------------------------------------
// View and projection initialization operations
//------------------------------------------------------------------------------


function XMMatrixLookAtLH(EyePosition: TXMVECTOR; FocusPosition: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
var
    EyeDirection: TXMVECTOR;
begin
    EyeDirection := XMVectorSubtract(FocusPosition, EyePosition);
    Result := XMMatrixLookToLH(EyePosition, EyeDirection, UpDirection);
end;



function XMMatrixLookAtRH(EyePosition: TXMVECTOR; FocusPosition: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
var
    NegEyeDirection: TXMVECTOR;
begin
    NegEyeDirection := XMVectorSubtract(EyePosition, FocusPosition);
    Result := XMMatrixLookToLH(EyePosition, NegEyeDirection, UpDirection);

end;



function XMMatrixLookToLH(EyePosition: TXMVECTOR; EyeDirection: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
var
    R0, R1, R2, NegEyePosition: TXMVECTOR;
    D0, D1, D2: TXMVECTOR;
begin
    assert(not XMVector3Equal(EyeDirection, XMVectorZero()));
    assert(not XMVector3IsInfinite(EyeDirection));
    assert(not XMVector3Equal(UpDirection, XMVectorZero()));
    assert(not XMVector3IsInfinite(UpDirection));

    R2 := XMVector3Normalize(EyeDirection);

    R0 := XMVector3Cross(UpDirection, R2);
    R0 := XMVector3Normalize(R0);

    R1 := XMVector3Cross(R2, R0);

    NegEyePosition := XMVectorNegate(EyePosition);

    D0 := XMVector3Dot(R0, NegEyePosition);
    D1 := XMVector3Dot(R1, NegEyePosition);
    D2 := XMVector3Dot(R2, NegEyePosition);

    Result.r[0] := XMVectorSelect(D0, R0, g_XMSelect1110.v);
    Result.r[1] := XMVectorSelect(D1, R1, g_XMSelect1110.v);
    Result.r[2] := XMVectorSelect(D2, R2, g_XMSelect1110.v);
    Result.r[3] := g_XMIdentityR3.v;

    Result := XMMatrixTranspose(Result);
end;



function XMMatrixLookToRH(EyePosition: TXMVECTOR; EyeDirection: TXMVECTOR; UpDirection: TXMVECTOR): TXMMATRIX;
var
    NegEyeDirection: TXMVECTOR;
begin
    NegEyeDirection := XMVectorNegate(EyeDirection);
    Result := XMMatrixLookToLH(EyePosition, NegEyeDirection, UpDirection);
end;

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixPerspectiveLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, fRange: single;
begin
    assert((NearZ > 0.0) and (FarZ > 0.0));
    assert(not XMScalarNearEqual(ViewWidth, 0.0, 0.00001));
    assert(not XMScalarNearEqual(ViewHeight, 0.0, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));
    TwoNearZ := NearZ + NearZ;
    fRange := FarZ / (FarZ - NearZ);

    Result.m[0, 0] := TwoNearZ / ViewWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := TwoNearZ / ViewHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := 1.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := -fRange * NearZ;
    Result.m[3, 3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixPerspectiveLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
    float TwoNearZ = NearZ + NearZ;
    float fRange = FarZ / (FarZ - NearZ);
    const XMVECTOR Zero = vdupq_n_f32(0);
    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( TwoNearZ / ViewWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( TwoNearZ / ViewHeight, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, g_XMIdentityR3.v, 2 );
    M.r[3] = vsetq_lane_f32( -fRange * NearZ, Zero, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixPerspectiveLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, fRange: single;
    rMem: TXMVECTOR;
begin
    TwoNearZ := NearZ + NearZ;
    fRange := FarZ / (FarZ - NearZ);
    // Note: This is recorded on the stack
    rMem.Create(TwoNearZ / ViewWidth,
        TwoNearZ / ViewHeight,
        fRange, -fRange * NearZ);
    asm

               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // TwoNearZ / ViewWidth,0,0,0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;

               // 0,TwoNearZ / ViewHeight,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               MOVUPS  XMM2, [g_XMMaskY]
               ANDPS   XMM0,XMM2 // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;

               // x=fRange,y=-fRange * NearZ,0,1.0f
               MOVSS   XMM1, [rMem]
               MOVUPS  XMM2, [g_XMIdentityR3]
               SHUFPS  XMM1, XMM2, _MM_SHUFFLE_3_2_3_2 // vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,_MM_SHUFFLE(3,2,3,2));
               // 0,0,fRange,1.0f
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_0_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(3,0,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[2]),XMM0 // M.r[2] = vTemp;
               // 0,0,-fRange * NearZ,0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_2_1_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(2,1,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vTemp;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixPerspectiveRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, fRange: single;
begin
    assert((NearZ > 0.0) and (FarZ > 0.0));
    assert(not XMScalarNearEqual(ViewWidth, 0.0, 0.00001));
    assert(not XMScalarNearEqual(ViewHeight, 0.0, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    TwoNearZ := NearZ + NearZ;
    fRange := FarZ / (NearZ - FarZ);


    Result.m[0, 0] := TwoNearZ / ViewWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := TwoNearZ / ViewHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := -1.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := fRange * NearZ;
    Result.m[3, 3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixPerspectiveRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
     float TwoNearZ = NearZ + NearZ;
    float fRange = FarZ / (NearZ - FarZ);
    const XMVECTOR Zero = vdupq_n_f32(0);

    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( TwoNearZ / ViewWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( TwoNearZ / ViewHeight, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, g_XMNegIdentityR3.v, 2 );
    M.r[3] = vsetq_lane_f32( fRange * NearZ, Zero, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixPerspectiveRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, fRange: single;
    rMem: TXMVECTOR;
begin
    TwoNearZ := NearZ + NearZ;
    fRange := FarZ / (NearZ - FarZ);
    // Note: This is recorded on the stack
    rMem.Create(TwoNearZ / ViewWidth,
        TwoNearZ / ViewHeight,
        fRange,
        fRange * NearZ);
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // TwoNearZ / ViewWidth,0,0,0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,TwoNearZ / ViewHeight,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               MOVUPS  XMM2, [g_XMMaskY]
               ANDPS   XMM0,XMM2 // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
               // x=fRange,y=-fRange * NearZ,0,-1.0f
               MOVSS   XMM1, [rMem]
               MOVUPS  XMM2, [g_XMNegIdentityR3]
               SHUFPS  XMM1, XMM2, _MM_SHUFFLE_3_2_3_2 // vValues = _mm_shuffle_ps(vValues,g_XMNegIdentityR3,_MM_SHUFFLE(3,2,3,2));
               // 0,0,fRange,-1.0f
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_0_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(3,0,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[2]),XMM0 // M.r[2] = vTemp;
               // 0,0,-fRange * NearZ,0
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_2_1_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(2,1,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vTemp;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixPerspectiveFovLH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    SinFov: single;
    CosFov: single;
    Height, Width, fRange: single;
begin
    assert((NearZ > 0.0) and (FarZ > 0.0));
    assert(not XMScalarNearEqual(FovAngleY, 0.0, 0.00001 * 2.0));
    assert(not XMScalarNearEqual(AspectRatio, 0.0, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    XMScalarSinCos(SinFov, CosFov, 0.5 * FovAngleY);

    Height := CosFov / SinFov;
    Width := Height / AspectRatio;
    fRange := FarZ / (FarZ - NearZ);


    Result.m[0, 0] := Width;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := Height;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := 1.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := -fRange * NearZ;
    Result.m[3, 3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixPerspectiveFovLH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
    float    SinFov;
    float    CosFov;
    XMScalarSinCos(&SinFov, &CosFov, 0.5f * FovAngleY);

    float fRange = FarZ / (FarZ-NearZ);
    float Height = CosFov / SinFov;
    float Width = Height / AspectRatio;
    const XMVECTOR Zero = vdupq_n_f32(0);

    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( Width, Zero, 0 );
    M.r[1] = vsetq_lane_f32( Height, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, g_XMIdentityR3.v, 2 );
    M.r[3] = vsetq_lane_f32( -fRange * NearZ, Zero, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixPerspectiveFovLH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    SinFov: single;
    CosFov: single;
    TwoNearZ, fRange, Height: single;
    rMem: TXMVECTOR;
begin
    XMScalarSinCos(SinFov, CosFov, 0.5 * FovAngleY);

    fRange := FarZ / (FarZ - NearZ);
    // Note: This is recorded on the stack
    Height := CosFov / SinFov;
    rMem.Create(
        Height / AspectRatio,
        Height,
        fRange, -fRange * NearZ
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // CosFov / SinFov,0,0,0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,Height / AspectRatio,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               MOVUPS  XMM2, [g_XMMaskY]
               ANDPS   XMM0,XMM2 // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;

               // x=fRange,y=-fRange * NearZ,0,1.0f
               MOVSS   XMM1, [rMem]
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               MOVUPS  XMM2, [g_XMIdentityR3]
               SHUFPS  XMM1, XMM2, _MM_SHUFFLE_3_2_3_2 // vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,_MM_SHUFFLE(3,2,3,2));
               // 0,0,fRange,1.0f
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_0_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(3,0,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[2]),XMM0 // M.r[2] = vTemp;
               // 0,0,-fRange * NearZ,0.0f
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_2_1_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(2,1,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vTemp;
    end;
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixPerspectiveFovRH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    SinFov: single;
    CosFov: single;
    Height, Width, fRange: single;
begin
    assert((NearZ > 0.0) and (FarZ > 0.0));
    assert(not XMScalarNearEqual(FovAngleY, 0.0, 0.00001 * 2.0));
    assert(not XMScalarNearEqual(AspectRatio, 0.0, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    XMScalarSinCos(SinFov, CosFov, 0.5 * FovAngleY);

    Height := CosFov / SinFov;
    Width := Height / AspectRatio;
    fRange := FarZ / (NearZ - FarZ);


    Result.m[0, 0] := Width;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := Height;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := -1.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := fRange * NearZ;
    Result.m[3, 3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixPerspectiveFovRH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
     float    SinFov;
    float    CosFov;
    XMScalarSinCos(&SinFov, &CosFov, 0.5f * FovAngleY);
    float fRange = FarZ / (NearZ-FarZ);
    float Height = CosFov / SinFov;
    float Width = Height / AspectRatio;
    const XMVECTOR Zero = vdupq_n_f32(0);

    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( Width, Zero, 0 );
    M.r[1] = vsetq_lane_f32( Height, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, g_XMNegIdentityR3.v, 2 );
    M.r[3] = vsetq_lane_f32( fRange * NearZ, Zero, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixPerspectiveFovRH(FovAngleY: single; AspectRatio: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    SinFov, CosFov, fRange, Height: single;
    rMem: TXMVECTOR;
begin
    XMScalarSinCos(SinFov, CosFov, 0.5 * FovAngleY);
    fRange := FarZ / (NearZ - FarZ);
    // Note: This is recorded on the stack
    Height := CosFov / SinFov;
    rMem.Create(
        Height / AspectRatio,
        Height,
        fRange,
        fRange * NearZ
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // CosFov / SinFov,0,0,0

               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,Height / AspectRatio,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               MOVUPS  XMM2, [g_XMMaskY]
               ANDPS   XMM0,XMM2 // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
               // x=fRange,y=-fRange * NearZ,0,-1.0f
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               MOVSS   XMM1, [rMem]
               MOVUPS  XMM2, [g_XMNegIdentityR3]
               SHUFPS  XMM1, XMM2, _MM_SHUFFLE_3_2_3_2 // vValues = _mm_shuffle_ps(vValues,g_XMNegIdentityR3,_MM_SHUFFLE(3,2,3,2));
               // 0,0,fRange,-1.0f
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_0_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(3,0,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[2]),XMM0 // M.r[2] = vTemp;
               // 0,0,fRange * NearZ,0.0f
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_2_1_0_0 // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(2,1,0,0));
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vTemp;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixPerspectiveOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, ReciprocalWidth, ReciprocalHeight, fRange: single;
begin
    assert((NearZ > 0.0) and (FarZ > 0.0));
    assert(not XMScalarNearEqual(ViewRight, ViewLeft, 0.00001));
    assert(not XMScalarNearEqual(ViewTop, ViewBottom, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    TwoNearZ := NearZ + NearZ;
    ReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    ReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := FarZ / (FarZ - NearZ);


    Result.m[0, 0] := TwoNearZ * ReciprocalWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := TwoNearZ * ReciprocalHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := -(ViewLeft + ViewRight) * ReciprocalWidth;
    Result.m[2, 1] := -(ViewTop + ViewBottom) * ReciprocalHeight;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := 1.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := -fRange * NearZ;
    Result.m[3, 3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixPerspectiveOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
     float TwoNearZ = NearZ + NearZ;
    float ReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float ReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = FarZ / (FarZ-NearZ);
    const XMVECTOR Zero = vdupq_n_f32(0);

    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( TwoNearZ * ReciprocalWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( TwoNearZ * ReciprocalHeight, Zero, 1 );
    M.r[2] = XMVectorSet(-(ViewLeft + ViewRight) * ReciprocalWidth,
                         -(ViewTop + ViewBottom) * ReciprocalHeight,
                         fRange,
                         1.0f);
    M.r[3] = vsetq_lane_f32( -fRange * NearZ, Zero, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixPerspectiveOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, fRange, ReciprocalWidth, ReciprocalHeight: single;
    rMem: TXMVECTOR;
begin

    TwoNearZ := NearZ + NearZ;
    ReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    ReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := FarZ / (FarZ - NearZ);
    // Note: This is recorded on the stack
    rMem.Create(
        TwoNearZ * ReciprocalWidth,
        TwoNearZ * ReciprocalHeight, -fRange * NearZ,
        0
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // TwoNearZ*ReciprocalWidth,0,0,0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,TwoNearZ*ReciprocalHeight,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               MOVUPS  XMM2, [g_XMMaskY]
               ANDPS   XMM0,XMM2 // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
               // 0,0,fRange,1.0f
    end;
    Result.r[2] := XMVectorSet(-(ViewLeft + ViewRight) * ReciprocalWidth, -(ViewTop + ViewBottom) * ReciprocalHeight, fRange, 1.0);
    asm
               // 0,0,-fRange * NearZ,0.0f
               MOVSS   XMM1, [rMem]
               ANDPS   XMM1,[g_XMMaskZ] // vValues = _mm_and_ps(vValues,g_XMMaskZ);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM1 // M.r[3] = vValues;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixPerspectiveOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, ReciprocalWidth, ReciprocalHeight, fRange: single;
begin
    assert((NearZ > 0.0) and (FarZ > 0.0));
    assert(not XMScalarNearEqual(ViewRight, ViewLeft, 0.00001));
    assert(not XMScalarNearEqual(ViewTop, ViewBottom, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    TwoNearZ := NearZ + NearZ;
    ReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    ReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := FarZ / (NearZ - FarZ);


    Result.m[0, 0] := TwoNearZ * ReciprocalWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := TwoNearZ * ReciprocalHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := (ViewLeft + ViewRight) * ReciprocalWidth;
    Result.m[2, 1] := (ViewTop + ViewBottom) * ReciprocalHeight;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := -1.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := fRange * NearZ;
    Result.m[3, 3] := 0.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixPerspectiveOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
    float TwoNearZ = NearZ + NearZ;
    float ReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float ReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = FarZ / (NearZ-FarZ);
    const XMVECTOR Zero = vdupq_n_f32(0);

    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( TwoNearZ * ReciprocalWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( TwoNearZ * ReciprocalHeight, Zero, 1 );
    M.r[2] = XMVectorSet((ViewLeft + ViewRight) * ReciprocalWidth,
                         (ViewTop + ViewBottom) * ReciprocalHeight,
                         fRange,
                         -1.0f);
    M.r[3] = vsetq_lane_f32( fRange * NearZ, Zero, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixPerspectiveOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    TwoNearZ, fRange, ReciprocalWidth, ReciprocalHeight: single;
    rMem: TXMVECTOR;
begin

    TwoNearZ := NearZ + NearZ;
    ReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    ReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := FarZ / (NearZ - FarZ);
    // Note: This is recorded on the stack
    rMem.Create(
        TwoNearZ * ReciprocalWidth,
        TwoNearZ * ReciprocalHeight,
        fRange * NearZ,
        0
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // TwoNearZ*ReciprocalWidth,0,0,0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,TwoNearZ*ReciprocalHeight,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               ANDPS   XMM0,[g_XMMaskY] // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
    end;
    // 0,0,fRange,1.0f
    Result.r[2] := XMVectorSet((ViewLeft + ViewRight) * ReciprocalWidth, (ViewTop + ViewBottom) * ReciprocalHeight, fRange, -1.0);
    // 0,0,-fRange * NearZ,0.0f
    asm
               MOVSS   XMM0, [rMem]
               ANDPS   XMM0 , [g_XMMaskZ] // vValues = _mm_and_ps(vValues,g_XMMaskZ);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vValues;
    end;
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixOrthographicLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    fRange: single;
begin
    assert(not XMScalarNearEqual(ViewWidth, 0.0, 0.00001));
    assert(not XMScalarNearEqual(ViewHeight, 0.0, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    fRange := 1.0 / (FarZ - NearZ);


    Result.m[0, 0] := 2.0 / ViewWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := 2.0 / ViewHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := -fRange * NearZ;
    Result.m[3, 3] := 1.0;

end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixOrthographicLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
     float fRange = 1.0f / (FarZ-NearZ);

    const XMVECTOR Zero = vdupq_n_f32(0);
    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( 2.0f / ViewWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( 2.0f / ViewHeight, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, Zero, 2 );
    M.r[3] = vsetq_lane_f32( -fRange * NearZ, g_XMIdentityR3.v, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixOrthographicLH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    fRange: single;
    rMem: TXMVECTOR;
begin

    fRange := 1.0 / (FarZ - NearZ);
    // Note: This is recorded on the stack
    rMem.Create(
        2.0 / ViewWidth,
        2.0 / ViewHeight,
        fRange, -fRange * NearZ
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // 2.0f / ViewWidth,0,0,0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,2.0f / ViewHeight,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               ANDPS   XMM0,[g_XMMaskY] // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
               // x=fRange,y=-fRange * NearZ,0,1.0f
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,_MM_SHUFFLE(3,2,3,2));
               MOVSS   XMM1, [rMem]
               SHUFPS  XMM1, [g_XMIdentityR3], _MM_SHUFFLE_3_2_3_2

               // 0,0,fRange,0.0f
               // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(2,0,0,0));
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_2_0_0_0
               MOVUPS  TXMVECTOR(TXMMATRIX([result]).r[2]),XMM1 //M.r[2] = vTemp;
               // 0,0,-fRange * NearZ,1.0f
               // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(3,1,0,0));
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_1_0_0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vTemp;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixOrthographicRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    fRange: single;
begin
    assert(not XMScalarNearEqual(ViewWidth, 0.0, 0.00001));
    assert(not XMScalarNearEqual(ViewHeight, 0.0, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    fRange := 1.0 / (NearZ - FarZ);


    Result.m[0, 0] := 2.0 / ViewWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := 2.0 / ViewHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := 0.0;
    Result.m[3, 1] := 0.0;
    Result.m[3, 2] := fRange * NearZ;
    Result.m[3, 3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixOrthographicRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
     float fRange = 1.0f / (NearZ-FarZ);

    const XMVECTOR Zero = vdupq_n_f32(0);
    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( 2.0f / ViewWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( 2.0f / ViewHeight, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, Zero, 2 );
    M.r[3] = vsetq_lane_f32( fRange * NearZ, g_XMIdentityR3.v, 2 );
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixOrthographicRH(ViewWidth: single; ViewHeight: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    fRange: single;
    rMem: TXMVECTOR;
begin

    fRange := 1.0 / (NearZ - FarZ);
    // Note: This is recorded on the stack
    rMem.Create(
        2.0 / ViewWidth,
        2.0 / ViewHeight,
        fRange,
        fRange * NearZ
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // 2.0f / ViewWidth,0,0,0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,2.0f / ViewHeight,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               ANDPS   XMM0,[g_XMMaskY] // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
               // x=fRange,y=fRange * NearZ,0,1.0f
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,_MM_SHUFFLE(3,2,3,2));
               MOVSS   XMM1, [rMem]
               SHUFPS  XMM1, [g_XMIdentityR3], _MM_SHUFFLE_3_2_3_2

               // 0,0,fRange,0.0f
               // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(2,0,0,0));
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_2_0_0_0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[2]),XMM0 // M.r[2] = vTemp;
               // 0,0,fRange * NearZ,1.0f
               // vTemp = _mm_shuffle_ps(vTemp,vValues,_MM_SHUFFLE(3,1,0,0));
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_1_0_0
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vTemp;
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixOrthographicOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    ReciprocalWidth, ReciprocalHeight, fRange: single;
begin
    assert(not XMScalarNearEqual(ViewRight, ViewLeft, 0.00001));
    assert(not XMScalarNearEqual(ViewTop, ViewBottom, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    ReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    ReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := 1.0 / (FarZ - NearZ);


    Result.m[0, 0] := ReciprocalWidth + ReciprocalWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := ReciprocalHeight + ReciprocalHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := 0.0;

    Result.m[3, 0] := -(ViewLeft + ViewRight) * ReciprocalWidth;
    Result.m[3, 1] := -(ViewTop + ViewBottom) * ReciprocalHeight;
    Result.m[3, 2] := -fRange * NearZ;
    Result.m[3, 3] := 1.0;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixOrthographicOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
    float ReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float ReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = 1.0f / (FarZ-NearZ);
    const XMVECTOR Zero = vdupq_n_f32(0);
    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( ReciprocalWidth + ReciprocalWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( ReciprocalHeight + ReciprocalHeight, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, Zero, 2 );
    M.r[3] = XMVectorSet(-(ViewLeft + ViewRight) * ReciprocalWidth,
                         -(ViewTop + ViewBottom) * ReciprocalHeight,
                         -fRange * NearZ,
                         1.0f);
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixOrthographicOffCenterLH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    fReciprocalWidth, fReciprocalHeight, fRange: single;
    rMem, rMem2: TXMVECTOR;
begin
    fReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    fReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := 1.0 / (FarZ - NearZ);
    // Note: This is recorded on the stack
    rMem.Create(
        fReciprocalWidth,
        fReciprocalHeight,
        fRange,
        1.0
        );
    rMem2.Create(-(ViewLeft + ViewRight), -(ViewTop + ViewBottom), -NearZ,
        1.0
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // fReciprocalWidth*2,0,0,0
               ADDPS   XMM0,XMM0 // vTemp = _mm_add_ss(vTemp,vTemp);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,fReciprocalHeight*2,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               ADDPS   XMM0, [g_XMMaskY] // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               ADDPS   XMM0,XMM0 // vTemp = _mm_add_ps(vTemp,vTemp);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
               // 0,0,fRange,0.0f
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               ANDPS   XMM0, [g_XMMaskZ] // vTemp = _mm_and_ps(vTemp,g_XMMaskZ);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[2]),XMM0 // M.r[2] = vTemp;
               // -(ViewLeft + ViewRight)*fReciprocalWidth,-(ViewTop + ViewBottom)*fReciprocalHeight,fRange*-NearZ,1.0f
               // vValues = _mm_mul_ps(vValues,rMem2);
               MOVSS   XMM0, [rMem]
               MULPS   XMM0, [rMem2]
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vValues;
    end;
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMMatrixOrthographicOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    ReciprocalWidth, ReciprocalHeight, fRange: single;
begin
    assert(not XMScalarNearEqual(ViewRight, ViewLeft, 0.00001));
    assert(not XMScalarNearEqual(ViewTop, ViewBottom, 0.00001));
    assert(not XMScalarNearEqual(FarZ, NearZ, 0.00001));

    ReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    ReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := 1.0 / (NearZ - FarZ);


    Result.m[0, 0] := ReciprocalWidth + ReciprocalWidth;
    Result.m[0, 1] := 0.0;
    Result.m[0, 2] := 0.0;
    Result.m[0, 3] := 0.0;

    Result.m[1, 0] := 0.0;
    Result.m[1, 1] := ReciprocalHeight + ReciprocalHeight;
    Result.m[1, 2] := 0.0;
    Result.m[1, 3] := 0.0;

    Result.m[2, 0] := 0.0;
    Result.m[2, 1] := 0.0;
    Result.m[2, 2] := fRange;
    Result.m[2, 3] := 0.0;

    Result.r[3] := XMVectorSet(-(ViewLeft + ViewRight) * ReciprocalWidth, -(ViewTop + ViewBottom) * ReciprocalHeight, fRange * NearZ, 1.0);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMMatrixOrthographicOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
begin
    (* ToDo
     float ReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float ReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = 1.0f / (NearZ-FarZ);
    const XMVECTOR Zero = vdupq_n_f32(0);
    XMMATRIX M;
    M.r[0] = vsetq_lane_f32( ReciprocalWidth + ReciprocalWidth, Zero, 0 );
    M.r[1] = vsetq_lane_f32( ReciprocalHeight + ReciprocalHeight, Zero, 1 );
    M.r[2] = vsetq_lane_f32( fRange, Zero, 2 );
    M.r[3] = XMVectorSet(-(ViewLeft + ViewRight) * ReciprocalWidth,
                         -(ViewTop + ViewBottom) * ReciprocalHeight,
                         fRange * NearZ,
                         1.0f);
    return M;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMMatrixOrthographicOffCenterRH(ViewLeft: single; ViewRight: single; ViewBottom: single; ViewTop: single; NearZ: single; FarZ: single): TXMMATRIX;
var
    fReciprocalWidth, fReciprocalHeight, fRange: single;
    rMem, rMem2: TXMVECTOR;
begin

    fReciprocalWidth := 1.0 / (ViewRight - ViewLeft);
    fReciprocalHeight := 1.0 / (ViewTop - ViewBottom);
    fRange := 1.0 / (NearZ - FarZ);
    // Note: This is recorded on the stack
    rMem.Create(
        fReciprocalWidth,
        fReciprocalHeight,
        fRange,
        1.0
        );
    rMem2.Create(-(ViewLeft + ViewRight), -(ViewTop + ViewBottom),
        NearZ,
        1.0
        );
    asm
               // Copy from memory to SSE register
               XORPS   XMM0, XMM0 // vTemp = _mm_setzero_ps();
               // Copy x only
               MOVSS   XMM0, [rMem] // vTemp = _mm_move_ss(vTemp,vValues);
               // fReciprocalWidth*2,0,0,0
               ADDPS   XMM0,XMM0 // vTemp = _mm_add_ss(vTemp,vTemp);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[0]),XMM0 // M.r[0] = vTemp;
               // 0,fReciprocalHeight*2,0,0
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               ADDPS   XMM0, [g_XMMaskY] // vTemp = _mm_and_ps(vTemp,g_XMMaskY);
               ADDPS   XMM0,XMM0 // vTemp = _mm_add_ps(vTemp,vTemp);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[1]),XMM0 // M.r[1] = vTemp;
               // 0,0,fRange,0.0f
               MOVSS   XMM0, [rMem] // vTemp = vValues;
               ANDPS   XMM0, [g_XMMaskZ] // vTemp = _mm_and_ps(vTemp,g_XMMaskZ);
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[2]),XMM0 // M.r[2] = vTemp;
               // -(ViewLeft + ViewRight)*fReciprocalWidth,-(ViewTop + ViewBottom)*fReciprocalHeight,fRange*-NearZ,1.0f
               // vValues = _mm_mul_ps(vValues,rMem2);
               MOVSS   XMM0, [rMem]
               MULPS   XMM0, [rMem2]
               MOVUPS  TXMVECTOR(TXMMATRIX(result).r[3]),XMM0 // M.r[3] = vValues;
    end;
end;

{$ENDIF}
//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------
function XMQuaternionEqual(Q1: TXMVECTOR; Q2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4Equal(Q1, Q2);
end;


function XMQuaternionNotEqual(Q1: TXMVECTOR; Q2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4NotEqual(Q1, Q2);
end;


function XMQuaternionIsNaN(Q: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4IsNaN(Q);
end;


function XMQuaternionIsInfinite(Q: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4IsInfinite(Q);
end;


function XMQuaternionIsIdentity(Q: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4Equal(Q, g_XMIdentityR3.v);
end;


//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------
function XMQuaternionDot(Q1: TXMVECTOR; Q2: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVector4Dot(Q1, Q2);
end;



// Returns the product Q2*Q1 (which is the concatenation of a rotation Q1 followed by the rotation Q2)

// [ (Q2.w * Q1.x) + (Q2.x * Q1.w) + (Q2.y * Q1.z) - (Q2.z * Q1.y),
//   (Q2.w * Q1.y) - (Q2.x * Q1.z) + (Q2.y * Q1.w) + (Q2.z * Q1.x),
//   (Q2.w * Q1.z) + (Q2.x * Q1.y) - (Q2.y * Q1.x) + (Q2.z * Q1.w),
//   (Q2.w * Q1.w) - (Q2.x * Q1.x) - (Q2.y * Q1.y) - (Q2.z * Q1.z) ]


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMQuaternionMultiply(Q1: TXMVECTOR; Q2: TXMVECTOR): TXMVECTOR; inline;
begin
    Result.f32[0] := (Q2.f32[3] * Q1.f32[0]) + (Q2.f32[0] * Q1.f32[3]) + (Q2.f32[1] * Q1.f32[2]) - (Q2.f32[2] * Q1.f32[1]);
    Result.f32[1] := (Q2.f32[3] * Q1.f32[1]) - (Q2.f32[0] * Q1.f32[2]) + (Q2.f32[1] * Q1.f32[3]) + (Q2.f32[2] * Q1.f32[0]);
    Result.f32[2] := (Q2.f32[3] * Q1.f32[2]) + (Q2.f32[0] * Q1.f32[1]) - (Q2.f32[1] * Q1.f32[0]) + (Q2.f32[2] * Q1.f32[3]);
    Result.f32[3] := (Q2.f32[3] * Q1.f32[3]) - (Q2.f32[0] * Q1.f32[0]) - (Q2.f32[1] * Q1.f32[1]) - (Q2.f32[2] * Q1.f32[2]);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMQuaternionMultiply(Q1: TXMVECTOR; Q2: TXMVECTOR): TXMVECTOR;
const
    ControlWZYX: TXMVECTORF32 = (f: (1.0, -1.0, 1.0, -1.0));
    ControlZWXY: TXMVECTORF32 = (f: (1.0, 1.0, -1.0, -1.0));
    ControlYXWZ: TXMVECTORF32 = (f: (-1.0, 1.0, 1.0, -1.0));
begin
 (* ToDo

    float32x2_t Q2L := vget_low_f32(Q2);
    float32x2_t Q2H := vget_high_f32(Q2);

    float32x4_t Q2X := vdupq_lane_f32( Q2L, 0 );
    float32x4_t Q2Y := vdupq_lane_f32( Q2L, 1 );
    float32x4_t Q2Z := vdupq_lane_f32( Q2H, 0 );
    XMVECTOR vResult := vmulq_lane_f32(Q1, Q2H, 1);

    // Mul by Q1WZYX
    float32x4_t vTemp := vrev64q_f32(Q1);
    vTemp := vcombine_f32( vget_high_f32(vTemp), vget_low_f32(vTemp) );
    Q2X := vmulq_f32(Q2X,vTemp);
    vResult = vmlaq_f32( vResult, Q2X, ControlWZYX );

    // Mul by Q1ZWXY
    vTemp = vrev64q_u32(vTemp);
    Q2Y = vmulq_f32(Q2Y,vTemp);
    vResult = vmlaq_f32(vResult, Q2Y, ControlZWXY);

    // Mul by Q1YXWZ
    vTemp = vrev64q_u32(vTemp);
    vTemp = vcombine_f32(vget_high_f32(vTemp), vget_low_f32(vTemp));
    Q2Z = vmulq_f32(Q2Z,vTemp);
    vResult = vmlaq_f32(vResult, Q2Z, ControlYXWZ);
    return vResult;
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMQuaternionMultiply(Q1: TXMVECTOR; Q2: TXMVECTOR): TXMVECTOR;
const
    ControlWZYX: TXMVECTORF32 = (f: (1.0, -1.0, 1.0, -1.0));
    ControlZWXY: TXMVECTORF32 = (f: (1.0, 1.0, -1.0, -1.0));
    ControlYXWZ: TXMVECTORF32 = (f: (-1.0, 1.0, 1.0, -1.0));
var
    Q2X, Q2Y, Q2Z, vResult, Q1Shuffle: TXMVECTOR;
begin
  (* ToDo
    // Copy to SSE registers and use as few as possible for x86
     Q2X := Q2;
     Q2Y := Q2;
     Q2Z := Q2;
     vResult := Q2;
    // Splat with one instruction
    vResult := XM_PERMUTE_PS(vResult,_MM_SHUFFLE(3,3,3,3));
    Q2X := XM_PERMUTE_PS(Q2X,_MM_SHUFFLE(0,0,0,0));
    Q2Y := XM_PERMUTE_PS(Q2Y,_MM_SHUFFLE(1,1,1,1));
    Q2Z := XM_PERMUTE_PS(Q2Z,_MM_SHUFFLE(2,2,2,2));
    // Retire Q1 and perform Q1*Q2W
    vResult := _mm_mul_ps(vResult,Q1);
     Q1Shuffle := Q1;
    // Shuffle the copies of Q1
    Q1Shuffle := XM_PERMUTE_PS(Q1Shuffle,_MM_SHUFFLE(0,1,2,3));
    // Mul by Q1WZYX
    Q2X := _mm_mul_ps(Q2X,Q1Shuffle);
    Q1Shuffle := XM_PERMUTE_PS(Q1Shuffle,_MM_SHUFFLE(2,3,0,1));
    // Flip the signs on y and z
    Q2X := _mm_mul_ps(Q2X,ControlWZYX);
    // Mul by Q1ZWXY
    Q2Y := _mm_mul_ps(Q2Y,Q1Shuffle);
    Q1Shuffle := XM_PERMUTE_PS(Q1Shuffle,_MM_SHUFFLE(0,1,2,3));
    // Flip the signs on z and w
    Q2Y := _mm_mul_ps(Q2Y,ControlZWXY);
    // Mul by Q1YXWZ
    Q2Z := _mm_mul_ps(Q2Z,Q1Shuffle);
    vResult := _mm_add_ps(vResult,Q2X);
    // Flip the signs on x and w
    Q2Z := _mm_mul_ps(Q2Z,ControlYXWZ);
    Q2Y := _mm_add_ps(Q2Y,Q2Z);
    addps xmm, xmm
    vResult := _mm_add_ps(vResult,Q2Y);
    addps xmm, xmm
    result:= vResult;
    *)
end;

{$ENDIF}


function XMQuaternionLengthSq(Q: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVector4LengthSq(Q);
end;


function XMQuaternionReciprocalLength(Q: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVector4ReciprocalLength(Q);
end;



function XMQuaternionLength(Q: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVector4Length(Q);
end;


function XMQuaternionNormalizeEst(Q: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4NormalizeEst(Q);
end;


function XMQuaternionNormalize(Q: TXMVECTOR): TXMVECTOR;
begin
    Result := XMVector4Normalize(Q);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMQuaternionConjugate(Q: TXMVECTOR): TXMVECTOR;
begin
    Result.f32[0] := -Q.f32[0];
    Result.f32[1] := -Q.f32[1];
    Result.f32[2] := -Q.f32[2];
    Result.f32[3] := Q.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMQuaternionConjugate(Q: TXMVECTOR): TXMVECTOR;
const
    NegativeOne3: TXMVECTORF32 = (f: (-1.0, -1.0, -1.0, 1.0));
begin
    (* ToDo
    return vmulq_f32(Q, NegativeOne3.v );
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMQuaternionConjugate(Q: TXMVECTOR): TXMVECTOR; assembler;
const
    NegativeOne3: TXMVECTORF32 = (f: (-1.0, -1.0, -1.0, 1.0));
asm
           VMOVUPS XMM0,[Q]
           VMULPS  XMM0, XMM0,[NegativeOne3]
           VMOVUPS [Result], XMM0
end;
{$ENDIF}

//------------------------------------------------------------------------------
// Returns the inverse of Q.
function XMQuaternionInverse(Q: TXMVECTOR): TXMVECTOR; inline;
var
    Zero, L, Conjugate, Control: TXMVECTOR;
begin
    Zero := XMVectorZero();

    L := XMVector4LengthSq(Q);
    Conjugate := XMQuaternionConjugate(Q);

    Control := XMVectorLessOrEqual(L, g_XMEpsilon.v);

    Result := XMVectorDivide(Conjugate, L);

    Result := XMVectorSelect(Result, Zero, Control);
end;

//------------------------------------------------------------------------------


// Returns the natural logarithm of Q.
function XMQuaternionLn(Q: TXMVECTOR): TXMVECTOR; inline;
const
    OneMinusEpsilon: TXMVECTORF32 = (f: (1.0 - 0.00001, 1.0 - 0.00001, 1.0 - 0.00001, 1.0 - 0.00001));
var
    QW, Q0, ControlW, Theta, SinTheta, S: TXMVECTOR;
begin
    QW := XMVectorSplatW(Q);
    Q0 := XMVectorSelect(g_XMSelect1110.v, Q, g_XMSelect1110.v);

    ControlW := XMVectorInBounds(QW, OneMinusEpsilon.v);

    Theta := XMVectorACos(QW);
    SinTheta := XMVectorSin(Theta);

    S := XMVectorDivide(Theta, SinTheta);

    Result := XMVectorMultiply(Q0, S);
    Result := XMVectorSelect(Q0, Result, ControlW);

end;

//------------------------------------------------------------------------------
// Returns the exponential of Q.
function XMQuaternionExp(Q: TXMVECTOR): TXMVECTOR; inline;
var
    Theta, SinTheta, CosTheta, S, Zero, Control: TXMVECTOR;
begin
    Theta := XMVector3Length(Q);
    XMVectorSinCos(SinTheta, CosTheta, Theta);
    S := XMVectorDivide(SinTheta, Theta);
    Result := XMVectorMultiply(Q, S);
    Zero := XMVectorZero();
    Control := XMVectorNearEqual(Theta, Zero, g_XMEpsilon.v);
    Result := XMVectorSelect(Result, Q, Control);
    Result := XMVectorSelect(CosTheta, Result, g_XMSelect1110.v);
end;


//------------------------------------------------------------------------------
// Returns the interpolated quaternion. If Q0 and Q1 are not unit quaternions, the resulting interpolation is undefined.
function XMQuaternionSlerp(Q0: TXMVECTOR; Q1: TXMVECTOR; t: single): TXMVECTOR; inline;
var
    TV: TXMVECTOR;
begin
    TV := XMVectorReplicate(t);
    Result := XMQuaternionSlerpV(Q0, Q1, TV);
end;




// Returns the interpolated quaternion. If Q0 and Q1 are not unit quaternions, the resulting interpolation is undefined.
// Result = Q0 * sin((1.0 - t) * Omega) / sin(Omega) + Q1 * sin(t * Omega) / sin(Omega)
{$IF DEFINED(_XM_NO_INTRINSICS_) OR DEFINED(_XM_ARM_NEON_INTRINSICS_) }
function XMQuaternionSlerpV(Q0: TXMVECTOR; Q1: TXMVECTOR; T: TXMVECTOR): TXMVECTOR; inline;
const
    OneMinusEpsilon: TXMVECTORF32 = (f: (1.0 - 0.00001, 1.0 - 0.00001, 1.0 - 0.00001, 1.0 - 0.00001));
var
    CosOmega, Zero, Control, Sign, SinOmega, Omega, SignMask, V01, InvSinOmega, S0, S1: TXMVECTOR;
begin
    assert((XMVectorGetY(T) = XMVectorGetX(T)) and (XMVectorGetZ(T) = XMVectorGetX(T)) and (XMVectorGetW(T) = XMVectorGetX(T)));
    CosOmega := XMQuaternionDot(Q0, Q1);

    Zero := XMVectorZero();
    Control := XMVectorLess(CosOmega, Zero);
    Sign := XMVectorSelect(g_XMOne.v, g_XMNegativeOne.v, Control);

    CosOmega := XMVectorMultiply(CosOmega, Sign);

    Control := XMVectorLess(CosOmega, OneMinusEpsilon);

    SinOmega := XMVectorNegativeMultiplySubtract(CosOmega, CosOmega, g_XMOne.v);
    SinOmega := XMVectorSqrt(SinOmega);

    Omega := XMVectorATan2(SinOmega, CosOmega);

    SignMask := XMVectorSplatSignMask();
    V01 := XMVectorShiftLeft(T, Zero, 2);
    SignMask := XMVectorShiftLeft(SignMask, Zero, 3);
    V01 := XMVectorXorInt(V01, SignMask);
    V01 := XMVectorAdd(g_XMIdentityR0.v, V01);

    InvSinOmega := XMVectorReciprocal(SinOmega);

    S0 := XMVectorMultiply(V01, Omega);
    S0 := XMVectorSin(S0);
    S0 := XMVectorMultiply(S0, InvSinOmega);

    S0 := XMVectorSelect(V01, S0, Control);

    S1 := XMVectorSplatY(S0);
    S0 := XMVectorSplatX(S0);

    S1 := XMVectorMultiply(S1, Sign);

    Result := XMVectorMultiply(Q0, S0);
    Result := XMVectorMultiplyAdd(Q1, S1, Result);
end;

{$ELSE}
function XMQuaternionSlerpV(Q0: TXMVECTOR; Q1: TXMVECTOR; T: TXMVECTOR): TXMVECTOR; inline;
const
    OneMinusEpsilon: TXMVECTORF32 = (f: (1.0 - 0.00001, 1.0 - 0.00001, 1.0 - 0.00001, 1.0 - 0.00001));
    SignMask2: TXMVECTORU32 = (u: ($80000000, $00000000, $00000000, $00000000));
var
    CosOmega, Zero, Control, Sign, Omega, SinOmega, V01, S1, S0: TXMVECTOR;
begin

    CosOmega := XMQuaternionDot(Q0, Q1);

    Zero := XMVectorZero();
    Control := XMVectorLess(CosOmega, Zero);
    Sign := XMVectorSelect(g_XMOne, g_XMNegativeOne, Control);
    asm
               // CosOmega = _mm_mul_ps(CosOmega, Sign);
               MOVUPS  XMM0, [CosOmega]
               MULPS   XMM0, [Sign]
               MOVUPS  [CosOmega],XMM0
    end;
    Control := XMVectorLess(CosOmega, OneMinusEpsilon);

    asm
               //  SinOmega = _mm_mul_ps(CosOmega,CosOmega);
               MOVUPS  XMM1,[CosOmega]
               MULPS   XMM1, XMM1
               // SinOmega = _mm_sub_ps(g_XMOne,SinOmega);
               MOVUPS  XMM0,[g_XMOne]
               SUBPS   XMM0, XMM1
               // SinOmega = _mm_sqrt_ps(SinOmega);
               SQRTPS  XMM0, XMM0
               MOVUPS  [SinOmega],XMM0
    end;

    Omega := XMVectorATan2(SinOmega, CosOmega);

    asm
               //  V01 = XM_PERMUTE_PS(T,_MM_SHUFFLE(2,3,0,1));
               MOVUPS  XMM0 ,[T]
               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_2_3_0_1
               // V01 = _mm_and_ps(V01,g_XMMaskXY);
               ANDPS   XMM0, [g_XMMaskXY]
               // V01 = _mm_xor_ps(V01,SignMask2);
               XORPS   XMM0, [SignMask2]
               // V01 = _mm_add_ps(g_XMIdentityR0, V01);
               ADDPS   XMM0, [g_XMIdentityR0]
               MOVUPS  [V01],XMM0
               //  S0 = _mm_mul_ps(V01, Omega);
               MULPS   XMM0, [Omega]
               MOVUPS  [S0],XMM0
    end;
    S0 := XMVectorSin(S0);
    asm
               // S0 = _mm_div_ps(S0, SinOmega);
               MOVUPS  XMM0, [S0]
               DIVPS   XMM0, [SinOmega]
               MOVUPS  [S0],XMM0
    end;
    S0 := XMVectorSelect(V01, S0, Control);

    S1 := XMVectorSplatY(S0);
    S0 := XMVectorSplatX(S0);

    asm
               //S1 = _mm_mul_ps(S1, Sign);
               MOVUPS  XMM1,[S1]
               MULPS   XMM1, [Sign]
               // Result = _mm_mul_ps(Q0, S0);
               MOVUPS  XMM0, [Q0]
               MULPS   XMM0, [S0]
               //S1 = _mm_mul_ps(S1, Q1);
               MULPS   XMM1, [Q1]
               //Result = _mm_add_ps(Result,S1);
               ADDPS   XMM0, XMM1
               //return Result;
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

//------------------------------------------------------------------------------
// Returns the interpolated quaternion. If Q0, Q1, Q2, and Q3 are not all unit quaternions, the returned quaternion is undefined.
function XMQuaternionSquad(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; Q3: TXMVECTOR; t: single): TXMVECTOR; inline;
var
    TV: TXMVECTOR;
begin
    TV := XMVectorReplicate(t);
    Result := XMQuaternionSquadV(Q0, Q1, Q2, Q3, TV);
end;

//------------------------------------------------------------------------------
// Returns the interpolated quaternion. If Q0, Q1, Q2, and Q3 are not unit quaternions, the resulting interpolation is undefined.
function XMQuaternionSquadV(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; Q3: TXMVECTOR; T: TXMVECTOR): TXMVECTOR; inline;
var
    TP, Two, Q03, Q12: TXMVECTOR;
begin
    assert((XMVectorGetY(T) = XMVectorGetX(T)) and (XMVectorGetZ(T) = XMVectorGetX(T)) and (XMVectorGetW(T) = XMVectorGetX(T)), 'Failure in XMQuaternionSquadV');

    TP := T;
    Two := XMVectorSplatConstant(2, 0);

    Q03 := XMQuaternionSlerpV(Q0, Q3, T);
    Q12 := XMQuaternionSlerpV(Q1, Q2, T);

    TP := XMVectorNegativeMultiplySubtract(TP, TP, TP);
    TP := XMVectorMultiply(TP, Two);

    Result := XMQuaternionSlerpV(Q03, Q12, TP);
end;


//------------------------------------------------------------------------------
// Provides addresses of setup control points for spherical quadrangle interpolation.
procedure XMQuaternionSquadSetup(out pA: TXMVECTOR; out pB: TXMVECTOR; out pC: TXMVECTOR; Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; Q3: TXMVECTOR); inline;
var
    LS12, LD12, SQ2, Control1: TXMVECTOR;
    LS01, LD01, SQ0, LS23, LD23, SQ3: TXMVECTOR;
    Control0, Control2, InvQ1, InvQ2, LnQ0, LnQ1, LnQ2, LnQ3: TXMVECTOR;
    NegativeOneQuarter, ExpQ02, ExpQ13: TXMVECTOR;
begin
    LS12 := XMQuaternionLengthSq(XMVectorAdd(Q1, Q2));
    LD12 := XMQuaternionLengthSq(XMVectorSubtract(Q1, Q2));
    SQ2 := XMVectorNegate(Q2);

    Control1 := XMVectorLess(LS12, LD12);
    SQ2 := XMVectorSelect(Q2, SQ2, Control1);

    LS01 := XMQuaternionLengthSq(XMVectorAdd(Q0, Q1));
    LD01 := XMQuaternionLengthSq(XMVectorSubtract(Q0, Q1));
    SQ0 := XMVectorNegate(Q0);

    LS23 := XMQuaternionLengthSq(XMVectorAdd(SQ2, Q3));
    LD23 := XMQuaternionLengthSq(XMVectorSubtract(SQ2, Q3));
    SQ3 := XMVectorNegate(Q3);

    Control0 := XMVectorLess(LS01, LD01);
    Control2 := XMVectorLess(LS23, LD23);

    SQ0 := XMVectorSelect(Q0, SQ0, Control0);
    SQ3 := XMVectorSelect(Q3, SQ3, Control2);

    InvQ1 := XMQuaternionInverse(Q1);
    InvQ2 := XMQuaternionInverse(SQ2);

    LnQ0 := XMQuaternionLn(XMQuaternionMultiply(InvQ1, SQ0));
    LnQ2 := XMQuaternionLn(XMQuaternionMultiply(InvQ1, SQ2));
    LnQ1 := XMQuaternionLn(XMQuaternionMultiply(InvQ2, Q1));
    LnQ3 := XMQuaternionLn(XMQuaternionMultiply(InvQ2, SQ3));

    NegativeOneQuarter := XMVectorSplatConstant(-1, 2);

    ExpQ02 := XMVectorMultiply(XMVectorAdd(LnQ0, LnQ2), NegativeOneQuarter);
    ExpQ13 := XMVectorMultiply(XMVectorAdd(LnQ1, LnQ3), NegativeOneQuarter);
    ExpQ02 := XMQuaternionExp(ExpQ02);
    ExpQ13 := XMQuaternionExp(ExpQ13);

    pA := XMQuaternionMultiply(Q1, ExpQ02);
    pB := XMQuaternionMultiply(SQ2, ExpQ13);
    pC := SQ2;
end;

//------------------------------------------------------------------------------
// Returns a quaternion in barycentric coordinates.
function XMQuaternionBaryCentric(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; f: single; g: single): TXMVECTOR; inline;
var
    s: single;
    Q01, Q02: TXMVECTOR;
begin
    s := f + g;

    if ((s < 0.00001) and (s > -0.00001)) then
        Result := Q0
    else
    begin
        Q01 := XMQuaternionSlerp(Q0, Q1, s);
        Q02 := XMQuaternionSlerp(Q0, Q2, s);

        Result := XMQuaternionSlerp(Q01, Q02, g / s);
    end;
end;

//------------------------------------------------------------------------------
// Returns a point in barycentric coordinates, using the specified quaternions.
function XMQuaternionBaryCentricV(Q0: TXMVECTOR; Q1: TXMVECTOR; Q2: TXMVECTOR; F: TXMVECTOR; G: TXMVECTOR): TXMVECTOR; inline;
var
    Epsilon, S, Q01, Q02, GS: TXMVECTOR;
begin
    assert((XMVectorGetY(F) = XMVectorGetX(F)) and (XMVectorGetZ(F) = XMVectorGetX(F)) and (XMVectorGetW(F) = XMVectorGetX(F)), 'Failure in XMQuaternionBaryCentricV (1)');
    assert((XMVectorGetY(G) = XMVectorGetX(G)) and (XMVectorGetZ(G) = XMVectorGetX(G)) and (XMVectorGetW(G) = XMVectorGetX(G)), 'Failure in XMQuaternionBaryCentricV (2)');

    Epsilon := XMVectorSplatConstant(1, 16);

    S := XMVectorAdd(F, G);

    if (XMVector4InBounds(S, Epsilon)) then
        Result := Q0
    else
    begin
        Q01 := XMQuaternionSlerpV(Q0, Q1, S);
        Q02 := XMQuaternionSlerpV(Q0, Q2, S);
        GS := XMVectorReciprocal(S);
        GS := XMVectorMultiply(G, GS);

        Result := XMQuaternionSlerpV(Q01, Q02, GS);
    end;
end;


//------------------------------------------------------------------------------
// Transformation operations
//------------------------------------------------------------------------------


//------------------------------------------------------------------------------
// Returns the identity quaternion.
function XMQuaternionIdentity: TXMVECTOR; inline;
begin
    Result := g_XMIdentityR3.v;
end;

//------------------------------------------------------------------------------
// Computes a rotation quaternion based on the pitch, yaw, and roll (Euler angles).
function XMQuaternionRotationRollPitchYaw(Pitch: single; Yaw: single; Roll: single): TXMVECTOR; inline;
var
    Angles: TXMVECTOR;
begin
    Angles := XMVectorSet(Pitch, Yaw, Roll, 0.0);
    Result := XMQuaternionRotationRollPitchYawFromVector(Angles);
end;



// Computes a rotation quaternion based on a vector containing the Euler angles (pitch, yaw, and roll).
function XMQuaternionRotationRollPitchYawFromVector(Angles: TXMVECTOR): TXMVECTOR; inline;
const
    Sign: TXMVECTORF32 = (f: (1.0, -1.0, -1.0, 1.0));
var
    HalfAngles: TXMVECTOR;
    SinAngles, CosAngles: TXMVECTOR;
    P0, Y0, R0, P1, Y1, R1: TXMVECTOR;
    Q1, Q0: TXMVECTOR;
begin
    HalfAngles := XMVectorMultiply(Angles, g_XMOneHalf.v);
    XMVectorSinCos(SinAngles, CosAngles, HalfAngles);

    P0 := XMVectorPermute(SinAngles, CosAngles, XM_PERMUTE_0X, XM_PERMUTE_1X, XM_PERMUTE_1X, XM_PERMUTE_1X);
    Y0 := XMVectorPermute(SinAngles, CosAngles, XM_PERMUTE_1Y, XM_PERMUTE_0Y, XM_PERMUTE_1Y, XM_PERMUTE_1Y);
    R0 := XMVectorPermute(SinAngles, CosAngles, XM_PERMUTE_1Z, XM_PERMUTE_1Z, XM_PERMUTE_0Z, XM_PERMUTE_1Z);
    P1 := XMVectorPermute(CosAngles, SinAngles, XM_PERMUTE_0X, XM_PERMUTE_1X, XM_PERMUTE_1X, XM_PERMUTE_1X);
    Y1 := XMVectorPermute(CosAngles, SinAngles, XM_PERMUTE_1Y, XM_PERMUTE_0Y, XM_PERMUTE_1Y, XM_PERMUTE_1Y);
    R1 := XMVectorPermute(CosAngles, SinAngles, XM_PERMUTE_1Z, XM_PERMUTE_1Z, XM_PERMUTE_0Z, XM_PERMUTE_1Z);

    Q1 := XMVectorMultiply(P1, Sign.v);
    Q0 := XMVectorMultiply(P0, Y0);
    Q1 := XMVectorMultiply(Q1, Y1);
    Q0 := XMVectorMultiply(Q0, R0);
    Result := XMVectorMultiplyAdd(Q1, R1, Q0);
end;



{$IF DEFINED(_XM_NO_INTRINSICS_) or DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMQuaternionRotationNormal(NormalAxis: TXMVECTOR; Angle: single): TXMVECTOR; inline;
var
    N, Scale: TXMVECTOR;
    SinV, CosV: single;
begin
    N := XMVectorSelect(g_XMOne.v, NormalAxis, g_XMSelect1110.v);

    XMScalarSinCos(SinV, CosV, 0.5 * Angle);

    Scale := XMVectorSet(SinV, SinV, SinV, CosV);
    Result := XMVectorMultiply(N, Scale);
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMQuaternionRotationNormal(NormalAxis: TXMVECTOR; Angle: single): TXMVECTOR; inline;
var
    N, vSine, vCosine, Scale: TXMVECTOR;
    f: single;
begin
    f := 0.5 * Angle;
    asm
               // N = _mm_and_ps(NormalAxis,g_XMMask3);
               MOVUPS  XMM0, [NormalAxis]
               ANDPS   XMM0, [g_XMMask3]
               // N = _mm_or_ps(N,g_XMIdentityR3);
               ORPS    XMM0, [g_XMIdentityR3]
               MOVUPS  [N],XMM0
               // Scale = _mm_set_ps1(0.5f * Angle);
               MOVSS   XMM1, [f]
               SHUFPS  XMM1, XMM1, 0
               MOVUPS  [Scale],XMM1
    end;
    XMVectorSinCos(vSine, vCosine, Scale);
    asm
               // Scale = _mm_and_ps(vSine,g_XMMask3);
               MOVUPS  XMM0, [vSine]
               ANDPS   XMM0, [g_XMMask3]
               // vCosine = _mm_and_ps(vCosine,g_XMMaskW);
               MOVUPS  XMM1, [vCosine]
               ANDPS   XMM1, [g_XMMaskW]
               // Scale = _mm_or_ps(Scale,vCosine);
               ORPS    XMM0,  XMM1
               // N = _mm_mul_ps(N,Scale);
               MULPS   XMM0, [N]
               // return N;
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

//------------------------------------------------------------------------------
function XMQuaternionRotationAxis(Axis: TXMVECTOR; Angle: single): TXMVECTOR; inline;
var
    Normal: TXMVECTOR;
begin
    assert(not XMVector3Equal(Axis, XMVectorZero()), 'Failure in XMQuaternionRotationAxis (1)');
    assert(not XMVector3IsInfinite(Axis), 'Failure in XMQuaternionRotationAxis (2)');

    Normal := XMVector3Normalize(Axis);
    Result := XMQuaternionRotationNormal(Normal, Angle);
end;

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMQuaternionRotationMatrix(M: TXMMATRIX): TXMVECTOR; inline;
var
    q: TXMVECTORF32;
    r22, dif10, omr22: single;
    fourXSqr, inv4x: single;
    fourYSqr, inv4y: single;
    fourZSqr, inv4z: single;
    sum10, opr22: single;
    fourWSqr, inv4w: single;
begin
    r22 := M.m[2, 2];
    if (r22 <= 0.0) then  // x^2 + y^2 >= z^2 + w^2
    begin
        dif10 := M.m[1, 1] - M.m[0, 0];
        omr22 := 1.0 - r22;
        if (dif10 <= 0.0) then  // x^2 >= y^2
        begin
            fourXSqr := omr22 - dif10;
            inv4x := 0.5 / sqrt(fourXSqr);
            q.f[0] := fourXSqr * inv4x;
            q.f[1] := (M.m[0, 1] + M.m[1, 0]) * inv4x;
            q.f[2] := (M.m[0, 2] + M.m[2, 0]) * inv4x;
            q.f[3] := (M.m[1, 2] - M.m[2, 1]) * inv4x;
        end
        else  // y^2 >= x^2
        begin
            fourYSqr := omr22 + dif10;
            inv4y := 0.5 / sqrt(fourYSqr);
            q.f[0] := (M.m[0, 1] + M.m[1, 0]) * inv4y;
            q.f[1] := fourYSqr * inv4y;
            q.f[2] := (M.m[1, 2] + M.m[2, 1]) * inv4y;
            q.f[3] := (M.m[2, 0] - M.m[0, 2]) * inv4y;
        end;
    end
    else  // z^2 + w^2 >= x^2 + y^2
    begin
        sum10 := M.m[1, 1] + M.m[0, 0];
        opr22 := 1.0 + r22;
        if (sum10 <= 0.0) then  // z^2 >= w^2
        begin
            fourZSqr := opr22 - sum10;
            inv4z := 0.5 / sqrt(fourZSqr);
            q.f[0] := (M.m[0, 2] + M.m[2, 0]) * inv4z;
            q.f[1] := (M.m[1, 2] + M.m[2, 1]) * inv4z;
            q.f[2] := fourZSqr * inv4z;
            q.f[3] := (M.m[0, 1] - M.m[1, 0]) * inv4z;
        end
        else  // w^2 >= z^2
        begin
            fourWSqr := opr22 + sum10;
            inv4w := 0.5 / sqrt(fourWSqr);
            q.f[0] := (M.m[1, 2] - M.m[2, 1]) * inv4w;
            q.f[1] := (M.m[2, 0] - M.m[0, 2]) * inv4w;
            q.f[2] := (M.m[0, 1] - M.m[1, 0]) * inv4w;
            q.f[3] := fourWSqr * inv4w;
        end;
    end;
    Result := q.v;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMQuaternionRotationMatrix(M: TXMMATRIX): TXMVECTOR;
begin
    (* ToDo
    static const XMVECTORF32 XMPMMP     = { { { +1.0f, -1.0f, -1.0f, +1.0f } } };
    static const XMVECTORF32 XMMPMP     = { { { -1.0f, +1.0f, -1.0f, +1.0f } } };
    static const XMVECTORF32 XMMMPP     = { { { -1.0f, -1.0f, +1.0f, +1.0f } } };
    static const XMVECTORU32 Select0110 = { { { XM_SELECT_0, XM_SELECT_1, XM_SELECT_1, XM_SELECT_0 } } };
    static const XMVECTORU32 Select0010 = { { { XM_SELECT_0, XM_SELECT_0, XM_SELECT_1, XM_SELECT_0 } } };

    XMVECTOR r0 = M.r[0];
    XMVECTOR r1 = M.r[1];
    XMVECTOR r2 = M.r[2];

    XMVECTOR r00 = vdupq_lane_f32(vget_low_f32(r0), 0);
    XMVECTOR r11 = vdupq_lane_f32(vget_low_f32(r1), 1);
    XMVECTOR r22 = vdupq_lane_f32(vget_high_f32(r2), 0);

    // x^2 >= y^2 equivalent to r11 - r00 <= 0
    XMVECTOR r11mr00 = vsubq_f32(r11, r00);
    XMVECTOR x2gey2 = vcleq_f32(r11mr00, g_XMZero);

    // z^2 >= w^2 equivalent to r11 + r00 <= 0
    XMVECTOR r11pr00 = vaddq_f32(r11, r00);
    XMVECTOR z2gew2 = vcleq_f32(r11pr00, g_XMZero);

    // x^2 + y^2 >= z^2 + w^2 equivalent to r22 <= 0
    XMVECTOR x2py2gez2pw2 = vcleq_f32(r22, g_XMZero);

    // (4*x^2, 4*y^2, 4*z^2, 4*w^2)
    XMVECTOR t0 = vmulq_f32( XMPMMP, r00 );
    XMVECTOR x2y2z2w2 = vmlaq_f32( t0, XMMPMP, r11 );
    x2y2z2w2 = vmlaq_f32( x2y2z2w2, XMMMPP, r22 );
    x2y2z2w2 = vaddq_f32( x2y2z2w2, g_XMOne );

    // (r01, r02, r12, r11)
    t0 = vextq_f32(r0, r0, 1);
    XMVECTOR t1 = vextq_f32(r1, r1, 1);
    t0 = vcombine_f32( vget_low_f32(t0), vrev64_f32( vget_low_f32( t1 ) ) );

    // (r10, r20, r21, r10)
    t1 = vextq_f32(r2, r2, 3);
    XMVECTOR r10 = vdupq_lane_f32( vget_low_f32(r1), 0 );
    t1 = vbslq_f32( Select0110, t1, r10 );

    // (4*x*y, 4*x*z, 4*y*z, unused)
    XMVECTOR xyxzyz = vaddq_f32(t0, t1);

    // (r21, r20, r10, r10)
    t0 = vcombine_f32( vrev64_f32( vget_low_f32(r2) ), vget_low_f32(r10) );

    // (r12, r02, r01, r12)
    XMVECTOR t2 = vcombine_f32( vrev64_f32( vget_high_f32(r0) ), vrev64_f32( vget_low_f32(r0) ) );
    XMVECTOR t3 = vdupq_lane_f32( vget_high_f32(r1), 0 );
    t1 = vbslq_f32( Select0110, t2, t3 );

    // (4*x*w, 4*y*w, 4*z*w, unused)
    XMVECTOR xwywzw = vsubq_f32(t0, t1);
    xwywzw = vmulq_f32(XMMPMP, xwywzw);

    // (4*x*x, 4*x*y, 4*x*z, 4*x*w)
    t0 = vextq_f32( xyxzyz, xyxzyz, 3 );
    t1 = vbslq_f32( Select0110, t0, x2y2z2w2 );
    t2 = vdupq_lane_f32( vget_low_f32(xwywzw), 0 );
    XMVECTOR tensor0 = vbslq_f32( g_XMSelect1110, t1, t2 );

    // (4*y*x, 4*y*y, 4*y*z, 4*y*w)
    t0 = vbslq_f32( g_XMSelect1011, xyxzyz, x2y2z2w2 );
    t1 = vdupq_lane_f32( vget_low_f32(xwywzw), 1 );
    XMVECTOR tensor1 = vbslq_f32( g_XMSelect1110, t0, t1 );

    // (4*z*x, 4*z*y, 4*z*z, 4*z*w)
    t0 = vextq_f32(xyxzyz, xyxzyz, 1);
    t1 = vcombine_f32( vget_low_f32(t0), vrev64_f32( vget_high_f32(xwywzw) ) );
    XMVECTOR tensor2 = vbslq_f32( Select0010, x2y2z2w2, t1 );

    // (4*w*x, 4*w*y, 4*w*z, 4*w*w)
    XMVECTOR tensor3 = vbslq_f32( g_XMSelect1110, xwywzw, x2y2z2w2 );

    // Select the row of the tensor-product matrix that has the largest
    // magnitude.
    t0 = vbslq_f32( x2gey2, tensor0, tensor1 );
    t1 = vbslq_f32( z2gew2, tensor2, tensor3 );
    t2 = vbslq_f32( x2py2gez2pw2, t0, t1 );

    // Normalize the row.  No division by zero is possible because the
    // quaternion is unit-length (and the row is a nonzero multiple of
    // the quaternion).
    t0 = XMVector4Length(t2);
    return XMVectorDivide(t2, t0);
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMQuaternionRotationMatrix(M: TXMMATRIX): TXMVECTOR; inline;
const
    XMPMMP: TXMVECTORF32 = (f: (+1.0, -1.0, -1.0, +1.0));
    XMMPMP: TXMVECTORF32 = (f: (-1.0, +1.0, -1.0, +1.0));
    XMMMPP: TXMVECTORF32 = (f: (-1.0, -1.0, +1.0, +1.0));
var
    r0, r1, r2: TXMVECTOR;
    r00, r11, r22: TXMVECTOR;
    x2gey2, z2gew2, x2py2gez2pw2: TXMVECTOR;
    t0, t1, t2, x2y2z2w2: TXMVECTOR;
    xyxzyz, xwywzw: TXMVECTOR;
    tensor0, tensor1, tensor2, tensor3: TXMVECTOR;
begin

    r0 := M.r[0];  // (r00, r01, r02, 0)
    r1 := M.r[1];  // (r10, r11, r12, 0)
    r2 := M.r[2];  // (r20, r21, r22, 0)

    // (r00, r00, r00, r00)
    asm
               //  r00 = XM_PERMUTE_PS(r0, _MM_SHUFFLE(0,0,0,0));
               MOVUPS  XMM0,  [r0]
               SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
               MOVUPS  [r00],XMM0

               // (r11, r11, r11, r11)
               //  r11 = XM_PERMUTE_PS(r1, _MM_SHUFFLE(1,1,1,1));
               MOVUPS  XMM1, [r1]
               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
               MOVUPS  [R11],XMM1
               // (r22, r22, r22, r22)
               //  r22 = XM_PERMUTE_PS(r2, _MM_SHUFFLE(2,2,2,2));
               MOVUPS  XMM2, [r2]
               SHUFPS  XMM2, XMM2, _MM_SHUFFLE_2_2_2_2
               MOVUPS  [r22],XMM2


               // x^2 >= y^2 equivalent to r11 - r00 <= 0
               // (r11 - r00, r11 - r00, r11 - r00, r11 - r00)
               //  r11mr00 = _mm_sub_ps(r11, r00);
               MOVUPS  XMM3,XMM1
               SUBPS   XMM3, XMM0
               //  x2gey2 = _mm_cmple_ps(r11mr00, g_XMZero);
               CMPPS   XMM3, [g_XMZero], 2
               MOVUPS  [x2gey2],XMM3

               // z^2 >= w^2 equivalent to r11 + r00 <= 0
               // (r11 + r00, r11 + r00, r11 + r00, r11 + r00)
               // r11pr00 = _mm_add_ps(r11, r00);
               MOVUPS  XMM3,XMM1
               ADDPS   XMM3, XMM0
               //  z2gew2 = _mm_cmple_ps(r11pr00, g_XMZero);
               CMPPS   XMM3, [g_XMZero], 2
               MOVUPS  [z2gew2],XMM3

               // x^2 + y^2 >= z^2 + w^2 equivalent to r22 <= 0
               // x2py2gez2pw2 = _mm_cmple_ps(r22, g_XMZero);
               MOVUPS  XMM3,XMM2
               CMPPS   XMM3, [g_XMZero], 2
               MOVUPS  [x2py2gez2pw2],XMM3

               // (+r00, -r00, -r00, +r00)
               // t0 = _mm_mul_ps(XMPMMP, r00);
               MULPS   XMM0, [XMPMMP]

               // (-r11, +r11, -r11, +r11)
               // t1 = _mm_mul_ps(XMMPMP, r11);
               MULPS   XMM1, [XMMPMP]

               // (-r22, -r22, +r22, +r22)
               //  t2 = _mm_mul_ps(XMMMPP, r22);
               MULPS   XMM2, [XMMMPP]

               // (4*x^2, 4*y^2, 4*z^2, 4*w^2)
               //  x2y2z2w2 = _mm_add_ps(t0, t1);
               MOVUPS  XMM3,XMM0
               ADDPS   XMM3, XMM1
               // x2y2z2w2 = _mm_add_ps(t2, x2y2z2w2);
               ADDPS   XMM3, XMM2
               // x2y2z2w2 = _mm_add_ps(x2y2z2w2, g_XMOne);
               ADDPS   XMM3, [g_XMOne]
               MOVUPS  [x2y2z2w2],XMM3

               // (r01, r02, r12, r11)
               // t0 = _mm_shuffle_ps(r0, r1, _MM_SHUFFLE(1,2,2,1));
               MOVUPS  XMM0, [r0]
               SHUFPS  XMM0, [r1], _MM_SHUFFLE_1_2_2_1
               // (r10, r10, r20, r21)
               // t1 = _mm_shuffle_ps(r1, r2, _MM_SHUFFLE(1,0,0,0));
               MOVUPS  XMM1, [r1]
               SHUFPS  XMM1, [r2], _MM_SHUFFLE_1_0_0_0
               // (r10, r20, r21, r10)
               // t1 = XM_PERMUTE_PS(t1, _MM_SHUFFLE(1,3,2,0));
               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_3_2_0
               // (4*x*y, 4*x*z, 4*y*z, unused)
               // xyxzyz = _mm_add_ps(t0, t1);
               ADDPS   XMM0, XMM1
               MOVUPS  [xyxzyz], XMM0


               // (r21, r20, r10, r10)
               // t0 = _mm_shuffle_ps(r2, r1, _MM_SHUFFLE(0,0,0,1));
               MOVUPS  XMM0, [r2]
               SHUFPS  XMM0, [r1], _MM_SHUFFLE_0_0_0_1
               // (r12, r12, r02, r01)
               // t1 = _mm_shuffle_ps(r1, r0, _MM_SHUFFLE(1,2,2,2));
               MOVUPS  XMM1, [r1]
               SHUFPS  XMM1, [r0], _MM_SHUFFLE_1_2_2_2
               // (r12, r02, r01, r12)
               // t1 = XM_PERMUTE_PS(t1, _MM_SHUFFLE(1,3,2,0));
               SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_3_2_0
               // (4*x*w, 4*y*w, 4*z*w, unused)
               //  xwywzw = _mm_sub_ps(t0, t1);
               SUBPS   XMM0, XMM1
               // xwywzw = _mm_mul_ps(XMMPMP, xwywzw);
               MULPS   XMM0, [XMMPMP]
               MOVUPS  [xwywzw], XMM0

               // (4*x^2, 4*y^2, 4*x*y, unused)
               //t0 = _mm_shuffle_ps(x2y2z2w2, xyxzyz, _MM_SHUFFLE(0,0,1,0));
               MOVUPS  XMM0,[x2y2z2w2]
               SHUFPS  XMM0, [xyxzyz], _MM_SHUFFLE_0_0_1_0
               // (4*z^2, 4*w^2, 4*z*w, unused)
               //t1 = _mm_shuffle_ps(x2y2z2w2, xwywzw, _MM_SHUFFLE(0,2,3,2));
               MOVUPS  XMM1, [x2y2z2w2]
               SHUFPS  XMM1, [xwywzw], _MM_SHUFFLE_0_2_3_2
               // (4*x*z, 4*y*z, 4*x*w, 4*y*w)
               //t2 = _mm_shuffle_ps(xyxzyz, xwywzw, _MM_SHUFFLE(1,0,2,1));
               MOVUPS  XMM2,[xyxzyz]
               SHUFPS  XMM2,[xwywzw], _MM_SHUFFLE_1_0_2_1

               // (4*x*x, 4*x*y, 4*x*z, 4*x*w)
               //  tensor0 = _mm_shuffle_ps(t0, t2, _MM_SHUFFLE(2,0,2,0));
               MOVUPS  XMM3, XMM0
               SHUFPS  XMM3, XMM2, _MM_SHUFFLE_2_0_2_0
               MOVUPS  [tensor0],XMM3
               // (4*y*x, 4*y*y, 4*y*z, 4*y*w)
               //  tensor1 = _mm_shuffle_ps(t0, t2, _MM_SHUFFLE(3,1,1,2));
               MOVUPS  XMM3,XMM0
               SHUFPS  XMM3, XMM2, _MM_SHUFFLE_3_1_1_2
               MOVUPS  [tensor1],XMM3
               // (4*z*x, 4*z*y, 4*z*z, 4*z*w)
               //  tensor2 = _mm_shuffle_ps(t2, t1, _MM_SHUFFLE(2,0,1,0));
               MOVUPS  XMM3,XMM2
               SHUFPS  XMM3, XMM1, _MM_SHUFFLE_2_0_1_0
               MOVUPS  [tensor2],XMM3
               // (4*w*x, 4*w*y, 4*w*z, 4*w*w)
               //  tensor3 = _mm_shuffle_ps(t2, t1, _MM_SHUFFLE(1,2,3,2));
               MOVUPS  XMM3,XMM2
               SHUFPS  XMM3, XMM1, _MM_SHUFFLE_1_2_3_2
               MOVUPS  [tensor3],XMM3

               // Select the row of the tensor-product matrix that has the largest
               // magnitude.
               // t0 = _mm_and_ps(x2gey2, tensor0);
               MOVUPS  XMM0, [x2gey2]
               ANDPS   XMM0, [tensor0]
               // t1 = _mm_andnot_ps(x2gey2, tensor1);
               MOVUPS  XMM1, [x2gey2]
               ANDNPS  XMM1, [tensor1]
               // t0 = _mm_or_ps(t0, t1);
               ORPS    XMM0, XMM1
               // t1 = _mm_and_ps(z2gew2, tensor2);
               MOVUPS  XMM1, [z2gew2]
               ANDPS   XMM1, [tensor2]
               // t2 = _mm_andnot_ps(z2gew2, tensor3);
               MOVUPS  XMM2,[z2gew2]
               ANDNPS  XMM2, [tensor3]
               // t1 = _mm_or_ps(t1, t2);
               ORPS    XMM1, XMM2
               // t0 = _mm_and_ps(x2py2gez2pw2, t0);
               ANDPS   XMM0, [x2py2gez2pw2]
               // t1 = _mm_andnot_ps(x2py2gez2pw2, t1);
               ANDNPS  XMM1, [x2py2gez2pw2]
               // t2 = _mm_or_ps(t0, t1);
               ORPS    XMM0, XMM1
               MOVUPS  [t2],XMM0
    end;
    // Normalize the row.  No division by zero is possible because the
    // quaternion is unit-length (and the row is a nonzero multiple of
    // the quaternion).
    t0 := XMVector4Length(t2);
    asm
               // return _mm_div_ps(t2, t0);
               MOVUPS  XMM0,[t2]
               DIVPS   XMM0, [t0]
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

//------------------------------------------------------------------------------
// Conversion operations
//------------------------------------------------------------------------------

procedure XMQuaternionToAxisAngle(out pAxis: TXMVECTOR; out pAngle: single; Q: TXMVECTOR); inline;
begin
    pAxis := Q;
    pAngle := 2.0 * XMScalarACos(XMVectorGetW(Q));
end;

{***************************************************************************
 *
 * Plane
 *
 ***************************************************************************}

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

function XMPlaneEqual(P1: TXMVECTOR; P2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4Equal(P1, P2);
end;


function XMPlaneNearEqual(P1: TXMVECTOR; P2: TXMVECTOR; Epsilon: TXMVECTOR): boolean; inline;
var
    NP1, NP2: TXMVECTOR;
begin
    NP1 := XMPlaneNormalize(P1);
    NP2 := XMPlaneNormalize(P2);
    Result := XMVector4NearEqual(NP1, NP2, Epsilon);
end;



function XMPlaneNotEqual(P1: TXMVECTOR; P2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4NotEqual(P1, P2);
end;


function XMPlaneIsNaN(P: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4IsNaN(P);
end;


function XMPlaneIsInfinite(P: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4IsInfinite(P);
end;


//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------


function XMPlaneDot(P: TXMVECTOR; V: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVector4Dot(P, V);
end;


function XMPlaneDotCoord(P: TXMVECTOR; V: TXMVECTOR): TXMVECTOR; inline;
var
    V3: TXMVECTOR;
begin
    // Result = P[0] * V[0] + P[1] * V[1] + P[2] * V[2] + P[3]
    V3 := XMVectorSelect(g_XMOne.v, V, g_XMSelect1110.v);
    Result := XMVector4Dot(P, V3);
end;



function XMPlaneDotNormal(P: TXMVECTOR; V: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVector3Dot(P, V);
end;



//------------------------------------------------------------------------------
// XMPlaneNormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.
{$IF DEFINED(_XM_NO_INTRINSICS_) or DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMPlaneNormalizeEst(P: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVector3ReciprocalLengthEst(P);
    Result := XMVectorMultiply(P, Result);
end;

{$ELSEIF  DEFINED(_XM_SSE4_INTRINSICS_) }
function XMPlaneNormalizeEst(P: TXMVECTOR): TXMVECTOR; assembler;
asm
           VMOVUPS XMM0, [P]
           VDPPS   XMM1, XMM0,XMM0, $7F  // vTemp in XMM1, P in XMM0
           VRSQRTPS XMM2, XMM1
           VMULPS  XMM0, XMM2, XMM0
           VMOVUPS [Result], XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMPlaneNormalizeEst(P: TXMVECTOR): TXMVECTOR;
begin
    (* ToDo
     // Perform the dot product
    TXMVECTOR vDot = _mm_mul_ps(P,P);
    mulps xmm, xmm
    // x=Dot.y, y=Dot.z
    TXMVECTOR vTemp = XM_PERMUTE_PS(vDot,_MM_SHUFFLE(2,1,2,1));
    shufps xmm, xmm, _MM_SHUFFLE_x_x_x_x
    // Result.x = x+y
    vDot = _mm_add_ss(vDot,vTemp);
    addss xmm, xmm
    // x=Dot.z
    vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
    shufps xmm, xmm, _MM_SHUFFLE_x_x_x_x
    // Result.x = (x+y)+z
    vDot = _mm_add_ss(vDot,vTemp);
    addss xmm, xmm
    // Splat x
    vDot = XM_PERMUTE_PS(vDot,_MM_SHUFFLE(0,0,0,0));
    shufps xmm, xmm, _MM_SHUFFLE_x_x_x_x
    // Get the reciprocal
    vDot = _mm_rsqrt_ps(vDot);
    rsqrtps xmm, xmm
    // Get the reciprocal
    vDot = _mm_mul_ps(vDot,P);
    mulps xmm, xmm
    return vDot;

    *)
end;

{$ENDIF}


{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMPlaneNormalize(P: TXMVECTOR): TXMVECTOR; inline;
var
    fLengthSq: single;
begin
    fLengthSq := sqrt((P.f32[0] * P.f32[0]) + (P.f32[1] * P.f32[1]) + (P.f32[2] * P.f32[2]));
    // Prevent divide by zero
    if (fLengthSq <> 0) then
        fLengthSq := 1.0 / fLengthSq;

    Result.f32[0] := P.f32[0] * fLengthSq;
    Result.f32[0] := P.f32[1] * fLengthSq;
    Result.f32[0] := P.f32[2] * fLengthSq;
    Result.f32[0] := P.f32[3] * fLengthSq;
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMPlaneNormalize(P: TXMVECTOR): TXMVECTOR; inline;
var
    vLength: TXMVECTOR;
begin
    vLength := XMVector3ReciprocalLength(P);
    Result := XMVectorMultiply(P, vLength);
end;

{$ELSEIF DEFINED(_XM_SSE4_INTRINSICS_)}
function XMPlaneNormalize(P: TXMVECTOR): TXMVECTOR; assembler;
asm
           //  vLengthSq = _mm_dp_ps( P, P, $7f );
           MOVUPS  XMM0 ,[P]
           DPPS    XMM0, XMM0, $7f
           // Prepare for the division
           //  vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           // vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Reciprocal mul to perform the normalization
           // vResult = _mm_div_ps(P,vResult);
           MOVUPS  XMM2, [P]
           DIVPS   XMM2, XMM1
           // Any that are infinity, set to zero
           // vResult = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM0, XMM2
           // return vResult;
           MOVUPS  [result],XMM0
end;
{$ELSE}// _XM_SSE_INTRINSICS_
function XMPlaneNormalize(P: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Perform the dot product on x,y and z only
           // vLengthSq = _mm_mul_ps(P,P);
           MOVUPS  XMM0,[P]
           MULPS   XMM0, XMM0
           // vTemp = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(2,1,2,1));
           MOVUPS  XMM1,XMM0
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_2_1_2_1
           //vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           //vTemp = XM_PERMUTE_PS(vTemp,_MM_SHUFFLE(1,1,1,1));
           SHUFPS  XMM1, XMM1, _MM_SHUFFLE_1_1_1_1
           //vLengthSq = _mm_add_ss(vLengthSq,vTemp);
           ADDSS   XMM0, XMM1
           //vLengthSq = XM_PERMUTE_PS(vLengthSq,_MM_SHUFFLE(0,0,0,0));
           SHUFPS  XMM0, XMM0, _MM_SHUFFLE_0_0_0_0
           // Prepare for the division
           // vResult = _mm_sqrt_ps(vLengthSq);
           SQRTPS  XMM1, XMM0
           // Failsafe on zero (Or epsilon) length planes
           // If the length is infinity, set the elements to zero
           //vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
           CMPPS   XMM0, [g_XMInfinity], 4
           // Reciprocal mul to perform the normalization
           //vResult = _mm_div_ps(P,vResult);
           MOVUPS  XMM2, [P]
           DIVPS   XMM2, XMM1
           // Any that are infinity, set to zero
           //vResult = _mm_and_ps(vResult,vLengthSq);
           ANDPS   XMM0, XMM2
           //return vResult;
           MOVUPS  [result],XMM0

end;
{$ENDIF}

// Returns the intersection of the plane P and the line defined by LinePoint1 and LinePoint2. If the line is parallel to the plane, all components of the returned vector are equal to QNaN.
function XMPlaneIntersectLine(P: TXMVECTOR; LinePoint1: TXMVECTOR; LinePoint2: TXMVECTOR): TXMVECTOR; inline;
var
    V1, V2, D, VT, Point, Zero, Control: TXMVECTOR;
begin
    V1 := XMVector3Dot(P, LinePoint1);
    V2 := XMVector3Dot(P, LinePoint2);
    D := XMVectorSubtract(V1, V2);

    VT := XMPlaneDotCoord(P, LinePoint1);
    VT := XMVectorDivide(VT, D);

    Point := XMVectorSubtract(LinePoint2, LinePoint1);
    Point := XMVectorMultiplyAdd(Point, VT, LinePoint1);

    Zero := XMVectorZero();
    Control := XMVectorNearEqual(D, Zero, g_XMEpsilon.v);

    Result := XMVectorSelect(Point, g_XMQNaN.v, Control);
end;


// Finds the intersection of two planes.
// If the planes are parallel to one another, all components of the returned point vectors are equal to QNaN.
procedure XMPlaneIntersectPlane(out pLinePoint1: TXMVECTOR; out pLinePoint2: TXMVECTOR; P1: TXMVECTOR; P2: TXMVECTOR); inline;
var
    V1, V2, LengthSq, P1W, Point, V3, P2W, LinePoint1, LinePoint2, Control: TXMVECTOR;
begin
    V1 := XMVector3Cross(P2, P1);

    LengthSq := XMVector3LengthSq(V1);

    V2 := XMVector3Cross(P2, V1);

    P1W := XMVectorSplatW(P1);
    Point := XMVectorMultiply(V2, P1W);

    V3 := XMVector3Cross(V1, P1);

    P2W := XMVectorSplatW(P2);
    Point := XMVectorMultiplyAdd(V3, P2W, Point);

    LinePoint1 := XMVectorDivide(Point, LengthSq);

    LinePoint2 := XMVectorAdd(LinePoint1, V1);

    Control := XMVectorLessOrEqual(LengthSq, g_XMEpsilon.v);
    pLinePoint1 := XMVectorSelect(LinePoint1, g_XMQNaN.v, Control);
    pLinePoint2 := XMVectorSelect(LinePoint2, g_XMQNaN.v, Control);
end;


function XMPlaneTransform(P: TXMVECTOR; M: TXMMATRIX): TXMVECTOR; inline;
var
    W, X, Y, Z: TXMVECTOR;
begin
    W := XMVectorSplatW(P);
    Z := XMVectorSplatZ(P);
    Y := XMVectorSplatY(P);
    X := XMVectorSplatX(P);

    Result := XMVectorMultiply(W, M.r[3]);
    Result := XMVectorMultiplyAdd(Z, M.r[2], Result);
    Result := XMVectorMultiplyAdd(Y, M.r[1], Result);
    Result := XMVectorMultiplyAdd(X, M.r[0], Result);
end;


function XMPlaneTransformStream(out pOutputStream: PXMFLOAT4; OutputStride: size_t; constref pInputStream: PXMFLOAT4; InputStride: size_t; PlaneCount: size_t; M: TXMMATRIX): PXMFLOAT4; inline;
begin
    Result := XMVector4TransformStream(pOutputStream, OutputStride, pInputStream, InputStride, PlaneCount, M);
end;

//------------------------------------------------------------------------------
// Conversion operations
//------------------------------------------------------------------------------


function XMPlaneFromPointNormal(Point: TXMVECTOR; Normal: TXMVECTOR): TXMVECTOR; inline;
var
    W: TXMVECTOR;
begin
    W := XMVector3Dot(Point, Normal);
    W := XMVectorNegate(W);
    Result := XMVectorSelect(W, Normal, g_XMSelect1110.v);
end;


function XMPlaneFromPoints(Point1: TXMVECTOR; Point2: TXMVECTOR; Point3: TXMVECTOR): TXMVECTOR; inline;
var
    V21, V31, N, D: TXMVECTOR;
begin
    V21 := XMVectorSubtract(Point1, Point2);
    V31 := XMVectorSubtract(Point1, Point3);

    N := XMVector3Cross(V21, V31);
    N := XMVector3Normalize(N);

    D := XMPlaneDotNormal(N, Point1);
    D := XMVectorNegate(D);

    Result := XMVectorSelect(D, N, g_XMSelect1110.v);
end;


{***************************************************************************
 *
 * Color
 *
 ***************************************************************************}

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

function XMColorEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4Equal(C1, C2);
end;



function XMColorNotEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4NotEqual(C1, C2);
end;



function XMColorGreater(C1: TXMVECTOR; C2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4Greater(C1, C2);
end;



function XMColorGreaterOrEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4GreaterOrEqual(C1, C2);
end;


function XMColorLess(C1: TXMVECTOR; C2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4Less(C1, C2);
end;



function XMColorLessOrEqual(C1: TXMVECTOR; C2: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4LessOrEqual(C1, C2);
end;



function XMColorIsNaN(C: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4IsNaN(C);
end;



function XMColorIsInfinite(C: TXMVECTOR): boolean; inline;
begin
    Result := XMVector4IsInfinite(C);
end;

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMColorNegative(constref vColor: TXMVECTOR): TXMVECTOR; inline;
begin
    Result.f32[0] := 1.0 - vColor.f32[0];
    Result.f32[1] := 1.0 - vColor.f32[1];
    Result.f32[2] := 1.0 - vColor.f32[2];
    Result.f32[3] := vColor.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMColorNegative(constref vColor: TXMVECTOR): TXMVECTOR; inline;
var
    vTemp: TXMVECTOR;
begin
    (* ToDo
     vTemp := veorq_u32(vColor,g_XMNegate3);
    result:= vaddq_f32(vTemp,g_XMOne3);
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMColorNegative(constref vColor: TXMVECTOR): TXMVECTOR; assembler;
asm
           // Negate only x,y and z.
           VMOVUPS XMM0,[vColor]
           VMOVUPS XMM1, [g_XMNegate3]
           VXORPS  XMM1,XMM0,XMM1 // vTemp in XMM1
           // Add 1,1,1,0 to -x,-y,-z,w
           VADDPS  XMM0, XMM1, [g_XMOne3]
           VMOVUPS [Result], XMM0
end;
{$ENDIF}


function XMColorModulate(C1: TXMVECTOR; C2: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVectorMultiply(C1, C2);
end;


{$IF DEFINED(_XM_NO_INTRINSICS_)}

// Luminance = 0.2125f * C[0] + 0.7154f * C[1] + 0.0721f * C[2];
// Result = (C - Luminance) * Saturation + Luminance;
function XMColorAdjustSaturation(constref vColor: TXMVECTOR; Saturation: single): TXMVECTOR; inline;
const
    gvLuminance: TXMVECTORF32 = (f: (0.2125, 0.7154, 0.0721, 0.0));
var
    fLuminance: single;
begin
    fLuminance := (vColor.f32[0] * gvLuminance.f[0]) + (vColor.f32[1] * gvLuminance.f[1]) + (vColor.f32[2] * gvLuminance.f[2]);
    Result.f32[0] := ((vColor.f32[0] - fLuminance) * Saturation) + fLuminance;
    Result.f32[1] := ((vColor.f32[1] - fLuminance) * Saturation) + fLuminance;
    Result.f32[2] := ((vColor.f32[2] - fLuminance) * Saturation) + fLuminance;
    Result.f32[3] := vColor.f32[3];
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMColorAdjustSaturation(constref vColor: TXMVECTOR; Saturation: single): TXMVECTOR; inline;
begin
    (* ToDo
    XMVECTOR vLuminance = XMVector3Dot( vColor, gvLuminance );
    XMVECTOR vResult = vsubq_f32(vColor, vLuminance);
    vResult = vmlaq_n_f32( vLuminance, vResult, fSaturation );
    return vbslq_f32( g_XMSelect1110, vResult, vColor );
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMColorAdjustSaturation(constref vColor: TXMVECTOR; Saturation: single): TXMVECTOR; inline;
const
    gvLuminance: TXMVECTORF32 = (f: (0.2125, 0.7154, 0.0721, 0.0));
var
    vLuminance: TXMVECTOR;
begin
    vLuminance := XMVector3Dot(vColor, gvLuminance);
    // Splat fSaturation
    asm
               // vSaturation = _mm_set_ps1(fSaturation);
               MOVSS   XMM1, [Saturation]
               SHUFPS  XMM1, XMM1, 0
               // vResult = ((vColor-vLuminance)*vSaturation)+vLuminance;
               // vResult = _mm_sub_ps(vColor,vLuminance);
               MOVUPS  XMM0 , [vColor]
               SUBPS   XMM0, [vLuminance]
               // vResult = _mm_mul_ps(vResult,vSaturation);
               MULPS   XMM0, XMM1
               // vResult = _mm_add_ps(vResult,vLuminance);
               ADDPS   XMM0, [vLuminance]
               // Retain w from the source color
               // vLuminance = _mm_shuffle_ps(vResult,vColor,_MM_SHUFFLE(3,2,2,2));   // x = vResult.z,y = vResult.z,z = vColor.z,w=vColor.w
               MOVUPS  XMM1, XMM0
               SHUFPS  XMM1, [vColor], _MM_SHUFFLE_3_2_2_2
               // vResult = _mm_shuffle_ps(vResult,vLuminance,_MM_SHUFFLE(3,0,1,0));  // x = vResult.x,y = vResult.y,z = vResult.z,w=vColor.w
               SHUFPS  XMM0, XMM1, _MM_SHUFFLE_3_0_1_0
               // return vResult;
               MOVUPS  [result],XMM0
    end;
end;

{$ENDIF}

{$IF DEFINED(_XM_NO_INTRINSICS_)}

// Result = (vColor - 0.5f) * fContrast + 0.5f;
function XMColorAdjustContrast(constref vColor: TXMVECTOR; constref Contrast: single): TXMVECTOR; inline;
begin
    Result.f32[0] := ((vColor.f32[0] - 0.5) * Contrast) + 0.5;
    Result.f32[1] := ((vColor.f32[1] - 0.5) * Contrast) + 0.5;
    Result.f32[2] := ((vColor.f32[2] - 0.5) * Contrast) + 0.5;
    Result.f32[3] := vColor.f32[3];        // Leave W untouched
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMColorAdjustContrast(constref vColor: TXMVECTOR; constref Contrast: single): TXMVECTOR; inline;
var
    vResult: TXMVECTOR;
begin
    (* ToDo
     vResult = vsubq_f32(vColor, g_XMOneHalf.v);
    vResult := vmlaq_n_f32( g_XMOneHalf.v, vResult, fContrast );
    result:= vbslq_f32( g_XMSelect1110, vResult, vColor );
    *)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMColorAdjustContrast(constref vColor: TXMVECTOR; constref Contrast: single): TXMVECTOR; assembler;
asm
           VMOVUPS XMM0, [vColor]         // vColor in XMM0
           VBROADCASTSS XMM1,[Contrast]  // Splat the scale , vScale in XMM1

           VSUBPS  XMM2, XMM0, [g_XMOneHalf] // Subtract 0.5f from the source (Saving source), vResult in XMM2
           VMULPS  XMM2, XMM2, XMM1          // Mul by scale
           VADDPS  XMM2, XMM2, [g_XMOneHalf]   // Add 0.5f

           // Retain w from the source color
           VSHUFPS XMM1, XMM2,XMM0, _MM_SHUFFLE_3_2_2_2       // x = vResult.z,y = vResult.z,z = vColor.z,w=vColor.w
           VSHUFPS XMM0, XMM2, XMM1, _MM_SHUFFLE_3_0_1_0      // x = vResult.x,y = vResult.y,z = vResult.z,w=vColor.w

           VMOVUPS [Result], XMM0
end;
{$ENDIF}


function XMColorRGBToHSL(rgb: TXMVECTOR): TXMVECTOR; inline;
var
    r, g, b, min, max, l, d, la: TXMVECTOR;
    s, h, d2, lha: TXMVECTOR;
begin
    r := XMVectorSplatX(rgb);
    g := XMVectorSplatY(rgb);
    b := XMVectorSplatZ(rgb);

    min := XMVectorMin(r, XMVectorMin(g, b));
    max := XMVectorMax(r, XMVectorMax(g, b));

    l := XMVectorMultiply(XMVectorAdd(min, max), g_XMOneHalf);

    d := XMVectorSubtract(max, min);

    la := XMVectorSelect(rgb, l, g_XMSelect1110);

    if (XMVector3Less(d, g_XMEpsilon)) then
    begin
        // Achromatic, assume H and S of 0
        Result := XMVectorSelect(la, g_XMZero, g_XMSelect1100);
        Exit;
    end
    else
    begin
        d2 := XMVectorAdd(min, max);

        if (XMVector3Greater(l, g_XMOneHalf)) then
        begin
            // d / (2-max-min)
            s := XMVectorDivide(d, XMVectorSubtract(g_XMTwo, d2));
        end
        else
        begin
            // d / (max+min)
            s := XMVectorDivide(d, d2);
        end;

        if (XMVector3Equal(r, max)) then
        begin
            // Red is max
            h := XMVectorDivide(XMVectorSubtract(g, b), d);
        end
        else if (XMVector3Equal(g, max)) then
        begin
            // Green is max
            h := XMVectorDivide(XMVectorSubtract(b, r), d);
            h := XMVectorAdd(h, g_XMTwo);
        end
        else
        begin
            // Blue is max
            h := XMVectorDivide(XMVectorSubtract(r, g), d);
            h := XMVectorAdd(h, g_XMFour);
        end;

        h := XMVectorDivide(h, g_XMSix);

        if (XMVector3Less(h, g_XMZero)) then
            h := XMVectorAdd(h, g_XMOne);

        lha := XMVectorSelect(la, h, g_XMSelect1100);
        Result := XMVectorSelect(s, lha, g_XMSelect1011);
    end;
end;



function XMColorHue2Clr(p, q, h: TXMVECTOR): TXMVECTOR; inline;
const
    oneSixth: TXMVECTORF32 = (f: (1.0 / 6.0, 1.0 / 6.0, 1.0 / 6.0, 1.0 / 6.0));
    twoThirds: TXMVECTORF32 = (f: (2.0 / 3.0, 2.0 / 3.0, 2.0 / 3.0, 2.0 / 3.0));
var
    t, t1, t2: TXMVECTOR;
begin
    t := h;

    if (XMVector3Less(t, g_XMZero)) then
        t := XMVectorAdd(t, g_XMOne);

    if (XMVector3Greater(t, g_XMOne)) then
        t := XMVectorSubtract(t, g_XMOne);

    if (XMVector3Less(t, oneSixth)) then
    begin
        // p + (q - p) * 6 * t
        t1 := XMVectorSubtract(q, p);
        t2 := XMVectorMultiply(g_XMSix, t);
        Result := XMVectorMultiplyAdd(t1, t2, p);
        Exit;
    end;

    if (XMVector3Less(t, g_XMOneHalf)) then
    begin
        Result := q;
        Exit;
    end;

    if (XMVector3Less(t, twoThirds)) then
    begin
        // p + (q - p) * 6 * (2/3 - t)
        t1 := XMVectorSubtract(q, p);
        t2 := XMVectorMultiply(g_XMSix, XMVectorSubtract(twoThirds, t));
        Result := XMVectorMultiplyAdd(t1, t2, p);
        Exit;
    end;

    Result := p;
end;


function XMColorHSLToRGB(hsl: TXMVECTOR): TXMVECTOR; inline;
const
    oneThird: TXMVECTORF32 = (f: (1.0 / 3.0, 1.0 / 3.0, 1.0 / 3.0, 1.0 / 3.0));
var
    s, l, h, q, p, r, g, b, rg, ba: TXMVECTOR;
begin
    s := XMVectorSplatY(hsl);
    l := XMVectorSplatZ(hsl);

    if (XMVector3NearEqual(s, g_XMZero, g_XMEpsilon)) then
    begin
        // Achromatic
        Result := XMVectorSelect(hsl, l, g_XMSelect1110);
    end
    else
    begin
        h := XMVectorSplatX(hsl);

        if (XMVector3Less(l, g_XMOneHalf)) then
        begin
            q := XMVectorMultiply(l, XMVectorAdd(g_XMOne, s));
        end
        else
        begin
            q := XMVectorSubtract(XMVectorAdd(l, s), XMVectorMultiply(l, s));
        end;

        p := XMVectorSubtract(XMVectorMultiply(g_XMTwo, l), q);

        r := XMColorHue2Clr(p, q, XMVectorAdd(h, oneThird));
        g := XMColorHue2Clr(p, q, h);
        b := XMColorHue2Clr(p, q, XMVectorSubtract(h, oneThird));

        rg := XMVectorSelect(g, r, g_XMSelect1000);
        ba := XMVectorSelect(hsl, b, g_XMSelect1110);

        Result := XMVectorSelect(ba, rg, g_XMSelect1100);
    end;
end;



function XMColorRGBToHSV(rgb: TXMVECTOR): TXMVECTOR; inline;
var
    r, g, b, min, v, d, s: TXMVECTOR;
    h, hv, hva: TXMVECTOR;
begin
    r := XMVectorSplatX(rgb);
    g := XMVectorSplatY(rgb);
    b := XMVectorSplatZ(rgb);

    min := XMVectorMin(r, XMVectorMin(g, b));
    v := XMVectorMax(r, XMVectorMax(g, b));

    d := XMVectorSubtract(v, min);
    if (XMVector3NearEqual(v, g_XMZero, g_XMEpsilon)) then
        s := g_XMZero
    else
        s := XMVectorDivide(d, v);

    if (XMVector3Less(d, g_XMEpsilon)) then
    begin
        // Achromatic, assume H of 0
        hv := XMVectorSelect(v, g_XMZero, g_XMSelect1000);
        hva := XMVectorSelect(rgb, hv, g_XMSelect1110);
        Result := XMVectorSelect(s, hva, g_XMSelect1011);
    end
    else
    begin
        if (XMVector3Equal(r, v)) then
        begin
            // Red is max
            h := XMVectorDivide(XMVectorSubtract(g, b), d);

            if (XMVector3Less(g, b)) then
                h := XMVectorAdd(h, g_XMSix);
        end
        else if (XMVector3Equal(g, v)) then
        begin
            // Green is max
            h := XMVectorDivide(XMVectorSubtract(b, r), d);
            h := XMVectorAdd(h, g_XMTwo);
        end
        else
        begin
            // Blue is max
            h := XMVectorDivide(XMVectorSubtract(r, g), d);
            h := XMVectorAdd(h, g_XMFour);
        end;

        h := XMVectorDivide(h, g_XMSix);

        hv := XMVectorSelect(v, h, g_XMSelect1000);
        hva := XMVectorSelect(rgb, hv, g_XMSelect1110);
        Result := XMVectorSelect(s, hva, g_XMSelect1011);
    end;
end;



function XMColorHSVToRGB(hsv: TXMVECTOR): TXMVECTOR; inline;
var
    h, s, v, h6, i, f, p, q, t: TXMVECTOR;
    _rgb: TXMVECTOR;
    ii: int32;
    vt, qv, pv, pq, tp, vp: TXMVECTOR;
begin
    h := XMVectorSplatX(hsv);
    s := XMVectorSplatY(hsv);
    v := XMVectorSplatZ(hsv);

    h6 := XMVectorMultiply(h, g_XMSix);

    i := XMVectorFloor(h6);
    f := XMVectorSubtract(h6, i);

    // p = v* (1-s)
    p := XMVectorMultiply(v, XMVectorSubtract(g_XMOne, s));

    // q = v*(1-f*s)
    q := XMVectorMultiply(v, XMVectorSubtract(g_XMOne, XMVectorMultiply(f, s)));

    // t = v*(1 - (1-f)*s)
    t := XMVectorMultiply(v, XMVectorSubtract(g_XMOne, XMVectorMultiply(XMVectorSubtract(g_XMOne, f), s)));

    ii := trunc((XMVectorGetX(XMVectorMod(i, g_XMSix))));

    case (ii) of
        0: // rgb = vtp
        begin
            vt := XMVectorSelect(t, v, g_XMSelect1000);
            _rgb := XMVectorSelect(p, vt, g_XMSelect1100);
        end;
        1: // rgb = qvp
        begin
            qv := XMVectorSelect(v, q, g_XMSelect1000);
            _rgb := XMVectorSelect(p, qv, g_XMSelect1100);
        end;
        2: // rgb = pvt
        begin
            pv := XMVectorSelect(v, p, g_XMSelect1000);
            _rgb := XMVectorSelect(t, pv, g_XMSelect1100);
        end;
        3: // rgb = pqv
        begin
            pq := XMVectorSelect(q, p, g_XMSelect1000);
            _rgb := XMVectorSelect(v, pq, g_XMSelect1100);
        end;
        4: // rgb = tpv
        begin
            tp := XMVectorSelect(p, t, g_XMSelect1000);
            _rgb := XMVectorSelect(v, tp, g_XMSelect1100);
        end;
        else // rgb = vpq
        begin
            vp := XMVectorSelect(p, v, g_XMSelect1000);
            _rgb := XMVectorSelect(q, vp, g_XMSelect1100);
        end;
    end;

    Result := XMVectorSelect(hsv, _rgb, g_XMSelect1110);
end;



function XMColorRGBToYUV(rgb: TXMVECTOR): TXMVECTOR; inline;
const
    Scale0: TXMVECTORF32 = (f: (0.299, -0.147, 0.615, 0.0));
    Scale1: TXMVECTORF32 = (f: (0.587, -0.289, -0.515, 0.0));
    Scale2: TXMVECTORF32 = (f: (0.114, 0.436, -0.100, 0.0));
var
    m: TXMMATRIX;
    clr: TXMVECTOR;
begin

    M.Create(Scale0, Scale1, Scale2, g_XMZero);
    clr := XMVector3Transform(rgb, M);

    Result := XMVectorSelect(rgb, clr, g_XMSelect1110);
end;



function XMColorYUVToRGB(yuv: TXMVECTOR): TXMVECTOR; inline;
const
    Scale1: TXMVECTORF32 = (f: (0.0, -0.395, 2.032, 0.0));
    Scale2: TXMVECTORF32 = (f: (1.140, -0.581, 0.0, 0.0));
var
    m: TXMMATRIX;
    clr: TXMVECTOR;
begin

    M.Create(g_XMOne, Scale1, Scale2, g_XMZero);
    clr := XMVector3Transform(yuv, M);

    Result := XMVectorSelect(yuv, clr, g_XMSelect1110);
end;



function XMColorRGBToYUV_HD(rgb: TXMVECTOR): TXMVECTOR; inline;
const
    Scale0: TXMVECTORF32 = (f: (0.2126, -0.0997, 0.6150, 0.0));
    Scale1: TXMVECTORF32 = (f: (0.7152, -0.3354, -0.5586, 0.0));
    Scale2: TXMVECTORF32 = (f: (0.0722, 0.4351, -0.0564, 0.0));
var
    m: TXMMATRIX;
    clr: TXMVECTOR;
begin
    M.Create(Scale0, Scale1, Scale2, g_XMZero);
    clr := XMVector3Transform(rgb, M);
    Result := XMVectorSelect(rgb, clr, g_XMSelect1110);
end;



function XMColorYUVToRGB_HD(yuv: TXMVECTOR): TXMVECTOR; inline;
const
    Scale1: TXMVECTORF32 = (f: (0.0, -0.2153, 2.1324, 0.0));
    Scale2: TXMVECTORF32 = (f: (1.2803, -0.3806, 0.0, 0.0));
var
    m: TXMMATRIX;
    clr: TXMVECTOR;
begin
    M.Create(g_XMOne, Scale1, Scale2, g_XMZero);
    clr := XMVector3Transform(yuv, M);
    Result := XMVectorSelect(yuv, clr, g_XMSelect1110);
end;



function XMColorRGBToXYZ(rgb: TXMVECTOR): TXMVECTOR; inline;
const
    Scale0: TXMVECTORF32 = (f: (0.4887180, 0.1762044, 0.0000000, 0.0));
    Scale1: TXMVECTORF32 = (f: (0.3106803, 0.8129847, 0.0102048, 0.0));
    Scale2: TXMVECTORF32 = (f: (0.2006017, 0.0108109, 0.9897952, 0.0));
    Scale: TXMVECTORF32 = (f: (1.0 / 0.17697, 1.0 / 0.17697, 1.0 / 0.17697, 0.0));
var
    m: TXMMATRIX;
    clr: TXMVECTOR;
begin
    M.Create(Scale0, Scale1, Scale2, g_XMZero);
    clr := XMVectorMultiply(XMVector3Transform(rgb, M), Scale);
    Result := XMVectorSelect(rgb, clr, g_XMSelect1110);
end;



function XMColorXYZToRGB(xyz: TXMVECTOR): TXMVECTOR; inline;
const
    Scale0: TXMVECTORF32 = (f: (2.3706743, -0.5138850, 0.0052982, 0.0));
    Scale1: TXMVECTORF32 = (f: (-0.9000405, 1.4253036, -0.0146949, 0.0));
    Scale2: TXMVECTORF32 = (f: (-0.4706338, 0.0885814, 1.0093968, 0.0));
    Scale: TXMVECTORF32 = (f: (0.17697, 0.17697, 0.17697, 0.0));
var
    m: TXMMATRIX;
    clr: TXMVECTOR;
begin
    M.Create(Scale0, Scale1, Scale2, g_XMZero);
    clr := XMVector3Transform(XMVectorMultiply(xyz, Scale), M);
    Result := XMVectorSelect(xyz, clr, g_XMSelect1110);
end;



function XMColorXYZToSRGB(xyz: TXMVECTOR): TXMVECTOR; inline;
const
    Scale0: TXMVECTORF32 = (f: (3.2406, -0.9689, 0.0557, 0.0));
    Scale1: TXMVECTORF32 = (f: (-1.5372, 1.8758, -0.2040, 0.0));
    Scale2: TXMVECTORF32 = (f: (-0.4986, 0.0415, 1.0570, 0.0));
    Cutoff: TXMVECTORF32 = (f: (0.0031308, 0.0031308, 0.0031308, 0.0));
    Exp: TXMVECTORF32 = (f: (1.0 / 2.4, 1.0 / 2.4, 1.0 / 2.4, 1.0));
var
    lclr, clr, largeC, smallC, sel: TXMVECTOR;
    M: TXMMATRIX;
begin
    M.Create(Scale0, Scale1, Scale2, g_XMZero);
    lclr := XMVector3Transform(xyz, M);

    sel := XMVectorGreater(lclr, Cutoff);

    // clr := 12.92 * lclr for lclr <= 0.0031308f
    smallC := XMVectorMultiply(lclr, g_XMsrgbScale);

    // clr := (1+a)*pow(lclr, 1/2.4) - a for lclr > 0.0031308 (where a = 0.055)
    largeC := XMVectorSubtract(XMVectorMultiply(g_XMsrgbA1, XMVectorPow(lclr, Exp)), g_XMsrgbA);

    clr := XMVectorSelect(smallC, largeC, sel);

    Result := XMVectorSelect(xyz, clr, g_XMSelect1110);
end;


function XMColorSRGBToXYZ(srgb: TXMVECTOR): TXMVECTOR; inline;
const
    Scale0: TXMVECTORF32 = (f: (0.4124, 0.2126, 0.0193, 0.0));
    Scale1: TXMVECTORF32 = (f: (0.3576, 0.7152, 0.1192, 0.0));
    Scale2: TXMVECTORF32 = (f: (0.1805, 0.0722, 0.9505, 0.0));
    Cutoff: TXMVECTORF32 = (f: (0.04045, 0.04045, 0.04045, 0.0));
    Exp: TXMVECTORF32 = (f: (2.4, 2.4, 2.4, 1.0));
var
    sel, smallC, largeC, lclr, clr: TXMVECTOR;
    M: TXMMATRIX;
begin
    sel := XMVectorGreater(srgb, Cutoff);
    // lclr = clr / 12.92
    smallC := XMVectorDivide(srgb, g_XMsrgbScale);
    // lclr = pow( (clr + a) / (1+a), 2.4 )
    largeC := XMVectorPow(XMVectorDivide(XMVectorAdd(srgb, g_XMsrgbA), g_XMsrgbA1), Exp);
    lclr := XMVectorSelect(smallC, largeC, sel);
    M.Create(Scale0, Scale1, Scale2, g_XMZero);
    clr := XMVector3Transform(lclr, M);
    Result := XMVectorSelect(srgb, clr, g_XMSelect1110);
end;



function XMColorRGBToSRGB(rgb: TXMVECTOR): TXMVECTOR; inline;
const
    Cutoff: TXMVECTORF32 = (f: (0.0031308, 0.0031308, 0.0031308, 1.0));
    Linear: TXMVECTORF32 = (f: (12.92, 12.92, 12.92, 1.0));
    Scale: TXMVECTORF32 = (f: (1.055, 1.055, 1.055, 1.0));
    Bias: TXMVECTORF32 = (f: (0.055, 0.055, 0.055, 0.0));
    InvGamma: TXMVECTORF32 = (f: (1.0 / 2.4, 1.0 / 2.4, 1.0 / 2.4, 1.0));
var
    V, V0, V1, select: TXMVECTOR;
begin
    V := XMVectorSaturate(rgb);
    V0 := XMVectorMultiply(V, Linear);
    V1 := XMVectorSubtract(XMVectorMultiply(Scale, XMVectorPow(V, InvGamma)), Bias);
    select := XMVectorLess(V, Cutoff);
    V := XMVectorSelect(V1, V0, select);
    Result := XMVectorSelect(rgb, V, g_XMSelect1110);
end;



function XMColorSRGBToRGB(srgb: TXMVECTOR): TXMVECTOR; inline;
const
    Cutoff: TXMVECTORF32 = (f: (0.04045, 0.040450, 0.040450, 1.0));
    ILinear: TXMVECTORF32 = (f: (1.0 / 12.920, 1.0 / 12.920, 1.0 / 12.920, 1.0));
    Scale: TXMVECTORF32 = (f: (1.0 / 1.0550, 1.0 / 1.0550, 1.0 / 1.0550, 1.0));
    Bias: TXMVECTORF32 = (f: (0.0550, 0.055, 0.0550, 0.0));
    Gamma: TXMVECTORF32 = (f: (2.40, 2.40, 2.40, 1.0));
var
    V, V0, V1, select: TXMVECTOR;
begin
    V := XMVectorSaturate(srgb);
    V0 := XMVectorMultiply(V, ILinear);
    V1 := XMVectorPow(XMVectorMultiply(XMVectorAdd(V, Bias), Scale), Gamma);
    select := XMVectorGreater(V, Cutoff);
    V := XMVectorSelect(V0, V1, select);
    Result := XMVectorSelect(srgb, V, g_XMSelect1110);
end;

{****************************************************************************
 *
 * Miscellaneous
 *
 ****************************************************************************}



{$IF DEFINED(_XM_NO_INTRINSICS_)}
function XMVerifyCPUSupport(CPURequired: TCPUType): boolean;
begin
    // No intrinsics path always supported
    Result := True;
end;

{$ELSE}

// see https://stackoverflow.com/questions/41507019/code-detects-mmx-sse-avx-but-not-avx2
function GetCPUID(Leaf, Subleaf: integer): TCPUInfo;
asm
{$IF Defined(CPUX86)}
           PUSH    EBX
           PUSH    EDI
           MOV     EDI, ECX
           MOV     ECX, EDX
           CPUID
           MOV     [EDI+$0], EAX
           MOV     [EDI+$4], EBX
           MOV     [EDI+$8], ECX
           MOV     [EDI+$c], EDX
           POP     EDI
           POP     EBX
{$ELSEIF Defined(CPUX64)}
           MOV     R9,RCX
           MOV     ECX,R8D
           MOV     R8,RBX
           MOV     EAX,EDX
           CPUID
           MOV     [R9+$0], EAX
           MOV     [R9+$4], EBX
           MOV     [R9+$8], ECX
           MOV     [R9+$c], EDX
           MOV     RBX, R8
{$ELSE}
  {$Message Fatal 'GetCPUID has not been implemented for this architecture.'}
{$IFEND}
end;

function XMVerifyCPUSupport(CPURequired: TCPUType): boolean;
var

    CPUInfo: TCPUInfo;
    Leaf: cardinal = 0;
    Subleaf: cardinal = 0;

begin
    CPUInfo := GetCPUID(0, 0);

    if CPURequired = CPU_AVX2 then
    begin
        if (CPUInfo[0] < 7) then
        begin
            Result := False;
            Exit;
        end;
    end
    else
    begin
        if (CPUInfo[0] < 1) then
        begin
            Result := False;
            Exit;
        end;
    end;

    CPUInfo := GetCPUID(1, 0);
    if CPURequired = CPU_AVX2 then
    begin
        // The compiler can emit FMA3 instructions even without explicit intrinsics use
        if ((CPUInfo[2] and $38081001) <> $38081001) then
        begin
            Result := False; // No F16C/AVX/OSXSAVE/SSE4.1/FMA3/SSE3 support
            Exit;
        end;
    end;
    (* ToDo
#elif defined(_XM_FMA3_INTRINSICS_) && defined(_XM_F16C_INTRINSICS_)
    if ((CPUInfo[2] and $38081001) <> $38081001) then
    begin
        result:= false; // No F16C/AVX/OSXSAVE/SSE4.1/FMA3/SSE3 support
        Exit;
    end;
#elif defined(_XM_FMA3_INTRINSICS_)
    if ((CPUInfo[2] and $18081001) <> $18081001) then
    begin
        result:= false; // No AVX/OSXSAVE/SSE4.1/FMA3/SSE3 support
        Exit;
    end;
#elif defined(_XM_F16C_INTRINSICS_)
    if ((CPUInfo[2] and $38080001) <> $38080001) then
    begin
        result:= false; // No F16C/AVX/OSXSAVE/SSE4.1/SSE3 support
        Exit;
    end;
        *)
    if (CPURequired = CPU_AVX2) then
    begin
        if ((CPUInfo[2] and $18080001) <> $18080001) then
        begin
            Result := False; // No AVX/OSXSAVE/SSE4.1/SSE3 support
            Exit;
        end;
    end;
    if (CPURequired = CPU_XM_SSE4) then
    begin
        if ((CPUInfo[2] and $80001) <> $80001) then
        begin
            Result := False; // No SSE3/SSE4.1 support
            Exit;
        end;
    end;
    if (CPURequired = CPU_XM_SSE3) then
    begin
        if ((CPUInfo[2] and $1) <> $1) then
        begin
            Result := False; // No SSE3 support
            Exit;
        end;
    end;

    // The x64 processor model requires SSE2 support, but no harm in checking
    if ((CPUInfo[3] and $6000000) <> $6000000) then
    begin
        Result := False; // No SSE2/SSE support
        Exit;
    end;
    if CPURequired = CPU_AVX2 then
    begin
        CPUInfo := GetCPUID(7, 0);
        if ((CPUInfo[1] and $20) <> $20) then
        begin
            Result := False; // No AVX2 support
            exit;
        end;
    end;
    Result := True;
end;

{$ENDIF}

//------------------------------------------------------------------------------

function XMVerifyCPUSupport2(): boolean; inline;
begin
    (* ToDo
{$IF  defined(_XM_SSE_INTRINSICS_) AND NOT defined(_XM_NO_INTRINSICS_)}
      GetCPUID
    int CPUInfo[4] = { -1 };
    __cpuid(CPUInfo, 0);

    {$ifdef __AVX2__ }
        if (CPUInfo[0] < 7) then
        begin
            result:= false;
            Exit;
        end;
    {$ELSE}
        if (CPUInfo[0] < 1) then
        begin
            result:= false;
            Exit;
        end;
    {$ENDIF}

    __cpuid(CPUInfo, 1);

    {$IF  defined(__AVX2__) OR defined(_XM_AVX2_INTRINSICS_)}
        // The compiler can emit FMA3 instructions even without explicit intrinsics use
        if ((CPUInfo[2] & $38081001) <> $38081001) then
        begin
            result:= false; // No F16C/AVX/OSXSAVE/SSE4.1/FMA3/SSE3 support
            Exit;
        end;
    {$ELSEIF  defined(_XM_FMA3_INTRINSICS_) AND defined(_XM_F16C_INTRINSICS_)
        if ((CPUInfo[2] & $38081001) <> $38081001) then
        begin
            result:= false; // No F16C/AVX/OSXSAVE/SSE4.1/FMA3/SSE3 support
            Exit;
        end;
    {$ELSEIF  defined(_XM_FMA3_INTRINSICS_)
        if ((CPUInfo[2] & $18081001) <> $18081001) then
        begin
            result:= false; // No AVX/OSXSAVE/SSE4.1/FMA3/SSE3 support
            Exit;
        end;
    {$ELSEIF  defined(_XM_F16C_INTRINSICS_)
        if ((CPUInfo[2] & $38080001) <> $38080001) then
        begin
            result:= false; // No F16C/AVX/OSXSAVE/SSE4.1/SSE3 support
            Exit;
        end;
    {$ELSEIF  defined(__AVX__)  OR  defined(_XM_AVX_INTRINSICS_)
        if ((CPUInfo[2] & $18080001) <> $18080001)then
        begin
            result:= false; // No AVX/OSXSAVE/SSE4.1/SSE3 support
            Exit;
        end;
    {$ELSEIF  defined(_XM_SSE4_INTRINSICS_)}
        if ((CPUInfo[2] & $80001) <> $80001)then
        begin
            result:= false; // No SSE3/SSE4.1 support
            Exit;
        end;
    {$ELSEIF  defined(_XM_SSE3_INTRINSICS_)}
        if (!(CPUInfo[2] & $1))  then
        begin
            result:= false; // No SSE3 support
            Exit;
        end;
    {$ENDIF}

    // The x64 processor model requires SSE2 support, but no harm in checking
    if ((CPUInfo[3] & $6000000) <> $6000000)  then
    begin
        result:= false; // No SSE2/SSE support
        Exit;
    end;

    {$IF  defined(__AVX2__)  OR  defined(_XM_AVX2_INTRINSICS_)}
        __cpuidex(CPUInfo, 7, 0);
        if (!(CPUInfo[1] & $20)) then
        begin
            result:= false; // No AVX2 support
            Exit;
            end;
    {$ENDIF}

    result:= true;
{$ELSEIF  defined(_XM_ARM_NEON_INTRINSICS_) AND NOT defined(_XM_NO_INTRINSICS_)}
    // ARM-NEON support is required for the Windows on ARM platform
    result:= true;
{$ELSE}
    // No intrinsics path always supported
    result:= true;
{$ENDIF}
*)
end;

{$IF DEFINED(_XM_NO_INTRINSICS_) or DEFINED(_XM_ARM_NEON_INTRINSICS_)}
function XMFresnelTerm(CosIncidentAngle: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR;
var
    G, S, D, V0, V1, V2, V3: TXMVECTOR;
begin
    assert(not XMVector4IsInfinite(CosIncidentAngle));
    // Result = 0.5f * (g - c)^2 / (g + c)^2 * ((c * (g + c) - 1)^2 / (c * (g - c) + 1)^2 + 1) where
    // c = CosIncidentAngle
    // g = sqrt(c^2 + RefractionIndex^2 - 1)

    G := XMVectorMultiplyAdd(RefractionIndex, RefractionIndex, g_XMNegativeOne.v);
    G := XMVectorMultiplyAdd(CosIncidentAngle, CosIncidentAngle, G);
    G := XMVectorAbs(G);
    G := XMVectorSqrt(G);

    S := XMVectorAdd(G, CosIncidentAngle);
    D := XMVectorSubtract(G, CosIncidentAngle);

    V0 := XMVectorMultiply(D, D);
    V1 := XMVectorMultiply(S, S);
    V1 := XMVectorReciprocal(V1);
    V0 := XMVectorMultiply(g_XMOneHalf.v, V0);
    V0 := XMVectorMultiply(V0, V1);

    V2 := XMVectorMultiplyAdd(CosIncidentAngle, S, g_XMNegativeOne.v);
    V3 := XMVectorMultiplyAdd(CosIncidentAngle, D, g_XMOne.v);
    V2 := XMVectorMultiply(V2, V2);
    V3 := XMVectorMultiply(V3, V3);
    V3 := XMVectorReciprocal(V3);
    V2 := XMVectorMultiplyAdd(V2, V3, g_XMOne.v);

    Result := XMVectorMultiply(V0, V2);

    Result := XMVectorSaturate(Result);
end;

{$ELSE}// _XM_SSE_INTRINSICS_
function XMFresnelTerm(CosIncidentAngle: TXMVECTOR; RefractionIndex: TXMVECTOR): TXMVECTOR; assembler;
var
    GAddC, GSubC, vResult: TXMVECTOR;
asm
           // G = sqrt(abs((RefractionIndex^2-1) + CosIncidentAngle^2))
           //  G = _mm_mul_ps(RefractionIndex,RefractionIndex);
           MOVUPS  XMM0,[RefractionIndex]
           MULPS   XMM0, XMM0
           //  vTemp = _mm_mul_ps(CosIncidentAngle,CosIncidentAngle);
           MOVUPS  XMM1,[CosIncidentAngle]
           MULPS   XMM1, XMM1
           // G = _mm_sub_ps(G,g_XMOne);
           SUBPS   XMM0, [g_XMOne]
           // vTemp = _mm_add_ps(vTemp,G);
           ADDPS   XMM1, XMM0
           // max((0-vTemp),vTemp) == abs(vTemp)
           // The abs is needed to deal with refraction and cosine being zero
           // G = _mm_setzero_ps();
           XORPS   XMM0, XMM0
           // G = _mm_sub_ps(G,vTemp);
           SUBPS   XMM0, XMM1
           // G = _mm_max_ps(G,vTemp);
           MAXPS   XMM0, XMM1
           // Last operation, the sqrt()
           // G = _mm_sqrt_ps(G);
           SQRTPS  XMM0, XMM0

           // Calc G-C and G+C
           //  GAddC = _mm_add_ps(G,CosIncidentAngle);
           MOVUPS  XMM1, [CosIncidentAngle]
           ADDPS   XMM1, XMM0
           MOVUPS  [GAddC],XMM1
           //  GSubC = _mm_sub_ps(G,CosIncidentAngle);
           MOVUPS  XMM2, XMM0
           SUBPS   XMM2, [CosIncidentAngle]
           MOVUPS  [GSubC],XMM2
           // Perform the term (0.5f *(g - c)^2) / (g + c)^2
           //  vResult = _mm_mul_ps(GSubC,GSubC);
           MULPS   XMM2, XMM2
           // vTemp = _mm_mul_ps(GAddC,GAddC);
           MULPS   XMM1, XMM1
           // vResult = _mm_mul_ps(vResult,g_XMOneHalf);
           MULPS   XMM2, [g_XMOneHalf]
           // vResult = _mm_div_ps(vResult,vTemp);
           DIVPS   XMM2, XMM1
           MOVUPS  [vResult],XMM2

           // Perform the term ((c * (g + c) - 1)^2 / (c * (g - c) + 1)^2 + 1)
           // GAddC = _mm_mul_ps(GAddC,CosIncidentAngle);
           MOVUPS  XMM0, [GAddC]
           MULPS   XMM0, [CosIncidentAngle]
           // GSubC = _mm_mul_ps(GSubC,CosIncidentAngle);
           MOVUPS  XMM1, [GSubC]
           MULPS   XMM1, [CosIncidentAngle]
           // GAddC = _mm_sub_ps(GAddC,g_XMOne);
           SUBPS   XMM0, [g_XMOne]
           // GSubC = _mm_add_ps(GSubC,g_XMOne);
           ADDPS   XMM1, [g_XMOne]
           // GAddC = _mm_mul_ps(GAddC,GAddC);
           MULPS   XMM0, XMM0
           // GSubC = _mm_mul_ps(GSubC,GSubC);
           MULPS   XMM1, XMM1
           // GAddC = _mm_div_ps(GAddC,GSubC);
           DIVPS   XMM0, XMM1
           // GAddC = _mm_add_ps(GAddC,g_XMOne);
           ADDPS   XMM0, [g_XMOne]
           // Multiply the two term parts
           // vResult = _mm_mul_ps(vResult,GAddC);
           MULPS   XMM0, [vResult]
           // Clamp to 0.0 - 1.0
           // vResult = _mm_max_ps(vResult,g_XMZero);
           MAXPS   XMM0, [g_XMZero]
           // vResult = _mm_min_ps(vResult,g_XMOne);
           MINPS   XMM0, [g_XMOne]
           // return vResult;
           MOVUPS  [result],XMM0
end;

{$ENDIF}


function XMScalarNearEqual(S1: single; S2: single; Epsilon: single): boolean; inline;
var
    Delta: single;
begin
    Delta := S1 - S2;
    Result := (abs(Delta) <= Epsilon);
end;


//------------------------------------------------------------------------------
// Modulo the range of the given angle such that -XM_PI <= Angle < XM_PI
function XMScalarModAngle(Angle: single): single; inline;
var
    fTemp, fAngle: single;
begin
    // Note: The modulo is performed with unsigned math only to work
    // around a precision error on numbers that are close to PI

    // Normalize the range from 0.0f to XM_2PI
    fAngle := Angle + XM_PI;
    // Perform the modulo, unsigned
    fTemp := abs(fAngle);
    fTemp := fTemp - (XM_2PI * (fTemp / XM_2PI));
    // Restore the number to the range of -XM_PI to XM_PI-epsilon
    fTemp := fTemp - XM_PI;
    // If the modulo'd value was negative, restore negation
    if (fAngle < 0.0) then
        fTemp := -fTemp;

    Result := fTemp;
end;



function XMScalarSin(Value: single): single; inline;
var
    quotient, y, y2: single;
begin
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    quotient := XM_1DIV2PI * Value;
    if (Value >= 0.0) then
        quotient := (quotient + 0.5)
    else
        quotient := (quotient - 0.5);
    y := Value - XM_2PI * quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    if (y > XM_PIDIV2) then
        y := XM_PI - y
    else if (y < -XM_PIDIV2) then
        y := -XM_PI - y;

    // 11-degree minimax approximation
    y2 := y * y;
    Result := (((((-2.3889859e-08 * y2 + 2.7525562e-06) * y2 - 0.00019840874) * y2 + 0.0083333310) * y2 - 0.16666667) * y2 + 1.0) * y;
end;


function XMScalarSinEst(Value: single): single; inline;
var
    quotient, y, y2: single;
begin
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    quotient := XM_1DIV2PI * Value;
    if (Value >= 0.0) then
        quotient := quotient + 0.5
    else
        quotient := quotient - 0.5;

    y := Value - XM_2PI * quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    if (y > XM_PIDIV2) then
        y := XM_PI - y
    else if (y < -XM_PIDIV2) then
        y := -XM_PI - y;

    // 7-degree minimax approximation
    y2 := y * y;
    Result := (((-0.00018524670 * y2 + 0.0083139502) * y2 - 0.16665852) * y2 + 1.0) * y;
end;


function XMScalarCos(Value: single): single; inline;
var
    quotient, y, y2, sign, p: single;
begin
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    quotient := XM_1DIV2PI * Value;
    if (Value >= 0.0) then
        quotient := quotient + 0.5
    else
        quotient := quotient - 0.5;

    y := Value - XM_2PI * quotient;

    // Map y to [-pi/2,pi/2] with cos(y) = sign*cos(x).

    if (y > XM_PIDIV2) then
    begin
        y := XM_PI - y;
        sign := -1.0;
    end
    else if (y < -XM_PIDIV2) then
    begin
        y := -XM_PI - y;
        sign := -1.0;
    end
    else
    begin
        sign := +1.0;
    end;

    // 10-degree minimax approximation
    y2 := y * y;
    p := ((((-2.6051615e-07 * y2 + 2.4760495e-05) * y2 - 0.0013888378) * y2 + 0.041666638) * y2 - 0.5) * y2 + 1.0;
    Result := sign * p;
end;


function XMScalarCosEst(Value: single): single; inline;
var
    quotient, y, y2, sign, p: single;
begin
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    quotient := XM_1DIV2PI * Value;
    if (Value >= 0.0) then
        quotient := quotient + 0.5
    else
        quotient := quotient - 0.5;

    y := Value - XM_2PI * quotient;

    // Map y to [-pi/2,pi/2] with cos(y) = sign*cos(x).
    if (y > XM_PIDIV2) then
    begin
        y := XM_PI - y;
        sign := -1.0;
    end
    else if (y < -XM_PIDIV2) then
    begin
        y := -XM_PI - y;
        sign := -1.0;
    end
    else
    begin
        sign := +1.0;
    end;

    // 6-degree minimax approximation
    y2 := y * y;
    p := ((-0.0012712436 * y2 + 0.041493919) * y2 - 0.49992746) * y2 + 1.0;
    Result := sign * p;
end;


procedure XMScalarSinCos(out pSin: single; out pCos: single; Value: single); inline;
var
    quotient, sign, y, y2, p: single;
begin
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    quotient := XM_1DIV2PI * Value;
    if (Value >= 0.0) then
        quotient := (quotient + 0.5)
    else
        quotient := (quotient - 0.5);

    y := Value - XM_2PI * quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    if (y > XM_PIDIV2) then
    begin
        y := XM_PI - y;
        sign := -1.0;
    end
    else if (y < -XM_PIDIV2) then
    begin
        y := -XM_PI - y;
        sign := -1.0;
    end
    else
    begin
        sign := +1.0;
    end;

    y2 := y * y;

    // 11-degree minimax approximation
    pSin := (((((-2.3889859e-08 * y2 + 2.7525562e-06) * y2 - 0.00019840874) * y2 + 0.0083333310) * y2 - 0.16666667) * y2 + 1.0) * y;

    // 10-degree minimax approximation
    p := ((((-2.6051615e-07 * y2 + 2.4760495e-05) * y2 - 0.0013888378) * y2 + 0.041666638) * y2 - 0.5) * y2 + 1.0;
    pCos := sign * p;
end;


procedure XMScalarSinCosEst(out pSin: single; out pCos: single; Value: single); inline;
var
    quotient, y, sign, y2, p: single;
begin
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    quotient := XM_1DIV2PI * Value;
    if (Value >= 0.0) then
        quotient := (quotient + 0.5)
    else
        quotient := (quotient - 0.5);

    y := Value - XM_2PI * quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    if (y > XM_PIDIV2) then
    begin
        y := XM_PI - y;
        sign := -1.0;
    end
    else if (y < -XM_PIDIV2) then
    begin
        y := -XM_PI - y;
        sign := -1.0;
    end
    else
    begin
        sign := +1.0;
    end;

    y2 := y * y;

    // 7-degree minimax approximation
    pSin := (((-0.00018524670 * y2 + 0.0083139502) * y2 - 0.16665852) * y2 + 1.0) * y;

    // 6-degree minimax approximation
    p := ((-0.0012712436 * y2 + 0.041493919) * y2 - 0.49992746) * y2 + 1.0;
    pCos := sign * p;
end;



function XMScalarASin(Value: single): single;
var
    nonnegative: boolean;
    x, omx, root: single;
begin
    // Clamp input to [-1,1].
    nonnegative := (Value >= 0.0);
    x := abs(Value);
    omx := 1.0 - x;
    if (omx < 0.0) then
        omx := 0.0;
    root := sqrt(omx);

    // 7-degree minimax approximation
    Result := ((((((-0.0012624911 * x + 0.0066700901) * x - 0.0170881256) * x + 0.0308918810) * x - 0.0501743046) * x + 0.0889789874) * x - 0.2145988016) * x + 1.5707963050;
    Result := Result * root;  // acos(|x|)

    // acos(x) = pi - acos(-x) when x < 0, asin(x) = pi/2 - acos(x)
    if nonnegative then
        Result := XM_PIDIV2 - Result
    else
        Result := Result - XM_PIDIV2;
end;


function XMScalarASinEst(Value: single): single;
var
    nonnegative: boolean;
    x, omx, root: single;
begin
    // Clamp input to [-1,1].
    nonnegative := (Value >= 0.0);
    x := abs(Value);
    omx := 1.0 - x;
    if (omx < 0.0) then
        omx := 0.0;
    root := sqrt(omx);

    // 3-degree minimax approximation
    Result := ((-0.0187293 * x + 0.0742610) * x - 0.2121144) * x + 1.5707288;
    Result := Result * root;  // acos(|x|)

    // acos(x) = pi - acos(-x) when x < 0, asin(x) = pi/2 - acos(x)
    if nonnegative then
        Result := XM_PIDIV2 - Result
    else
        Result := Result - XM_PIDIV2;
end;


function XMScalarACos(Value: single): single; inline;
var
    nonnegative: boolean;
    x, omx, root: single;
begin
    // Clamp input to [-1,1].
    nonnegative := (Value >= 0.0);
    x := abs(Value);
    omx := 1.0 - x;
    if (omx < 0.0) then
        omx := 0.0;
    root := sqrt(omx);

    // 7-degree minimax approximation
    Result := ((((((-0.0012624911 * x + 0.0066700901) * x - 0.0170881256) * x + 0.0308918810) * x - 0.0501743046) * x + 0.0889789874) * x - 0.2145988016) * x + 1.5707963050;
    Result := Result * root;
    // acos(x) = pi - acos(-x) when x < 0
    if not nonnegative then
        Result := XM_PI - Result;
end;




function XMScalarACosEst(Value: single): single; inline;
var
    nonnegative: boolean;
    x, omx, root: single;
begin
    // Clamp input to [-1,1].
    nonnegative := (Value >= 0.0);
    x := abs(Value);
    omx := 1.0 - x;
    if (omx < 0.0) then
        omx := 0.0;
    root := sqrt(omx);

    // 3-degree minimax approximation
    Result := ((-0.0187293 * x + 0.0742610) * x - 0.2121144) * x + 1.5707288;
    Result := Result * root;

    // acos(x) = pi - acos(-x) when x < 0
    if not nonnegative then
        Result := XM_PI - Result;
end;

{ TXMFLOAT4X4A - 4x4 Matrix: 32 bit floating point components aligned on a 16 byte boundary }

constructor TXMFLOAT4X4A.Create(m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23, m30, m31, m32, m33: single);
begin
    _11 := m00;
    _12 := m01;
    _13 := m02;
    _14 := m03;
    _21 := m10;
    _22 := m11;
    _23 := m12;
    _24 := m13;
    _31 := m20;
    _32 := m21;
    _33 := m22;
    _34 := m23;
    _41 := m30;
    _42 := m31;
    _43 := m32;
    _44 := m33;
end;

constructor TXMFLOAT4X4A.Create(constref pArray: PSingle);
begin

end;

function TXMFLOAT4X4A.Get(Row, Column: size_t): single;
begin
    Result := m[Row, Column];
end;

{ TXMFLOAT4X4 - 4x4 Matrix: 32 bit floating point components }

constructor TXMFLOAT4X4.Create(m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23, m30, m31, m32, m33: single);
begin
    _11 := m00;
    _12 := m01;
    _13 := m02;
    _14 := m03;
    _21 := m10;
    _22 := m11;
    _23 := m12;
    _24 := m13;
    _31 := m20;
    _32 := m21;
    _33 := m22;
    _34 := m23;
    _41 := m30;
    _42 := m31;
    _43 := m32;
    _44 := m33;
end;

constructor TXMFLOAT4X4.Create(constref pArray: PSingle);
begin
    assert(pArray <> nil);

    m[0, 0] := pArray[0];
    m[0, 1] := pArray[1];
    m[0, 2] := pArray[2];
    m[0, 3] := pArray[3];

    m[1, 0] := pArray[4];
    m[1, 1] := pArray[5];
    m[1, 2] := pArray[6];
    m[1, 3] := pArray[7];

    m[2, 0] := pArray[8];
    m[2, 1] := pArray[9];
    m[2, 2] := pArray[10];
    m[2, 3] := pArray[11];

    m[3, 0] := pArray[12];
    m[3, 1] := pArray[13];
    m[3, 2] := pArray[14];
    m[3, 3] := pArray[15];
end;

function TXMFLOAT4X4.Get(Row, Column: size_t): single;
begin
    Result := m[Row, Column];
end;

{ TXMFLOAT4X3A - 4x3 Matrix: 32 bit floating point components aligned on a 16 byte boundary }

constructor TXMFLOAT4X3A.Create(m00, m01, m02, m10, m11, m12, m20, m21, m22, m30, m31, m32: single);
begin
    _11 := m00;
    _12 := m01;
    _13 := m02;
    _21 := m10;
    _22 := m11;
    _23 := m12;
    _31 := m20;
    _32 := m21;
    _33 := m22;
    _41 := m30;
    _42 := m31;
    _43 := m32;
end;

constructor TXMFLOAT4X3A.Create(constref pArray: PSingle);
begin

end;

function TXMFLOAT4X3A.Get(Row, Column: size_t): single;
begin
    Result := m[Row, Column];
end;

{****************************************************************************
 *
 * XMFLOAT4X3 operators
 *
 ****************************************************************************}

//------------------------------------------------------------------------------
{ TXMFLOAT4X3 - 4x3 Matrix: 32 bit floating point components }

constructor TXMFLOAT4X3.Create(m00, m01, m02, m10, m11, m12, m20, m21, m22, m30, m31, m32: single);
begin
    _11 := m00;
    _12 := m01;
    _13 := m02;
    _21 := m10;
    _22 := m11;
    _23 := m12;
    _31 := m20;
    _32 := m21;
    _33 := m22;
    _41 := m30;
    _42 := m31;
    _43 := m32;
end;

constructor TXMFLOAT4X3.Create(constref pArray: PSingle);
begin
    assert(pArray <> nil);

    m[0, 0] := pArray[0];
    m[0, 1] := pArray[1];
    m[0, 2] := pArray[2];

    m[1, 0] := pArray[3];
    m[1, 1] := pArray[4];
    m[1, 2] := pArray[5];

    m[2, 0] := pArray[6];
    m[2, 1] := pArray[7];
    m[2, 2] := pArray[8];

    m[3, 0] := pArray[9];
    m[3, 1] := pArray[10];
    m[3, 2] := pArray[11];
end;

function TXMFLOAT4X3.Get(Row, Column: size_t): single;
begin
    Result := m[Row, Column];
end;

{ TXMUINT4 - 4D Vector; 32 bit unsigned integer components }

constructor TXMUINT4.Create(_X, _Y, _Z, _W: uint32);
begin
    x := _x;
    y := _y;
    z := _z;
    w := _w;
end;

constructor TXMUINT4.Create(pArray: Puint32);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
    w := pArray[3];
end;

//------------------------------------------------------------------------------
{ TXMINT4 - 4D Vector; 32 bit signed integer components}

constructor TXMINT4.Create(_X, _Y, _Z, _W: int32);
begin
    x := _x;
    y := _y;
    z := _z;
    w := _w;
end;

constructor TXMINT4.Create(pArray: Pint32);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
    w := pArray[3];
end;

{ TXMFLOAT4A - 4D Vector; 32 bit floating point components aligned on a 16 byte boundary }

constructor TXMFLOAT4A.Create(pArray: PSingle);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
    w := pArray[3];
end;

constructor TXMFLOAT4A.Create(_X, _Y, _Z, _W: single);
begin
    x := _x;
    y := _y;
    z := _z;
    w := _w;
end;

class operator TXMFLOAT4A.Implicit(Float4: TXMFLOAT4): TXMFLOAT4A;
begin
    Result.x := Float4.x;
    Result.y := Float4.y;
    Result.z := Float4.z;
    Result.w := Float4.w;
end;

{ TXMUINT3 - 3D Vector; 32 bit unsigned integer components }

constructor TXMUINT3.Create(_x, _y, _z: Uint32);
begin
    x := _x;
    y := _y;
    z := _z;
end;

constructor TXMUINT3.Create(pArray: PUint32);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
end;

{ TXMINT3 }

constructor TXMINT3.Create(_x, _y, _z: int32);
begin
    x := _x;
    y := _y;
    z := _z;
end;

constructor TXMINT3.Create(pArray: Pint32);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
end;

{ TXMFLOAT3A - 3D Vector; 32 bit floating point components aligned on a 16 byte boundary }

constructor TXMFLOAT3A.Create(_x, _y, _z: single);
begin
    x := _x;
    y := _y;
    z := _z;
end;

constructor TXMFLOAT3A.Create(pArray: Psingle);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
end;

class operator TXMFLOAT3A.Implicit(Float3: TXMFLOAT3): TXMFLOAT3A;
begin
    Result.x := Float3.x;
    Result.y := Float3.y;
    Result.z := Float3.z;
end;

{ TXMUINT2 - 2D Vector; 32 bit unsigned integer components }

constructor TXMUINT2.Create(_x, _y: uint32);
begin
    x := _x;
    y := _y;
end;

constructor TXMUINT2.Create(pArray: PUINT32);
begin
    x := pArray[0];
    y := pArray[1];
end;

{ TXMINT2 - 2D Vector; 32 bit signed integer components}

constructor TXMINT2.Create(_x, _y: int32);
begin
    x := _x;
    y := _y;
end;

constructor TXMINT2.Create(pArray: PINT32);
begin
    x := pArray[0];
    y := pArray[1];
end;

{ TXMFLOAT2A }
// 2D Vector; 32 bit floating point components aligned on a 16 byte boundary
constructor TXMFLOAT2A.Create(_x, _y: single);
begin
    x := _x;
    y := _y;
end;

constructor TXMFLOAT2A.Create(pArray: PSingle);
begin
    x := pArray[0];
    y := pArray[1];
end;

class operator TXMFLOAT2A.Implicit(Float2: TXMFLOAT2): TXMFLOAT2A;
begin
    Result.x := Float2.x;
    Result.y := Float2.y;
end;


//------------------------------------------------------------------------------
// 2D Vector; 32 bit floating point components
{ TXMFLOAT2 }

constructor TXMFLOAT2.Create(_x, _y: single);
begin
    x := _x;
    y := _y;
end;

constructor TXMFLOAT2.Create(pArray: PSingle);
begin
    x := pArray[0];
    y := pArray[1];
end;

{ TXMVECTORU8 }

class operator TXMVECTORU8.Implicit(a: TXMVECTORU8): PByte;
begin
    Result := @a.u[0];
end;

class operator TXMVECTORU8.Implicit(a: TXMVECTORU8): TXMVECTOR;
begin
    Result := a.v;
end;

{ TXMVECTORI32 }

class operator TXMVECTORI32.Implicit(a: TXMVECTORI32): Pint32;
begin
    Result := @a.i[0];
end;

class operator TXMVECTORI32.Implicit(a: TXMVECTORI32): TXMVECTOR;
begin
    Result := a.v;
end;

(****************************************************************************
 *
 * XMFLOAT3X3 operators
 *
 ****************************************************************************)
{ TXMFLOAT3X3 - 3x3 Matrix: 32 bit floating point components }

constructor TXMFLOAT3X3.Create(constref pArray: PSingle);
var
    Row, Column: size_t;
begin
    assert(pArray <> nil);
    for Row := 0 to 2 do
    begin
        for Column := 0 to 2 do
        begin
            m[Row, Column] := pArray[Row * 3 + Column];
        end;
    end;
end;

constructor TXMFLOAT3X3.Create(m00, m01, m02, m10, m11, m12, m20, m21, m22: single);
begin
    _11 := m00;
    _12 := m01;
    _13 := m02;
    _21 := m10;
    _22 := m11;
    _23 := m12;
    _31 := m20;
    _32 := m21;
    _33 := m22;
end;

function TXMFLOAT3X3.Get(Row, Column: size_t): single;
begin
    Result := m[Row, Column];
end;

{ TXMFLOAT4 - 4D Vector; 32 bit floating point components }

constructor TXMFLOAT4.Create(pArray: PSingle);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
    w := pArray[3];
end;

constructor TXMFLOAT4.Create(_X, _Y, _Z, _W: single);
begin
    x := _x;
    y := _y;
    z := _z;
    w := _w;
end;



{ TXMFLOAT3 - 3D Vector; 32 bit floating point components }

constructor TXMFLOAT3.Create(_x, _y, _z: single);
begin
    x := _x;
    y := _y;
    z := _z;
end;

constructor TXMFLOAT3.Create(pArray: Psingle);
begin
    x := pArray[0];
    y := pArray[1];
    z := pArray[2];
end;


(****************************************************************************
 *
 * XMMATRIX operators and methods
 *
 ****************************************************************************)

//------------------------------------------------------------------------------


{ TXMMATRIX }

constructor TXMMATRIX.Create(R0, R1, R2, R3: TXMVECTOR);
begin
    r[0] := R0;
    r[1] := R1;
    r[2] := R2;
    r[3] := R3;
end;

constructor TXMMATRIX.Create(m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23, m30, m31, m32, m33: single);
begin
    r[0] := XMVectorSet(m00, m01, m02, m03);
    r[1] := XMVectorSet(m10, m11, m12, m13);
    r[2] := XMVectorSet(m20, m21, m22, m23);
    r[3] := XMVectorSet(m30, m31, m32, m33);
end;

constructor TXMMATRIX.Create(pArray: PSingle);
begin
    assert(pArray <> nil);
    r[0] := XMLoadFloat4(pArray);
    r[1] := XMLoadFloat4(pArray + 4);
    r[2] := XMLoadFloat4(pArray + 8);
    r[3] := XMLoadFloat4(pArray + 12);
end;

class operator TXMMATRIX.Positive(a: TXMMATRIX): TXMMATRIX;
begin
    Result := a;
end;

class operator TXMMATRIX.Negative(a: TXMMATRIX): TXMMATRIX;
begin
    Result.r[0] := XMVectorNegate(a.r[0]);
    Result.r[1] := XMVectorNegate(a.r[1]);
    Result.r[2] := XMVectorNegate(a.r[2]);
    Result.r[3] := XMVectorNegate(a.r[3]);
end;

class operator TXMMATRIX.Add(a, b: TXMMATRIX): TXMMATRIX;
begin
    Result.r[0] := XMVectorAdd(a.r[0], b.r[0]);
    Result.r[1] := XMVectorAdd(a.r[1], b.r[1]);
    Result.r[2] := XMVectorAdd(a.r[2], b.r[2]);
    Result.r[3] := XMVectorAdd(a.r[3], b.r[3]);
end;

class operator TXMMATRIX.Subtract(a, b: TXMMATRIX): TXMMATRIX;
begin
    Result.r[0] := XMVectorSubtract(a.r[0], b.r[0]);
    Result.r[1] := XMVectorSubtract(a.r[1], b.r[1]);
    Result.r[2] := XMVectorSubtract(a.r[2], b.r[2]);
    Result.r[3] := XMVectorSubtract(a.r[3], b.r[3]);
end;

class operator TXMMATRIX.Multiply(a, b: TXMMATRIX): TXMMATRIX;
begin
    Result := XMMatrixMultiply(a, b);
end;

class operator TXMMATRIX.Multiply(M: TXMMATRIX; s: single): TXMMATRIX;
begin
    Result.r[0] := XMVectorScale(M.r[0], S);
    Result.r[1] := XMVectorScale(M.r[1], S);
    Result.r[2] := XMVectorScale(M.r[2], S);
    Result.r[3] := XMVectorScale(M.r[3], S);
end;

{$IF DEFINED(_XM_NO_INTRINSICS_)}
class operator TXMMATRIX.Divide(M: TXMMATRIX; s: single): TXMMATRIX;
var
    vS: TXMVECTOR;
begin
    vS := XMVectorReplicate(S);
    Result.r[0] := XMVectorDivide(M.r[0], vS);
    Result.r[1] := XMVectorDivide(M.r[1], vS);
    Result.r[2] := XMVectorDivide(M.r[2], vS);
    Result.r[3] := XMVectorDivide(M.r[3], vS);
end;

{$ELSEIF DEFINED(_XM_ARM_NEON_INTRINSICS_)}
class operator TXMMATRIX.Divide(M: TXMMATRIX; s: single): TXMMATRIX;
begin
    (* ToDo
   #if defined(_M_ARM64) || defined(_M_HYBRID_X86_ARM64)
    float32x4_t vS = vdupq_n_f32( S );
    r[0] = vdivq_f32( r[0], vS );
    r[1] = vdivq_f32( r[1], vS );
    r[2] = vdivq_f32( r[2], vS );
    r[3] = vdivq_f32( r[3], vS );
#else
    // 2 iterations of Newton-Raphson refinement of reciprocal
    float32x2_t vS = vdup_n_f32( S );
    float32x2_t R0 = vrecpe_f32( vS );
    float32x2_t S0 = vrecps_f32( R0, vS );
    R0 = vmul_f32( S0, R0 );
    S0 = vrecps_f32( R0, vS );
    R0 = vmul_f32( S0, R0 );
    float32x4_t Reciprocal = vcombine_u32(R0, R0);
    r[0] = vmulq_f32( r[0], Reciprocal );
    r[1] = vmulq_f32( r[1], Reciprocal );
    r[2] = vmulq_f32( r[2], Reciprocal );
    r[3] = vmulq_f32( r[3], Reciprocal );
#endif
    return *this;
*)
end;

{$ELSE}// _XM_SSE_INTRINSICS_
class operator TXMMATRIX.Divide(M: TXMMATRIX; s: single): TXMMATRIX; assembler;
asm
           // __m128 vS = _mm_set_ps1( S );
           MOVSS   XMM4, [S]
           SHUFPS  XMM4, XMM0, 0
           // r[0] = _mm_div_ps( r[0], vS );
           MOVSS   XMM0, TXMVECTOR(TXMMATRIX([M]).R[0])
           DIVPS   XMM0, XMM4
           MOVSS   TXMVECTOR(TXMMATRIX([result]).R[0]),XMM0
           // r[1] = _mm_div_ps( r[1], vS );
           MOVSS   XMM0, TXMVECTOR(TXMMATRIX([M]).R[1])
           DIVPS   XMM0, XMM4
           MOVSS   TXMVECTOR(TXMMATRIX([result]).R[1]),XMM0
           // r[2] = _mm_div_ps( r[2], vS );
           MOVSS   XMM0, TXMVECTOR(TXMMATRIX([M]).R[2])
           DIVPS   XMM0, XMM4
           MOVSS   TXMVECTOR(TXMMATRIX([result]).R[2]),XMM0
           // r[3] = _mm_div_ps( r[3], vS );
           MOVSS   XMM0, TXMVECTOR(TXMMATRIX([M]).R[3])
           DIVPS   XMM0, XMM4
           MOVSS   TXMVECTOR(TXMMATRIX([result]).R[3]),XMM0
end;
{$ENDIF}

function TXMMATRIX.Get(Row, Column: size_t): single;
begin
    Result := m[Row, Column];
end;

{ TXMVECTORU32 }

class operator TXMVECTORU32.Implicit(a: TXMVECTORU32): Puint32; inline;
begin
    Result := @a.u[0];
end;

class operator TXMVECTORU32.Implicit(a: TXMVECTORU32): TXMVECTOR; inline;
begin
    Result := a.v;
end;

{ TXMVECTORF32 }

class operator TXMVECTORF32.Implicit(a: TXMVECTORF32): PSingle; inline;
begin
    Result := @a.f[0];
end;

class operator TXMVECTORF32.Implicit(a: TXMVECTORF32): TXMVECTOR; inline;

begin
    Result := a.v;
end;



{***************************************************************************
 *
 * XMVECTOR operators
 *
 ***************************************************************************}

{ TXMVECTOR }

constructor TXMVECTOR.Create(x, y, z, w: single);
begin
    f32[0] := x;
    f32[1] := y;
    f32[2] := z;
    f32[3] := w;
end;

class operator TXMVECTOR.Positive(a: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := a;
end;

class operator TXMVECTOR.Negative(a: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVectorNegate(a);
end;


class operator TXMVECTOR.Subtract(a, b: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVectorSubtract(a, b);
end;

class operator TXMVECTOR.Multiply(a, b: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVectorMultiply(a, b);
end;

class operator TXMVECTOR.Divide(a, b: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVectorDivide(a, b);
end;

class operator TXMVECTOR.Multiply(v: TXMVECTOR; s: single): TXMVECTOR; inline;
begin
    Result := XMVectorScale(v, s);
end;

class operator TXMVECTOR.Divide(v: TXMVECTOR; s: single): TXMVECTOR; inline;
var
    vs: TXMVECTOR;
begin
    vS := XMVectorReplicate(S);
    Result := XMVectorDivide(V, vS);
end;

class operator TXMVECTOR.Add(a, b: TXMVECTOR): TXMVECTOR; inline;
begin
    Result := XMVectorAdd(a, b);
end;

end.
